{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self,x):\n",
    "        raise NotImplementedError()\n",
    "    def backward(self, out_grad, learning_rate):\n",
    "        raise NotImplementedError()\n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, in_size, out_size, enable_bias=True) -> None:\n",
    "        '''\n",
    "        in_size: the number of datapoints in the input, i.e. the input size\n",
    "        out_size: the number of neurons in the layer, i.e. the output size\n",
    "        enable_bias: whether to include bias in the layer\n",
    "        '''\n",
    "        # intialize weights and bias\n",
    "        self.initialize_parameters(in_size, out_size, enable_bias)\n",
    "\n",
    "    def initialize_parameters(self, in_size, out_size, enable_bias):\n",
    "        '''\n",
    "        reinitialize the weights and bias\n",
    "        '''\n",
    "        #self.weights = kaiming_uniform(out_size, in_size)\n",
    "        #self.bias = kaiming_uniform(out_size, 1) if enable_bias is not None else None\n",
    "        self.weights = np.random.randn(out_size, in_size) * np.sqrt(2/in_size)\n",
    "        self.bias = np.random.randn(out_size, 1) * np.sqrt(2/in_size) if enable_bias is not None else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        input: input to the layer\n",
    "        return: linear transformation of the input, WX + B\n",
    "        '''\n",
    "        self.input = input\n",
    "        return  (self.weights @ input) + (self.bias if self.bias is not None else 0)\n",
    "    \n",
    "    def backward(self, out_grad, learning_rate):\n",
    "        '''\n",
    "        out_grad: gradient of the loss w.r.t the output of the layer\n",
    "        learning_rate: learning rate for updating the weights\n",
    "        return: gradient of the loss w.r.t the input of the layer so that it can be used as input to the previous layer to ues in updating its parameters \n",
    "        '''\n",
    "        new_outgrad = self.weights.T @ out_grad     # gradient of the loss w.r.t the input of the layer\n",
    "        \n",
    "        w_grad = out_grad @ self.input.T            # gradient of the loss w.r.t the weights\n",
    "        self.weights -= learning_rate * w_grad      # update the weights\n",
    "\n",
    "        if (self.bias is not None):\n",
    "            b_grad = np.mean(out_grad, axis=1).reshape(-1,1)    # gradient of the loss w.r.t the bias\n",
    "                                                                # mean to equalize the broadcasting\n",
    "                                                                # reshape from (n,) to (n,1) to make it compatible with the bias shape\n",
    "            self.bias -= learning_rate * b_grad              # update the bias        \n",
    "\n",
    "        return new_outgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        None\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: the input of the RelU layer\n",
    "        return: RelU(x)\n",
    "        '''\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, 0)\n",
    "        # return np.maximum(x, 0)\n",
    "    \n",
    "    def backward(self, out_grad, learning_rate):\n",
    "        '''\n",
    "        out_grad: the output gradient of the previous layer\n",
    "        return: RelU'(x) âŠ™ outgrad\n",
    "        '''\n",
    "        Relu_prime = np.where(self.x > 0, 1, 0)     # derivative of the Relu function\n",
    "        #Relu_prime = (self.x > 0).astype(int)\n",
    "\n",
    "        return np.multiply(Relu_prime, out_grad)    # element-wise multiplication of the derivative of the Relu function and the output gradient of the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    def __init__(self) -> None:\n",
    "        None\n",
    "    def loss(self, y_predict, y):\n",
    "        '''\n",
    "        y_predict: the predicted value\n",
    "        y: the actual value (target): must be one hot encoded\n",
    "        return: the mean squared error loss\n",
    "        '''\n",
    "        return np.mean((y_predict - y)**2)\n",
    "    \n",
    "    def loss_gradient(self, y_predict, y):\n",
    "        '''\n",
    "        y_predict: the predicted value\n",
    "        y: the actual value (target)\n",
    "        return: the gradient of the loss w.r.t the predicted value\n",
    "        '''\n",
    "        no_of_classes = y.shape[0]          # y is transposed\n",
    "        no_of_datapoints = y.shape[1]\n",
    "        return (2/ (no_of_classes * no_of_datapoints)) * (y_predict - y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def loss(self, y_predict, y):\n",
    "        '''\n",
    "        y_predict: the predicted value (raw scores/logits)\n",
    "        y: the actual value (target): must be one hot encoded\n",
    "        return: the categorical cross-entropy loss\n",
    "        '''\n",
    "        # Avoid division by zero\n",
    "        epsilon = 1e-15\n",
    "        # Clip values to avoid log(0)\n",
    "        y_predict = np.clip(y_predict, epsilon, 1 - epsilon)\n",
    "        return -np.sum(y * np.log(y_predict)) / y.shape[1]  # Divide by number of samples for mean\n",
    "    \n",
    "    def loss_gradient(self, y_predict, y):\n",
    "        '''\n",
    "        y_predict: the predicted value (raw scores/logits)\n",
    "        y: the actual value (target)\n",
    "        return: the gradient of the loss w.r.t the predicted value\n",
    "        '''\n",
    "        return (y_predict - y) / y.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, x, y, layers_dim, learning_rate, max_iterations, activation_layer, loss, enable_bias=True) -> None:\n",
    "        '''\n",
    "        x: the input data\n",
    "        y: the target data\n",
    "        layers_dim: the dimensions of the layers of the neural network\n",
    "        learning_rate: the learning rate of the neural network\n",
    "        max_iterations: the maximum number of iterations to train the neural network\n",
    "        activation_function: the activation function of the neural network\n",
    "        error: the error function of the neural network\n",
    "        '''\n",
    "        self.x = x.T\n",
    "        self.y = y.T\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.network = []\n",
    "        self.initialize_network(layers_dim, activation_layer, enable_bias)\n",
    "    \n",
    "    def initialize_network(self, layers_dim, activation_layer, enable_bias) -> None:\n",
    "        '''\n",
    "        initialize the layers of the neural network\n",
    "        '''\n",
    "        # check for the dimensions of the input and output layers\n",
    "        assert layers_dim[0] == self.x.shape[0], f\"The input layer dimension does not match the input data, it should be {self.x.shape[0]}\"\n",
    "        assert layers_dim[-1] == self.y.shape[0], f\"The output layer dimension does not match the target data, it should be {self.y.shape[0]}\"\n",
    "        \n",
    "        for i in range(len(layers_dim) - 1):\n",
    "            self.network.append(Linear(layers_dim[i], layers_dim[i+1], enable_bias))\n",
    "            self.network.append(activation_layer())\n",
    "\n",
    "    def forward(self):\n",
    "        output = self.x   # input to the first layer\n",
    "        for layer in self.network:\n",
    "            output = layer(output)      # the output of the previous layer is the input to the next layer\n",
    "            \n",
    "        return output   # the output of the last layer\n",
    "    \n",
    "    def backward(self, y_predict):\n",
    "        out_grad = self.loss.loss_gradient(self, y_predict, self.y)    # gradient of the loss w.r.t the output of the last layer\n",
    "        for layer in reversed(self.network):\n",
    "            out_grad = layer.backward(out_grad, self.learning_rate)    # gradient of the loss w.r.t the input of the layer\n",
    "        return out_grad\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.max_iterations):\n",
    "            y_predict = self.forward()    # the output of the last layer\n",
    "            loss = self.loss.loss(self, y_predict, self.y)\n",
    "            accuracy = calculate_accuracy(y_predict, self.y)\n",
    "            self.backward(y_predict)\n",
    "            print(f\"Epoch: {epoch + 1}, Loss: {loss:.2f}, accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        output = x\n",
    "        for layer in self.network:\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, true_labels):\n",
    "    '''\n",
    "    predictions: the predicted labels\n",
    "    true_labels: the actual labels\n",
    "    return: the accuracy of the model\n",
    "    '''\n",
    "    pred_labels = np.argmax(predictions, axis=0)\n",
    "    true_labels = np.argmax(true_labels, axis=0)\n",
    "    accuracy = np.mean(pred_labels == true_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_uniform(fan_in, fan_out, **kwargs):\n",
    "    bound = math.sqrt(2) * math.sqrt(3/fan_in)\n",
    "    u = random.uniform(-bound, bound,fan_in*fan_out) \n",
    "    return np.reshape(u,(fan_in,fan_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fashion_mnist = fashion_mnist.load_data()       # load the fashion mnist dataset\n",
    "\n",
    "# plot the 10 classes of the fashion mnist dataset\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 10, figsize=(20, 20))\n",
    "for i in range(10):\n",
    "    ax[i].imshow(fashion_mnist[0][0][i], cmap='gray')\n",
    "    ax[i].set_title(fashion_mnist[0][1][i])\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.75, accuracy: 5.34%\n",
      "Epoch: 2, Loss: 0.20, accuracy: 12.67%\n",
      "Epoch: 3, Loss: 0.11, accuracy: 11.41%\n",
      "Epoch: 4, Loss: 0.11, accuracy: 13.01%\n",
      "Epoch: 5, Loss: 0.11, accuracy: 14.74%\n",
      "Epoch: 6, Loss: 0.10, accuracy: 16.37%\n",
      "Epoch: 7, Loss: 0.10, accuracy: 17.66%\n",
      "Epoch: 8, Loss: 0.10, accuracy: 18.58%\n",
      "Epoch: 9, Loss: 0.10, accuracy: 19.42%\n",
      "Epoch: 10, Loss: 0.10, accuracy: 20.35%\n",
      "Epoch: 11, Loss: 0.10, accuracy: 21.23%\n",
      "Epoch: 12, Loss: 0.10, accuracy: 22.12%\n",
      "Epoch: 13, Loss: 0.09, accuracy: 22.93%\n",
      "Epoch: 14, Loss: 0.09, accuracy: 23.72%\n",
      "Epoch: 15, Loss: 0.09, accuracy: 24.48%\n",
      "Epoch: 16, Loss: 0.09, accuracy: 25.12%\n",
      "Epoch: 17, Loss: 0.09, accuracy: 25.76%\n",
      "Epoch: 18, Loss: 0.09, accuracy: 26.42%\n",
      "Epoch: 19, Loss: 0.09, accuracy: 27.08%\n",
      "Epoch: 20, Loss: 0.09, accuracy: 27.70%\n",
      "Epoch: 21, Loss: 0.09, accuracy: 28.36%\n",
      "Epoch: 22, Loss: 0.09, accuracy: 29.03%\n",
      "Epoch: 23, Loss: 0.09, accuracy: 29.65%\n",
      "Epoch: 24, Loss: 0.09, accuracy: 30.37%\n",
      "Epoch: 25, Loss: 0.09, accuracy: 31.09%\n",
      "Epoch: 26, Loss: 0.09, accuracy: 31.92%\n",
      "Epoch: 27, Loss: 0.09, accuracy: 32.86%\n",
      "Epoch: 28, Loss: 0.08, accuracy: 33.96%\n",
      "Epoch: 29, Loss: 0.08, accuracy: 35.16%\n",
      "Epoch: 30, Loss: 0.08, accuracy: 36.38%\n",
      "Epoch: 31, Loss: 0.08, accuracy: 37.69%\n",
      "Epoch: 32, Loss: 0.08, accuracy: 38.84%\n",
      "Epoch: 33, Loss: 0.08, accuracy: 39.90%\n",
      "Epoch: 34, Loss: 0.08, accuracy: 40.74%\n",
      "Epoch: 35, Loss: 0.08, accuracy: 41.59%\n",
      "Epoch: 36, Loss: 0.08, accuracy: 42.36%\n",
      "Epoch: 37, Loss: 0.08, accuracy: 43.15%\n",
      "Epoch: 38, Loss: 0.08, accuracy: 43.95%\n",
      "Epoch: 39, Loss: 0.08, accuracy: 44.69%\n",
      "Epoch: 40, Loss: 0.08, accuracy: 45.47%\n",
      "Epoch: 41, Loss: 0.07, accuracy: 46.12%\n",
      "Epoch: 42, Loss: 0.07, accuracy: 46.79%\n",
      "Epoch: 43, Loss: 0.07, accuracy: 47.36%\n",
      "Epoch: 44, Loss: 0.07, accuracy: 47.84%\n",
      "Epoch: 45, Loss: 0.07, accuracy: 48.25%\n",
      "Epoch: 46, Loss: 0.07, accuracy: 48.63%\n",
      "Epoch: 47, Loss: 0.07, accuracy: 49.02%\n",
      "Epoch: 48, Loss: 0.07, accuracy: 49.29%\n",
      "Epoch: 49, Loss: 0.07, accuracy: 49.54%\n",
      "Epoch: 50, Loss: 0.07, accuracy: 49.80%\n",
      "Epoch: 51, Loss: 0.07, accuracy: 50.05%\n",
      "Epoch: 52, Loss: 0.07, accuracy: 50.28%\n",
      "Epoch: 53, Loss: 0.07, accuracy: 50.50%\n",
      "Epoch: 54, Loss: 0.07, accuracy: 50.72%\n",
      "Epoch: 55, Loss: 0.07, accuracy: 50.92%\n",
      "Epoch: 56, Loss: 0.07, accuracy: 51.12%\n",
      "Epoch: 57, Loss: 0.07, accuracy: 51.31%\n",
      "Epoch: 58, Loss: 0.07, accuracy: 51.47%\n",
      "Epoch: 59, Loss: 0.07, accuracy: 51.62%\n",
      "Epoch: 60, Loss: 0.07, accuracy: 51.76%\n",
      "Epoch: 61, Loss: 0.07, accuracy: 51.89%\n",
      "Epoch: 62, Loss: 0.07, accuracy: 52.06%\n",
      "Epoch: 63, Loss: 0.07, accuracy: 52.24%\n",
      "Epoch: 64, Loss: 0.07, accuracy: 52.40%\n",
      "Epoch: 65, Loss: 0.07, accuracy: 52.53%\n",
      "Epoch: 66, Loss: 0.07, accuracy: 52.68%\n",
      "Epoch: 67, Loss: 0.07, accuracy: 52.81%\n",
      "Epoch: 68, Loss: 0.07, accuracy: 52.93%\n",
      "Epoch: 69, Loss: 0.07, accuracy: 53.10%\n",
      "Epoch: 70, Loss: 0.07, accuracy: 53.24%\n",
      "Epoch: 71, Loss: 0.07, accuracy: 53.36%\n",
      "Epoch: 72, Loss: 0.07, accuracy: 53.47%\n",
      "Epoch: 73, Loss: 0.07, accuracy: 53.59%\n",
      "Epoch: 74, Loss: 0.07, accuracy: 53.73%\n",
      "Epoch: 75, Loss: 0.07, accuracy: 53.84%\n",
      "Epoch: 76, Loss: 0.07, accuracy: 53.97%\n",
      "Epoch: 77, Loss: 0.06, accuracy: 54.06%\n",
      "Epoch: 78, Loss: 0.06, accuracy: 54.18%\n",
      "Epoch: 79, Loss: 0.06, accuracy: 54.30%\n",
      "Epoch: 80, Loss: 0.06, accuracy: 54.40%\n",
      "Epoch: 81, Loss: 0.06, accuracy: 54.54%\n",
      "Epoch: 82, Loss: 0.06, accuracy: 54.65%\n",
      "Epoch: 83, Loss: 0.06, accuracy: 54.77%\n",
      "Epoch: 84, Loss: 0.06, accuracy: 54.90%\n",
      "Epoch: 85, Loss: 0.06, accuracy: 54.99%\n",
      "Epoch: 86, Loss: 0.06, accuracy: 55.11%\n",
      "Epoch: 87, Loss: 0.06, accuracy: 55.21%\n",
      "Epoch: 88, Loss: 0.06, accuracy: 55.33%\n",
      "Epoch: 89, Loss: 0.06, accuracy: 55.41%\n",
      "Epoch: 90, Loss: 0.06, accuracy: 55.54%\n",
      "Epoch: 91, Loss: 0.06, accuracy: 55.65%\n",
      "Epoch: 92, Loss: 0.06, accuracy: 55.75%\n",
      "Epoch: 93, Loss: 0.06, accuracy: 55.84%\n",
      "Epoch: 94, Loss: 0.06, accuracy: 55.97%\n",
      "Epoch: 95, Loss: 0.06, accuracy: 56.04%\n",
      "Epoch: 96, Loss: 0.06, accuracy: 56.14%\n",
      "Epoch: 97, Loss: 0.06, accuracy: 56.26%\n",
      "Epoch: 98, Loss: 0.06, accuracy: 56.36%\n",
      "Epoch: 99, Loss: 0.06, accuracy: 56.47%\n",
      "Epoch: 100, Loss: 0.06, accuracy: 56.57%\n",
      "Epoch: 101, Loss: 0.06, accuracy: 56.67%\n",
      "Epoch: 102, Loss: 0.06, accuracy: 56.78%\n",
      "Epoch: 103, Loss: 0.06, accuracy: 56.90%\n",
      "Epoch: 104, Loss: 0.06, accuracy: 57.01%\n",
      "Epoch: 105, Loss: 0.06, accuracy: 57.13%\n",
      "Epoch: 106, Loss: 0.06, accuracy: 57.24%\n",
      "Epoch: 107, Loss: 0.06, accuracy: 57.33%\n",
      "Epoch: 108, Loss: 0.06, accuracy: 57.46%\n",
      "Epoch: 109, Loss: 0.06, accuracy: 57.57%\n",
      "Epoch: 110, Loss: 0.06, accuracy: 57.70%\n",
      "Epoch: 111, Loss: 0.06, accuracy: 57.81%\n",
      "Epoch: 112, Loss: 0.06, accuracy: 57.94%\n",
      "Epoch: 113, Loss: 0.06, accuracy: 58.05%\n",
      "Epoch: 114, Loss: 0.06, accuracy: 58.17%\n",
      "Epoch: 115, Loss: 0.06, accuracy: 58.27%\n",
      "Epoch: 116, Loss: 0.06, accuracy: 58.34%\n",
      "Epoch: 117, Loss: 0.06, accuracy: 58.42%\n",
      "Epoch: 118, Loss: 0.06, accuracy: 58.53%\n",
      "Epoch: 119, Loss: 0.06, accuracy: 58.62%\n",
      "Epoch: 120, Loss: 0.06, accuracy: 58.75%\n",
      "Epoch: 121, Loss: 0.06, accuracy: 58.85%\n",
      "Epoch: 122, Loss: 0.06, accuracy: 58.92%\n",
      "Epoch: 123, Loss: 0.06, accuracy: 59.03%\n",
      "Epoch: 124, Loss: 0.06, accuracy: 59.10%\n",
      "Epoch: 125, Loss: 0.06, accuracy: 59.19%\n",
      "Epoch: 126, Loss: 0.06, accuracy: 59.26%\n",
      "Epoch: 127, Loss: 0.06, accuracy: 59.36%\n",
      "Epoch: 128, Loss: 0.06, accuracy: 59.47%\n",
      "Epoch: 129, Loss: 0.06, accuracy: 59.57%\n",
      "Epoch: 130, Loss: 0.06, accuracy: 59.67%\n",
      "Epoch: 131, Loss: 0.06, accuracy: 59.77%\n",
      "Epoch: 132, Loss: 0.06, accuracy: 59.87%\n",
      "Epoch: 133, Loss: 0.06, accuracy: 59.96%\n",
      "Epoch: 134, Loss: 0.06, accuracy: 60.05%\n",
      "Epoch: 135, Loss: 0.06, accuracy: 60.15%\n",
      "Epoch: 136, Loss: 0.06, accuracy: 60.27%\n",
      "Epoch: 137, Loss: 0.06, accuracy: 60.37%\n",
      "Epoch: 138, Loss: 0.06, accuracy: 60.48%\n",
      "Epoch: 139, Loss: 0.06, accuracy: 60.62%\n",
      "Epoch: 140, Loss: 0.06, accuracy: 60.72%\n",
      "Epoch: 141, Loss: 0.06, accuracy: 60.84%\n",
      "Epoch: 142, Loss: 0.06, accuracy: 60.95%\n",
      "Epoch: 143, Loss: 0.06, accuracy: 61.04%\n",
      "Epoch: 144, Loss: 0.06, accuracy: 61.15%\n",
      "Epoch: 145, Loss: 0.06, accuracy: 61.24%\n",
      "Epoch: 146, Loss: 0.06, accuracy: 61.35%\n",
      "Epoch: 147, Loss: 0.06, accuracy: 61.46%\n",
      "Epoch: 148, Loss: 0.06, accuracy: 61.56%\n",
      "Epoch: 149, Loss: 0.06, accuracy: 61.68%\n",
      "Epoch: 150, Loss: 0.06, accuracy: 61.75%\n",
      "Epoch: 151, Loss: 0.06, accuracy: 61.85%\n",
      "Epoch: 152, Loss: 0.06, accuracy: 61.98%\n",
      "Epoch: 153, Loss: 0.06, accuracy: 62.10%\n",
      "Epoch: 154, Loss: 0.06, accuracy: 62.21%\n",
      "Epoch: 155, Loss: 0.06, accuracy: 62.30%\n",
      "Epoch: 156, Loss: 0.06, accuracy: 62.42%\n",
      "Epoch: 157, Loss: 0.06, accuracy: 62.53%\n",
      "Epoch: 158, Loss: 0.05, accuracy: 62.62%\n",
      "Epoch: 159, Loss: 0.05, accuracy: 62.70%\n",
      "Epoch: 160, Loss: 0.05, accuracy: 62.80%\n",
      "Epoch: 161, Loss: 0.05, accuracy: 62.90%\n",
      "Epoch: 162, Loss: 0.05, accuracy: 63.00%\n",
      "Epoch: 163, Loss: 0.05, accuracy: 63.11%\n",
      "Epoch: 164, Loss: 0.05, accuracy: 63.20%\n",
      "Epoch: 165, Loss: 0.05, accuracy: 63.30%\n",
      "Epoch: 166, Loss: 0.05, accuracy: 63.38%\n",
      "Epoch: 167, Loss: 0.05, accuracy: 63.46%\n",
      "Epoch: 168, Loss: 0.05, accuracy: 63.53%\n",
      "Epoch: 169, Loss: 0.05, accuracy: 63.65%\n",
      "Epoch: 170, Loss: 0.05, accuracy: 63.71%\n",
      "Epoch: 171, Loss: 0.05, accuracy: 63.78%\n",
      "Epoch: 172, Loss: 0.05, accuracy: 63.87%\n",
      "Epoch: 173, Loss: 0.05, accuracy: 63.98%\n",
      "Epoch: 174, Loss: 0.05, accuracy: 64.06%\n",
      "Epoch: 175, Loss: 0.05, accuracy: 64.14%\n",
      "Epoch: 176, Loss: 0.05, accuracy: 64.22%\n",
      "Epoch: 177, Loss: 0.05, accuracy: 64.29%\n",
      "Epoch: 178, Loss: 0.05, accuracy: 64.36%\n",
      "Epoch: 179, Loss: 0.05, accuracy: 64.45%\n",
      "Epoch: 180, Loss: 0.05, accuracy: 64.55%\n",
      "Epoch: 181, Loss: 0.05, accuracy: 64.66%\n",
      "Epoch: 182, Loss: 0.05, accuracy: 64.73%\n",
      "Epoch: 183, Loss: 0.05, accuracy: 64.81%\n",
      "Epoch: 184, Loss: 0.05, accuracy: 64.88%\n",
      "Epoch: 185, Loss: 0.05, accuracy: 64.92%\n",
      "Epoch: 186, Loss: 0.05, accuracy: 65.00%\n",
      "Epoch: 187, Loss: 0.05, accuracy: 65.08%\n",
      "Epoch: 188, Loss: 0.05, accuracy: 65.15%\n",
      "Epoch: 189, Loss: 0.05, accuracy: 65.23%\n",
      "Epoch: 190, Loss: 0.05, accuracy: 65.30%\n",
      "Epoch: 191, Loss: 0.05, accuracy: 65.37%\n",
      "Epoch: 192, Loss: 0.05, accuracy: 65.44%\n",
      "Epoch: 193, Loss: 0.05, accuracy: 65.53%\n",
      "Epoch: 194, Loss: 0.05, accuracy: 65.59%\n",
      "Epoch: 195, Loss: 0.05, accuracy: 65.66%\n",
      "Epoch: 196, Loss: 0.05, accuracy: 65.71%\n",
      "Epoch: 197, Loss: 0.05, accuracy: 65.79%\n",
      "Epoch: 198, Loss: 0.05, accuracy: 65.86%\n",
      "Epoch: 199, Loss: 0.05, accuracy: 65.95%\n",
      "Epoch: 200, Loss: 0.05, accuracy: 66.06%\n",
      "Epoch: 201, Loss: 0.05, accuracy: 66.13%\n",
      "Epoch: 202, Loss: 0.05, accuracy: 66.20%\n",
      "Epoch: 203, Loss: 0.05, accuracy: 66.25%\n",
      "Epoch: 204, Loss: 0.05, accuracy: 66.34%\n",
      "Epoch: 205, Loss: 0.05, accuracy: 66.41%\n",
      "Epoch: 206, Loss: 0.05, accuracy: 66.48%\n",
      "Epoch: 207, Loss: 0.05, accuracy: 66.55%\n",
      "Epoch: 208, Loss: 0.05, accuracy: 66.60%\n",
      "Epoch: 209, Loss: 0.05, accuracy: 66.69%\n",
      "Epoch: 210, Loss: 0.05, accuracy: 66.78%\n",
      "Epoch: 211, Loss: 0.05, accuracy: 66.85%\n",
      "Epoch: 212, Loss: 0.05, accuracy: 66.90%\n",
      "Epoch: 213, Loss: 0.05, accuracy: 66.96%\n",
      "Epoch: 214, Loss: 0.05, accuracy: 67.02%\n",
      "Epoch: 215, Loss: 0.05, accuracy: 67.08%\n",
      "Epoch: 216, Loss: 0.05, accuracy: 67.14%\n",
      "Epoch: 217, Loss: 0.05, accuracy: 67.20%\n",
      "Epoch: 218, Loss: 0.05, accuracy: 67.27%\n",
      "Epoch: 219, Loss: 0.05, accuracy: 67.33%\n",
      "Epoch: 220, Loss: 0.05, accuracy: 67.39%\n",
      "Epoch: 221, Loss: 0.05, accuracy: 67.45%\n",
      "Epoch: 222, Loss: 0.05, accuracy: 67.51%\n",
      "Epoch: 223, Loss: 0.05, accuracy: 67.56%\n",
      "Epoch: 224, Loss: 0.05, accuracy: 67.62%\n",
      "Epoch: 225, Loss: 0.05, accuracy: 67.68%\n",
      "Epoch: 226, Loss: 0.05, accuracy: 67.73%\n",
      "Epoch: 227, Loss: 0.05, accuracy: 67.79%\n",
      "Epoch: 228, Loss: 0.05, accuracy: 67.84%\n",
      "Epoch: 229, Loss: 0.05, accuracy: 67.87%\n",
      "Epoch: 230, Loss: 0.05, accuracy: 67.90%\n",
      "Epoch: 231, Loss: 0.05, accuracy: 67.96%\n",
      "Epoch: 232, Loss: 0.05, accuracy: 68.02%\n",
      "Epoch: 233, Loss: 0.05, accuracy: 68.06%\n",
      "Epoch: 234, Loss: 0.05, accuracy: 68.10%\n",
      "Epoch: 235, Loss: 0.05, accuracy: 68.17%\n",
      "Epoch: 236, Loss: 0.05, accuracy: 68.21%\n",
      "Epoch: 237, Loss: 0.05, accuracy: 68.26%\n",
      "Epoch: 238, Loss: 0.05, accuracy: 68.33%\n",
      "Epoch: 239, Loss: 0.05, accuracy: 68.39%\n",
      "Epoch: 240, Loss: 0.05, accuracy: 68.44%\n",
      "Epoch: 241, Loss: 0.05, accuracy: 68.52%\n",
      "Epoch: 242, Loss: 0.05, accuracy: 68.58%\n",
      "Epoch: 243, Loss: 0.05, accuracy: 68.65%\n",
      "Epoch: 244, Loss: 0.05, accuracy: 68.74%\n",
      "Epoch: 245, Loss: 0.05, accuracy: 68.85%\n",
      "Epoch: 246, Loss: 0.05, accuracy: 68.98%\n",
      "Epoch: 247, Loss: 0.05, accuracy: 69.14%\n",
      "Epoch: 248, Loss: 0.05, accuracy: 69.31%\n",
      "Epoch: 249, Loss: 0.05, accuracy: 69.50%\n",
      "Epoch: 250, Loss: 0.05, accuracy: 69.76%\n",
      "Epoch: 251, Loss: 0.05, accuracy: 70.10%\n",
      "Epoch: 252, Loss: 0.05, accuracy: 70.56%\n",
      "Epoch: 253, Loss: 0.05, accuracy: 71.10%\n",
      "Epoch: 254, Loss: 0.05, accuracy: 71.79%\n",
      "Epoch: 255, Loss: 0.05, accuracy: 72.43%\n",
      "Epoch: 256, Loss: 0.05, accuracy: 72.95%\n",
      "Epoch: 257, Loss: 0.04, accuracy: 73.33%\n",
      "Epoch: 258, Loss: 0.04, accuracy: 73.71%\n",
      "Epoch: 259, Loss: 0.04, accuracy: 73.98%\n",
      "Epoch: 260, Loss: 0.04, accuracy: 74.19%\n",
      "Epoch: 261, Loss: 0.04, accuracy: 74.38%\n",
      "Epoch: 262, Loss: 0.04, accuracy: 74.52%\n",
      "Epoch: 263, Loss: 0.04, accuracy: 74.67%\n",
      "Epoch: 264, Loss: 0.04, accuracy: 74.78%\n",
      "Epoch: 265, Loss: 0.04, accuracy: 74.90%\n",
      "Epoch: 266, Loss: 0.04, accuracy: 75.03%\n",
      "Epoch: 267, Loss: 0.04, accuracy: 75.13%\n",
      "Epoch: 268, Loss: 0.04, accuracy: 75.25%\n",
      "Epoch: 269, Loss: 0.04, accuracy: 75.35%\n",
      "Epoch: 270, Loss: 0.04, accuracy: 75.39%\n",
      "Epoch: 271, Loss: 0.04, accuracy: 75.47%\n",
      "Epoch: 272, Loss: 0.04, accuracy: 75.56%\n",
      "Epoch: 273, Loss: 0.04, accuracy: 75.67%\n",
      "Epoch: 274, Loss: 0.04, accuracy: 75.79%\n",
      "Epoch: 275, Loss: 0.04, accuracy: 75.84%\n",
      "Epoch: 276, Loss: 0.04, accuracy: 75.92%\n",
      "Epoch: 277, Loss: 0.04, accuracy: 76.03%\n",
      "Epoch: 278, Loss: 0.04, accuracy: 76.11%\n",
      "Epoch: 279, Loss: 0.04, accuracy: 76.16%\n",
      "Epoch: 280, Loss: 0.04, accuracy: 76.24%\n",
      "Epoch: 281, Loss: 0.04, accuracy: 76.29%\n",
      "Epoch: 282, Loss: 0.04, accuracy: 76.35%\n",
      "Epoch: 283, Loss: 0.04, accuracy: 76.41%\n",
      "Epoch: 284, Loss: 0.04, accuracy: 76.43%\n",
      "Epoch: 285, Loss: 0.04, accuracy: 76.48%\n",
      "Epoch: 286, Loss: 0.04, accuracy: 76.54%\n",
      "Epoch: 287, Loss: 0.04, accuracy: 76.59%\n",
      "Epoch: 288, Loss: 0.04, accuracy: 76.66%\n",
      "Epoch: 289, Loss: 0.04, accuracy: 76.72%\n",
      "Epoch: 290, Loss: 0.04, accuracy: 76.78%\n",
      "Epoch: 291, Loss: 0.04, accuracy: 76.84%\n",
      "Epoch: 292, Loss: 0.04, accuracy: 76.88%\n",
      "Epoch: 293, Loss: 0.04, accuracy: 76.91%\n",
      "Epoch: 294, Loss: 0.04, accuracy: 76.95%\n",
      "Epoch: 295, Loss: 0.04, accuracy: 77.01%\n",
      "Epoch: 296, Loss: 0.04, accuracy: 77.05%\n",
      "Epoch: 297, Loss: 0.04, accuracy: 77.09%\n",
      "Epoch: 298, Loss: 0.04, accuracy: 77.14%\n",
      "Epoch: 299, Loss: 0.04, accuracy: 77.19%\n",
      "Epoch: 300, Loss: 0.04, accuracy: 77.23%\n",
      "Epoch: 301, Loss: 0.04, accuracy: 77.27%\n",
      "Epoch: 302, Loss: 0.04, accuracy: 77.30%\n",
      "Epoch: 303, Loss: 0.04, accuracy: 77.33%\n",
      "Epoch: 304, Loss: 0.04, accuracy: 77.39%\n",
      "Epoch: 305, Loss: 0.04, accuracy: 77.42%\n",
      "Epoch: 306, Loss: 0.04, accuracy: 77.45%\n",
      "Epoch: 307, Loss: 0.04, accuracy: 77.48%\n",
      "Epoch: 308, Loss: 0.04, accuracy: 77.52%\n",
      "Epoch: 309, Loss: 0.04, accuracy: 77.58%\n",
      "Epoch: 310, Loss: 0.04, accuracy: 77.61%\n",
      "Epoch: 311, Loss: 0.04, accuracy: 77.63%\n",
      "Epoch: 312, Loss: 0.04, accuracy: 77.66%\n",
      "Epoch: 313, Loss: 0.04, accuracy: 77.68%\n",
      "Epoch: 314, Loss: 0.04, accuracy: 77.72%\n",
      "Epoch: 315, Loss: 0.04, accuracy: 77.75%\n",
      "Epoch: 316, Loss: 0.04, accuracy: 77.77%\n",
      "Epoch: 317, Loss: 0.04, accuracy: 77.80%\n",
      "Epoch: 318, Loss: 0.04, accuracy: 77.84%\n",
      "Epoch: 319, Loss: 0.04, accuracy: 77.87%\n",
      "Epoch: 320, Loss: 0.04, accuracy: 77.91%\n",
      "Epoch: 321, Loss: 0.04, accuracy: 77.92%\n",
      "Epoch: 322, Loss: 0.04, accuracy: 77.94%\n",
      "Epoch: 323, Loss: 0.04, accuracy: 77.97%\n",
      "Epoch: 324, Loss: 0.04, accuracy: 78.02%\n",
      "Epoch: 325, Loss: 0.04, accuracy: 78.05%\n",
      "Epoch: 326, Loss: 0.04, accuracy: 78.09%\n",
      "Epoch: 327, Loss: 0.04, accuracy: 78.13%\n",
      "Epoch: 328, Loss: 0.04, accuracy: 78.15%\n",
      "Epoch: 329, Loss: 0.04, accuracy: 78.20%\n",
      "Epoch: 330, Loss: 0.04, accuracy: 78.22%\n",
      "Epoch: 331, Loss: 0.04, accuracy: 78.26%\n",
      "Epoch: 332, Loss: 0.04, accuracy: 78.30%\n",
      "Epoch: 333, Loss: 0.04, accuracy: 78.33%\n",
      "Epoch: 334, Loss: 0.04, accuracy: 78.35%\n",
      "Epoch: 335, Loss: 0.04, accuracy: 78.36%\n",
      "Epoch: 336, Loss: 0.04, accuracy: 78.40%\n",
      "Epoch: 337, Loss: 0.04, accuracy: 78.42%\n",
      "Epoch: 338, Loss: 0.04, accuracy: 78.44%\n",
      "Epoch: 339, Loss: 0.04, accuracy: 78.46%\n",
      "Epoch: 340, Loss: 0.04, accuracy: 78.49%\n",
      "Epoch: 341, Loss: 0.04, accuracy: 78.52%\n",
      "Epoch: 342, Loss: 0.04, accuracy: 78.55%\n",
      "Epoch: 343, Loss: 0.04, accuracy: 78.56%\n",
      "Epoch: 344, Loss: 0.04, accuracy: 78.60%\n",
      "Epoch: 345, Loss: 0.04, accuracy: 78.63%\n",
      "Epoch: 346, Loss: 0.04, accuracy: 78.66%\n",
      "Epoch: 347, Loss: 0.04, accuracy: 78.68%\n",
      "Epoch: 348, Loss: 0.04, accuracy: 78.71%\n",
      "Epoch: 349, Loss: 0.04, accuracy: 78.74%\n",
      "Epoch: 350, Loss: 0.04, accuracy: 78.76%\n",
      "Epoch: 351, Loss: 0.04, accuracy: 78.79%\n",
      "Epoch: 352, Loss: 0.04, accuracy: 78.81%\n",
      "Epoch: 353, Loss: 0.04, accuracy: 78.84%\n",
      "Epoch: 354, Loss: 0.04, accuracy: 78.86%\n",
      "Epoch: 355, Loss: 0.04, accuracy: 78.88%\n",
      "Epoch: 356, Loss: 0.04, accuracy: 78.90%\n",
      "Epoch: 357, Loss: 0.04, accuracy: 78.93%\n",
      "Epoch: 358, Loss: 0.04, accuracy: 78.97%\n",
      "Epoch: 359, Loss: 0.04, accuracy: 78.99%\n",
      "Epoch: 360, Loss: 0.04, accuracy: 79.03%\n",
      "Epoch: 361, Loss: 0.04, accuracy: 79.04%\n",
      "Epoch: 362, Loss: 0.04, accuracy: 79.05%\n",
      "Epoch: 363, Loss: 0.04, accuracy: 79.08%\n",
      "Epoch: 364, Loss: 0.04, accuracy: 79.11%\n",
      "Epoch: 365, Loss: 0.04, accuracy: 79.13%\n",
      "Epoch: 366, Loss: 0.04, accuracy: 79.17%\n",
      "Epoch: 367, Loss: 0.04, accuracy: 79.18%\n",
      "Epoch: 368, Loss: 0.04, accuracy: 79.21%\n",
      "Epoch: 369, Loss: 0.04, accuracy: 79.22%\n",
      "Epoch: 370, Loss: 0.04, accuracy: 79.24%\n",
      "Epoch: 371, Loss: 0.04, accuracy: 79.25%\n",
      "Epoch: 372, Loss: 0.04, accuracy: 79.27%\n",
      "Epoch: 373, Loss: 0.04, accuracy: 79.29%\n",
      "Epoch: 374, Loss: 0.04, accuracy: 79.31%\n",
      "Epoch: 375, Loss: 0.04, accuracy: 79.33%\n",
      "Epoch: 376, Loss: 0.04, accuracy: 79.35%\n",
      "Epoch: 377, Loss: 0.04, accuracy: 79.38%\n",
      "Epoch: 378, Loss: 0.04, accuracy: 79.39%\n",
      "Epoch: 379, Loss: 0.04, accuracy: 79.41%\n",
      "Epoch: 380, Loss: 0.04, accuracy: 79.43%\n",
      "Epoch: 381, Loss: 0.04, accuracy: 79.46%\n",
      "Epoch: 382, Loss: 0.04, accuracy: 79.47%\n",
      "Epoch: 383, Loss: 0.04, accuracy: 79.49%\n",
      "Epoch: 384, Loss: 0.04, accuracy: 79.50%\n",
      "Epoch: 385, Loss: 0.04, accuracy: 79.53%\n",
      "Epoch: 386, Loss: 0.04, accuracy: 79.56%\n",
      "Epoch: 387, Loss: 0.04, accuracy: 79.57%\n",
      "Epoch: 388, Loss: 0.04, accuracy: 79.61%\n",
      "Epoch: 389, Loss: 0.04, accuracy: 79.64%\n",
      "Epoch: 390, Loss: 0.04, accuracy: 79.65%\n",
      "Epoch: 391, Loss: 0.04, accuracy: 79.66%\n",
      "Epoch: 392, Loss: 0.04, accuracy: 79.68%\n",
      "Epoch: 393, Loss: 0.04, accuracy: 79.69%\n",
      "Epoch: 394, Loss: 0.04, accuracy: 79.72%\n",
      "Epoch: 395, Loss: 0.04, accuracy: 79.74%\n",
      "Epoch: 396, Loss: 0.04, accuracy: 79.76%\n",
      "Epoch: 397, Loss: 0.04, accuracy: 79.77%\n",
      "Epoch: 398, Loss: 0.04, accuracy: 79.79%\n",
      "Epoch: 399, Loss: 0.04, accuracy: 79.82%\n",
      "Epoch: 400, Loss: 0.04, accuracy: 79.83%\n",
      "Epoch: 401, Loss: 0.04, accuracy: 79.85%\n",
      "Epoch: 402, Loss: 0.04, accuracy: 79.86%\n",
      "Epoch: 403, Loss: 0.04, accuracy: 79.89%\n",
      "Epoch: 404, Loss: 0.04, accuracy: 79.92%\n",
      "Epoch: 405, Loss: 0.04, accuracy: 79.94%\n",
      "Epoch: 406, Loss: 0.04, accuracy: 79.95%\n",
      "Epoch: 407, Loss: 0.04, accuracy: 79.97%\n",
      "Epoch: 408, Loss: 0.04, accuracy: 79.97%\n",
      "Epoch: 409, Loss: 0.04, accuracy: 79.99%\n",
      "Epoch: 410, Loss: 0.04, accuracy: 80.02%\n",
      "Epoch: 411, Loss: 0.04, accuracy: 80.04%\n",
      "Epoch: 412, Loss: 0.04, accuracy: 80.07%\n",
      "Epoch: 413, Loss: 0.04, accuracy: 80.09%\n",
      "Epoch: 414, Loss: 0.03, accuracy: 80.11%\n",
      "Epoch: 415, Loss: 0.03, accuracy: 80.14%\n",
      "Epoch: 416, Loss: 0.03, accuracy: 80.15%\n",
      "Epoch: 417, Loss: 0.03, accuracy: 80.16%\n",
      "Epoch: 418, Loss: 0.03, accuracy: 80.17%\n",
      "Epoch: 419, Loss: 0.03, accuracy: 80.19%\n",
      "Epoch: 420, Loss: 0.03, accuracy: 80.20%\n",
      "Epoch: 421, Loss: 0.03, accuracy: 80.22%\n",
      "Epoch: 422, Loss: 0.03, accuracy: 80.22%\n",
      "Epoch: 423, Loss: 0.03, accuracy: 80.24%\n",
      "Epoch: 424, Loss: 0.03, accuracy: 80.25%\n",
      "Epoch: 425, Loss: 0.03, accuracy: 80.25%\n",
      "Epoch: 426, Loss: 0.03, accuracy: 80.26%\n",
      "Epoch: 427, Loss: 0.03, accuracy: 80.29%\n",
      "Epoch: 428, Loss: 0.03, accuracy: 80.31%\n",
      "Epoch: 429, Loss: 0.03, accuracy: 80.33%\n",
      "Epoch: 430, Loss: 0.03, accuracy: 80.33%\n",
      "Epoch: 431, Loss: 0.03, accuracy: 80.35%\n",
      "Epoch: 432, Loss: 0.03, accuracy: 80.36%\n",
      "Epoch: 433, Loss: 0.03, accuracy: 80.38%\n",
      "Epoch: 434, Loss: 0.03, accuracy: 80.40%\n",
      "Epoch: 435, Loss: 0.03, accuracy: 80.41%\n",
      "Epoch: 436, Loss: 0.03, accuracy: 80.42%\n",
      "Epoch: 437, Loss: 0.03, accuracy: 80.43%\n",
      "Epoch: 438, Loss: 0.03, accuracy: 80.44%\n",
      "Epoch: 439, Loss: 0.03, accuracy: 80.46%\n",
      "Epoch: 440, Loss: 0.03, accuracy: 80.46%\n",
      "Epoch: 441, Loss: 0.03, accuracy: 80.49%\n",
      "Epoch: 442, Loss: 0.03, accuracy: 80.51%\n",
      "Epoch: 443, Loss: 0.03, accuracy: 80.52%\n",
      "Epoch: 444, Loss: 0.03, accuracy: 80.53%\n",
      "Epoch: 445, Loss: 0.03, accuracy: 80.55%\n",
      "Epoch: 446, Loss: 0.03, accuracy: 80.56%\n",
      "Epoch: 447, Loss: 0.03, accuracy: 80.58%\n",
      "Epoch: 448, Loss: 0.03, accuracy: 80.60%\n",
      "Epoch: 449, Loss: 0.03, accuracy: 80.60%\n",
      "Epoch: 450, Loss: 0.03, accuracy: 80.61%\n",
      "Epoch: 451, Loss: 0.03, accuracy: 80.62%\n",
      "Epoch: 452, Loss: 0.03, accuracy: 80.63%\n",
      "Epoch: 453, Loss: 0.03, accuracy: 80.64%\n",
      "Epoch: 454, Loss: 0.03, accuracy: 80.65%\n",
      "Epoch: 455, Loss: 0.03, accuracy: 80.67%\n",
      "Epoch: 456, Loss: 0.03, accuracy: 80.68%\n",
      "Epoch: 457, Loss: 0.03, accuracy: 80.70%\n",
      "Epoch: 458, Loss: 0.03, accuracy: 80.72%\n",
      "Epoch: 459, Loss: 0.03, accuracy: 80.73%\n",
      "Epoch: 460, Loss: 0.03, accuracy: 80.74%\n",
      "Epoch: 461, Loss: 0.03, accuracy: 80.76%\n",
      "Epoch: 462, Loss: 0.03, accuracy: 80.77%\n",
      "Epoch: 463, Loss: 0.03, accuracy: 80.79%\n",
      "Epoch: 464, Loss: 0.03, accuracy: 80.80%\n",
      "Epoch: 465, Loss: 0.03, accuracy: 80.82%\n",
      "Epoch: 466, Loss: 0.03, accuracy: 80.84%\n",
      "Epoch: 467, Loss: 0.03, accuracy: 80.85%\n",
      "Epoch: 468, Loss: 0.03, accuracy: 80.87%\n",
      "Epoch: 469, Loss: 0.03, accuracy: 80.89%\n",
      "Epoch: 470, Loss: 0.03, accuracy: 80.91%\n",
      "Epoch: 471, Loss: 0.03, accuracy: 80.92%\n",
      "Epoch: 472, Loss: 0.03, accuracy: 80.93%\n",
      "Epoch: 473, Loss: 0.03, accuracy: 80.94%\n",
      "Epoch: 474, Loss: 0.03, accuracy: 80.96%\n",
      "Epoch: 475, Loss: 0.03, accuracy: 80.96%\n",
      "Epoch: 476, Loss: 0.03, accuracy: 80.97%\n",
      "Epoch: 477, Loss: 0.03, accuracy: 80.98%\n",
      "Epoch: 478, Loss: 0.03, accuracy: 80.99%\n",
      "Epoch: 479, Loss: 0.03, accuracy: 81.01%\n",
      "Epoch: 480, Loss: 0.03, accuracy: 81.03%\n",
      "Epoch: 481, Loss: 0.03, accuracy: 81.05%\n",
      "Epoch: 482, Loss: 0.03, accuracy: 81.05%\n",
      "Epoch: 483, Loss: 0.03, accuracy: 81.07%\n",
      "Epoch: 484, Loss: 0.03, accuracy: 81.08%\n",
      "Epoch: 485, Loss: 0.03, accuracy: 81.09%\n",
      "Epoch: 486, Loss: 0.03, accuracy: 81.10%\n",
      "Epoch: 487, Loss: 0.03, accuracy: 81.12%\n",
      "Epoch: 488, Loss: 0.03, accuracy: 81.14%\n",
      "Epoch: 489, Loss: 0.03, accuracy: 81.15%\n",
      "Epoch: 490, Loss: 0.03, accuracy: 81.16%\n",
      "Epoch: 491, Loss: 0.03, accuracy: 81.17%\n",
      "Epoch: 492, Loss: 0.03, accuracy: 81.18%\n",
      "Epoch: 493, Loss: 0.03, accuracy: 81.19%\n",
      "Epoch: 494, Loss: 0.03, accuracy: 81.21%\n",
      "Epoch: 495, Loss: 0.03, accuracy: 81.22%\n",
      "Epoch: 496, Loss: 0.03, accuracy: 81.23%\n",
      "Epoch: 497, Loss: 0.03, accuracy: 81.24%\n",
      "Epoch: 498, Loss: 0.03, accuracy: 81.26%\n",
      "Epoch: 499, Loss: 0.03, accuracy: 81.27%\n",
      "Epoch: 500, Loss: 0.03, accuracy: 81.28%\n",
      "Epoch: 501, Loss: 0.03, accuracy: 81.29%\n",
      "Epoch: 502, Loss: 0.03, accuracy: 81.31%\n",
      "Epoch: 503, Loss: 0.03, accuracy: 81.31%\n",
      "Epoch: 504, Loss: 0.03, accuracy: 81.33%\n",
      "Epoch: 505, Loss: 0.03, accuracy: 81.34%\n",
      "Epoch: 506, Loss: 0.03, accuracy: 81.35%\n",
      "Epoch: 507, Loss: 0.03, accuracy: 81.36%\n",
      "Epoch: 508, Loss: 0.03, accuracy: 81.36%\n",
      "Epoch: 509, Loss: 0.03, accuracy: 81.38%\n",
      "Epoch: 510, Loss: 0.03, accuracy: 81.39%\n",
      "Epoch: 511, Loss: 0.03, accuracy: 81.40%\n",
      "Epoch: 512, Loss: 0.03, accuracy: 81.42%\n",
      "Epoch: 513, Loss: 0.03, accuracy: 81.42%\n",
      "Epoch: 514, Loss: 0.03, accuracy: 81.42%\n",
      "Epoch: 515, Loss: 0.03, accuracy: 81.44%\n",
      "Epoch: 516, Loss: 0.03, accuracy: 81.45%\n",
      "Epoch: 517, Loss: 0.03, accuracy: 81.47%\n",
      "Epoch: 518, Loss: 0.03, accuracy: 81.50%\n",
      "Epoch: 519, Loss: 0.03, accuracy: 81.51%\n",
      "Epoch: 520, Loss: 0.03, accuracy: 81.53%\n",
      "Epoch: 521, Loss: 0.03, accuracy: 81.54%\n",
      "Epoch: 522, Loss: 0.03, accuracy: 81.55%\n",
      "Epoch: 523, Loss: 0.03, accuracy: 81.56%\n",
      "Epoch: 524, Loss: 0.03, accuracy: 81.57%\n",
      "Epoch: 525, Loss: 0.03, accuracy: 81.57%\n",
      "Epoch: 526, Loss: 0.03, accuracy: 81.58%\n",
      "Epoch: 527, Loss: 0.03, accuracy: 81.59%\n",
      "Epoch: 528, Loss: 0.03, accuracy: 81.60%\n",
      "Epoch: 529, Loss: 0.03, accuracy: 81.60%\n",
      "Epoch: 530, Loss: 0.03, accuracy: 81.62%\n",
      "Epoch: 531, Loss: 0.03, accuracy: 81.63%\n",
      "Epoch: 532, Loss: 0.03, accuracy: 81.64%\n",
      "Epoch: 533, Loss: 0.03, accuracy: 81.64%\n",
      "Epoch: 534, Loss: 0.03, accuracy: 81.65%\n",
      "Epoch: 535, Loss: 0.03, accuracy: 81.67%\n",
      "Epoch: 536, Loss: 0.03, accuracy: 81.69%\n",
      "Epoch: 537, Loss: 0.03, accuracy: 81.72%\n",
      "Epoch: 538, Loss: 0.03, accuracy: 81.72%\n",
      "Epoch: 539, Loss: 0.03, accuracy: 81.74%\n",
      "Epoch: 540, Loss: 0.03, accuracy: 81.75%\n",
      "Epoch: 541, Loss: 0.03, accuracy: 81.75%\n",
      "Epoch: 542, Loss: 0.03, accuracy: 81.76%\n",
      "Epoch: 543, Loss: 0.03, accuracy: 81.77%\n",
      "Epoch: 544, Loss: 0.03, accuracy: 81.78%\n",
      "Epoch: 545, Loss: 0.03, accuracy: 81.79%\n",
      "Epoch: 546, Loss: 0.03, accuracy: 81.81%\n",
      "Epoch: 547, Loss: 0.03, accuracy: 81.83%\n",
      "Epoch: 548, Loss: 0.03, accuracy: 81.85%\n",
      "Epoch: 549, Loss: 0.03, accuracy: 81.85%\n",
      "Epoch: 550, Loss: 0.03, accuracy: 81.87%\n",
      "Epoch: 551, Loss: 0.03, accuracy: 81.88%\n",
      "Epoch: 552, Loss: 0.03, accuracy: 81.89%\n",
      "Epoch: 553, Loss: 0.03, accuracy: 81.90%\n",
      "Epoch: 554, Loss: 0.03, accuracy: 81.92%\n",
      "Epoch: 555, Loss: 0.03, accuracy: 81.93%\n",
      "Epoch: 556, Loss: 0.03, accuracy: 81.94%\n",
      "Epoch: 557, Loss: 0.03, accuracy: 81.95%\n",
      "Epoch: 558, Loss: 0.03, accuracy: 81.96%\n",
      "Epoch: 559, Loss: 0.03, accuracy: 81.97%\n",
      "Epoch: 560, Loss: 0.03, accuracy: 81.98%\n",
      "Epoch: 561, Loss: 0.03, accuracy: 81.99%\n",
      "Epoch: 562, Loss: 0.03, accuracy: 82.00%\n",
      "Epoch: 563, Loss: 0.03, accuracy: 82.02%\n",
      "Epoch: 564, Loss: 0.03, accuracy: 82.02%\n",
      "Epoch: 565, Loss: 0.03, accuracy: 82.03%\n",
      "Epoch: 566, Loss: 0.03, accuracy: 82.03%\n",
      "Epoch: 567, Loss: 0.03, accuracy: 82.04%\n",
      "Epoch: 568, Loss: 0.03, accuracy: 82.04%\n",
      "Epoch: 569, Loss: 0.03, accuracy: 82.06%\n",
      "Epoch: 570, Loss: 0.03, accuracy: 82.06%\n",
      "Epoch: 571, Loss: 0.03, accuracy: 82.08%\n",
      "Epoch: 572, Loss: 0.03, accuracy: 82.09%\n",
      "Epoch: 573, Loss: 0.03, accuracy: 82.11%\n",
      "Epoch: 574, Loss: 0.03, accuracy: 82.12%\n",
      "Epoch: 575, Loss: 0.03, accuracy: 82.13%\n",
      "Epoch: 576, Loss: 0.03, accuracy: 82.14%\n",
      "Epoch: 577, Loss: 0.03, accuracy: 82.15%\n",
      "Epoch: 578, Loss: 0.03, accuracy: 82.16%\n",
      "Epoch: 579, Loss: 0.03, accuracy: 82.17%\n",
      "Epoch: 580, Loss: 0.03, accuracy: 82.18%\n",
      "Epoch: 581, Loss: 0.03, accuracy: 82.19%\n",
      "Epoch: 582, Loss: 0.03, accuracy: 82.20%\n",
      "Epoch: 583, Loss: 0.03, accuracy: 82.21%\n",
      "Epoch: 584, Loss: 0.03, accuracy: 82.22%\n",
      "Epoch: 585, Loss: 0.03, accuracy: 82.22%\n",
      "Epoch: 586, Loss: 0.03, accuracy: 82.22%\n",
      "Epoch: 587, Loss: 0.03, accuracy: 82.22%\n",
      "Epoch: 588, Loss: 0.03, accuracy: 82.24%\n",
      "Epoch: 589, Loss: 0.03, accuracy: 82.26%\n",
      "Epoch: 590, Loss: 0.03, accuracy: 82.28%\n",
      "Epoch: 591, Loss: 0.03, accuracy: 82.29%\n",
      "Epoch: 592, Loss: 0.03, accuracy: 82.30%\n",
      "Epoch: 593, Loss: 0.03, accuracy: 82.32%\n",
      "Epoch: 594, Loss: 0.03, accuracy: 82.33%\n",
      "Epoch: 595, Loss: 0.03, accuracy: 82.35%\n",
      "Epoch: 596, Loss: 0.03, accuracy: 82.35%\n",
      "Epoch: 597, Loss: 0.03, accuracy: 82.36%\n",
      "Epoch: 598, Loss: 0.03, accuracy: 82.36%\n",
      "Epoch: 599, Loss: 0.03, accuracy: 82.37%\n",
      "Epoch: 600, Loss: 0.03, accuracy: 82.39%\n",
      "Epoch: 601, Loss: 0.03, accuracy: 82.40%\n",
      "Epoch: 602, Loss: 0.03, accuracy: 82.40%\n",
      "Epoch: 603, Loss: 0.03, accuracy: 82.40%\n",
      "Epoch: 604, Loss: 0.03, accuracy: 82.41%\n",
      "Epoch: 605, Loss: 0.03, accuracy: 82.42%\n",
      "Epoch: 606, Loss: 0.03, accuracy: 82.44%\n",
      "Epoch: 607, Loss: 0.03, accuracy: 82.46%\n",
      "Epoch: 608, Loss: 0.03, accuracy: 82.48%\n",
      "Epoch: 609, Loss: 0.03, accuracy: 82.48%\n",
      "Epoch: 610, Loss: 0.03, accuracy: 82.49%\n",
      "Epoch: 611, Loss: 0.03, accuracy: 82.50%\n",
      "Epoch: 612, Loss: 0.03, accuracy: 82.51%\n",
      "Epoch: 613, Loss: 0.03, accuracy: 82.53%\n",
      "Epoch: 614, Loss: 0.03, accuracy: 82.53%\n",
      "Epoch: 615, Loss: 0.03, accuracy: 82.54%\n",
      "Epoch: 616, Loss: 0.03, accuracy: 82.55%\n",
      "Epoch: 617, Loss: 0.03, accuracy: 82.57%\n",
      "Epoch: 618, Loss: 0.03, accuracy: 82.58%\n",
      "Epoch: 619, Loss: 0.03, accuracy: 82.59%\n",
      "Epoch: 620, Loss: 0.03, accuracy: 82.60%\n",
      "Epoch: 621, Loss: 0.03, accuracy: 82.60%\n",
      "Epoch: 622, Loss: 0.03, accuracy: 82.61%\n",
      "Epoch: 623, Loss: 0.03, accuracy: 82.63%\n",
      "Epoch: 624, Loss: 0.03, accuracy: 82.64%\n",
      "Epoch: 625, Loss: 0.03, accuracy: 82.65%\n",
      "Epoch: 626, Loss: 0.03, accuracy: 82.65%\n",
      "Epoch: 627, Loss: 0.03, accuracy: 82.66%\n",
      "Epoch: 628, Loss: 0.03, accuracy: 82.67%\n",
      "Epoch: 629, Loss: 0.03, accuracy: 82.69%\n",
      "Epoch: 630, Loss: 0.03, accuracy: 82.69%\n",
      "Epoch: 631, Loss: 0.03, accuracy: 82.71%\n",
      "Epoch: 632, Loss: 0.03, accuracy: 82.72%\n",
      "Epoch: 633, Loss: 0.03, accuracy: 82.73%\n",
      "Epoch: 634, Loss: 0.03, accuracy: 82.74%\n",
      "Epoch: 635, Loss: 0.03, accuracy: 82.75%\n",
      "Epoch: 636, Loss: 0.03, accuracy: 82.77%\n",
      "Epoch: 637, Loss: 0.03, accuracy: 82.78%\n",
      "Epoch: 638, Loss: 0.03, accuracy: 82.79%\n",
      "Epoch: 639, Loss: 0.03, accuracy: 82.79%\n",
      "Epoch: 640, Loss: 0.03, accuracy: 82.80%\n",
      "Epoch: 641, Loss: 0.03, accuracy: 82.82%\n",
      "Epoch: 642, Loss: 0.03, accuracy: 82.82%\n",
      "Epoch: 643, Loss: 0.03, accuracy: 82.84%\n",
      "Epoch: 644, Loss: 0.03, accuracy: 82.84%\n",
      "Epoch: 645, Loss: 0.03, accuracy: 82.85%\n",
      "Epoch: 646, Loss: 0.03, accuracy: 82.86%\n",
      "Epoch: 647, Loss: 0.03, accuracy: 82.87%\n",
      "Epoch: 648, Loss: 0.03, accuracy: 82.89%\n",
      "Epoch: 649, Loss: 0.03, accuracy: 82.89%\n",
      "Epoch: 650, Loss: 0.03, accuracy: 82.89%\n",
      "Epoch: 651, Loss: 0.03, accuracy: 82.90%\n",
      "Epoch: 652, Loss: 0.03, accuracy: 82.92%\n",
      "Epoch: 653, Loss: 0.03, accuracy: 82.92%\n",
      "Epoch: 654, Loss: 0.03, accuracy: 82.92%\n",
      "Epoch: 655, Loss: 0.03, accuracy: 82.93%\n",
      "Epoch: 656, Loss: 0.03, accuracy: 82.94%\n",
      "Epoch: 657, Loss: 0.03, accuracy: 82.94%\n",
      "Epoch: 658, Loss: 0.03, accuracy: 82.96%\n",
      "Epoch: 659, Loss: 0.03, accuracy: 82.97%\n",
      "Epoch: 660, Loss: 0.03, accuracy: 82.98%\n",
      "Epoch: 661, Loss: 0.03, accuracy: 83.00%\n",
      "Epoch: 662, Loss: 0.03, accuracy: 83.01%\n",
      "Epoch: 663, Loss: 0.03, accuracy: 83.02%\n",
      "Epoch: 664, Loss: 0.03, accuracy: 83.04%\n",
      "Epoch: 665, Loss: 0.03, accuracy: 83.05%\n",
      "Epoch: 666, Loss: 0.03, accuracy: 83.05%\n",
      "Epoch: 667, Loss: 0.03, accuracy: 83.06%\n",
      "Epoch: 668, Loss: 0.03, accuracy: 83.07%\n",
      "Epoch: 669, Loss: 0.03, accuracy: 83.08%\n",
      "Epoch: 670, Loss: 0.03, accuracy: 83.09%\n",
      "Epoch: 671, Loss: 0.03, accuracy: 83.10%\n",
      "Epoch: 672, Loss: 0.03, accuracy: 83.10%\n",
      "Epoch: 673, Loss: 0.03, accuracy: 83.11%\n",
      "Epoch: 674, Loss: 0.03, accuracy: 83.11%\n",
      "Epoch: 675, Loss: 0.03, accuracy: 83.12%\n",
      "Epoch: 676, Loss: 0.03, accuracy: 83.14%\n",
      "Epoch: 677, Loss: 0.03, accuracy: 83.15%\n",
      "Epoch: 678, Loss: 0.03, accuracy: 83.15%\n",
      "Epoch: 679, Loss: 0.03, accuracy: 83.16%\n",
      "Epoch: 680, Loss: 0.03, accuracy: 83.17%\n",
      "Epoch: 681, Loss: 0.03, accuracy: 83.17%\n",
      "Epoch: 682, Loss: 0.03, accuracy: 83.18%\n",
      "Epoch: 683, Loss: 0.03, accuracy: 83.19%\n",
      "Epoch: 684, Loss: 0.03, accuracy: 83.19%\n",
      "Epoch: 685, Loss: 0.03, accuracy: 83.19%\n",
      "Epoch: 686, Loss: 0.03, accuracy: 83.20%\n",
      "Epoch: 687, Loss: 0.03, accuracy: 83.20%\n",
      "Epoch: 688, Loss: 0.03, accuracy: 83.21%\n",
      "Epoch: 689, Loss: 0.03, accuracy: 83.21%\n",
      "Epoch: 690, Loss: 0.03, accuracy: 83.23%\n",
      "Epoch: 691, Loss: 0.03, accuracy: 83.24%\n",
      "Epoch: 692, Loss: 0.03, accuracy: 83.26%\n",
      "Epoch: 693, Loss: 0.03, accuracy: 83.27%\n",
      "Epoch: 694, Loss: 0.03, accuracy: 83.28%\n",
      "Epoch: 695, Loss: 0.03, accuracy: 83.29%\n",
      "Epoch: 696, Loss: 0.03, accuracy: 83.29%\n",
      "Epoch: 697, Loss: 0.03, accuracy: 83.29%\n",
      "Epoch: 698, Loss: 0.03, accuracy: 83.30%\n",
      "Epoch: 699, Loss: 0.03, accuracy: 83.31%\n",
      "Epoch: 700, Loss: 0.03, accuracy: 83.32%\n",
      "Epoch: 701, Loss: 0.03, accuracy: 83.34%\n",
      "Epoch: 702, Loss: 0.03, accuracy: 83.35%\n",
      "Epoch: 703, Loss: 0.03, accuracy: 83.35%\n",
      "Epoch: 704, Loss: 0.03, accuracy: 83.36%\n",
      "Epoch: 705, Loss: 0.03, accuracy: 83.36%\n",
      "Epoch: 706, Loss: 0.03, accuracy: 83.36%\n",
      "Epoch: 707, Loss: 0.03, accuracy: 83.37%\n",
      "Epoch: 708, Loss: 0.03, accuracy: 83.38%\n",
      "Epoch: 709, Loss: 0.03, accuracy: 83.40%\n",
      "Epoch: 710, Loss: 0.03, accuracy: 83.41%\n",
      "Epoch: 711, Loss: 0.03, accuracy: 83.42%\n",
      "Epoch: 712, Loss: 0.03, accuracy: 83.43%\n",
      "Epoch: 713, Loss: 0.03, accuracy: 83.43%\n",
      "Epoch: 714, Loss: 0.03, accuracy: 83.44%\n",
      "Epoch: 715, Loss: 0.03, accuracy: 83.45%\n",
      "Epoch: 716, Loss: 0.03, accuracy: 83.46%\n",
      "Epoch: 717, Loss: 0.03, accuracy: 83.47%\n",
      "Epoch: 718, Loss: 0.03, accuracy: 83.47%\n",
      "Epoch: 719, Loss: 0.03, accuracy: 83.48%\n",
      "Epoch: 720, Loss: 0.03, accuracy: 83.48%\n",
      "Epoch: 721, Loss: 0.03, accuracy: 83.49%\n",
      "Epoch: 722, Loss: 0.03, accuracy: 83.49%\n",
      "Epoch: 723, Loss: 0.03, accuracy: 83.50%\n",
      "Epoch: 724, Loss: 0.03, accuracy: 83.51%\n",
      "Epoch: 725, Loss: 0.03, accuracy: 83.52%\n",
      "Epoch: 726, Loss: 0.03, accuracy: 83.54%\n",
      "Epoch: 727, Loss: 0.03, accuracy: 83.54%\n",
      "Epoch: 728, Loss: 0.03, accuracy: 83.56%\n",
      "Epoch: 729, Loss: 0.03, accuracy: 83.57%\n",
      "Epoch: 730, Loss: 0.03, accuracy: 83.57%\n",
      "Epoch: 731, Loss: 0.03, accuracy: 83.58%\n",
      "Epoch: 732, Loss: 0.03, accuracy: 83.58%\n",
      "Epoch: 733, Loss: 0.03, accuracy: 83.58%\n",
      "Epoch: 734, Loss: 0.03, accuracy: 83.59%\n",
      "Epoch: 735, Loss: 0.03, accuracy: 83.59%\n",
      "Epoch: 736, Loss: 0.03, accuracy: 83.60%\n",
      "Epoch: 737, Loss: 0.03, accuracy: 83.61%\n",
      "Epoch: 738, Loss: 0.03, accuracy: 83.62%\n",
      "Epoch: 739, Loss: 0.03, accuracy: 83.62%\n",
      "Epoch: 740, Loss: 0.03, accuracy: 83.62%\n",
      "Epoch: 741, Loss: 0.03, accuracy: 83.63%\n",
      "Epoch: 742, Loss: 0.03, accuracy: 83.64%\n",
      "Epoch: 743, Loss: 0.03, accuracy: 83.64%\n",
      "Epoch: 744, Loss: 0.03, accuracy: 83.65%\n",
      "Epoch: 745, Loss: 0.03, accuracy: 83.65%\n",
      "Epoch: 746, Loss: 0.03, accuracy: 83.66%\n",
      "Epoch: 747, Loss: 0.03, accuracy: 83.67%\n",
      "Epoch: 748, Loss: 0.03, accuracy: 83.68%\n",
      "Epoch: 749, Loss: 0.03, accuracy: 83.69%\n",
      "Epoch: 750, Loss: 0.03, accuracy: 83.69%\n",
      "Epoch: 751, Loss: 0.03, accuracy: 83.70%\n",
      "Epoch: 752, Loss: 0.03, accuracy: 83.71%\n",
      "Epoch: 753, Loss: 0.03, accuracy: 83.71%\n",
      "Epoch: 754, Loss: 0.03, accuracy: 83.71%\n",
      "Epoch: 755, Loss: 0.03, accuracy: 83.71%\n",
      "Epoch: 756, Loss: 0.03, accuracy: 83.72%\n",
      "Epoch: 757, Loss: 0.03, accuracy: 83.73%\n",
      "Epoch: 758, Loss: 0.03, accuracy: 83.73%\n",
      "Epoch: 759, Loss: 0.03, accuracy: 83.74%\n",
      "Epoch: 760, Loss: 0.03, accuracy: 83.74%\n",
      "Epoch: 761, Loss: 0.03, accuracy: 83.74%\n",
      "Epoch: 762, Loss: 0.03, accuracy: 83.75%\n",
      "Epoch: 763, Loss: 0.03, accuracy: 83.75%\n",
      "Epoch: 764, Loss: 0.03, accuracy: 83.75%\n",
      "Epoch: 765, Loss: 0.03, accuracy: 83.75%\n",
      "Epoch: 766, Loss: 0.03, accuracy: 83.77%\n",
      "Epoch: 767, Loss: 0.03, accuracy: 83.78%\n",
      "Epoch: 768, Loss: 0.03, accuracy: 83.78%\n",
      "Epoch: 769, Loss: 0.03, accuracy: 83.79%\n",
      "Epoch: 770, Loss: 0.03, accuracy: 83.79%\n",
      "Epoch: 771, Loss: 0.03, accuracy: 83.80%\n",
      "Epoch: 772, Loss: 0.03, accuracy: 83.81%\n",
      "Epoch: 773, Loss: 0.03, accuracy: 83.81%\n",
      "Epoch: 774, Loss: 0.03, accuracy: 83.83%\n",
      "Epoch: 775, Loss: 0.03, accuracy: 83.83%\n",
      "Epoch: 776, Loss: 0.03, accuracy: 83.84%\n",
      "Epoch: 777, Loss: 0.03, accuracy: 83.85%\n",
      "Epoch: 778, Loss: 0.03, accuracy: 83.85%\n",
      "Epoch: 779, Loss: 0.03, accuracy: 83.86%\n",
      "Epoch: 780, Loss: 0.03, accuracy: 83.87%\n",
      "Epoch: 781, Loss: 0.03, accuracy: 83.88%\n",
      "Epoch: 782, Loss: 0.03, accuracy: 83.88%\n",
      "Epoch: 783, Loss: 0.03, accuracy: 83.89%\n",
      "Epoch: 784, Loss: 0.03, accuracy: 83.90%\n",
      "Epoch: 785, Loss: 0.03, accuracy: 83.90%\n",
      "Epoch: 786, Loss: 0.03, accuracy: 83.90%\n",
      "Epoch: 787, Loss: 0.03, accuracy: 83.91%\n",
      "Epoch: 788, Loss: 0.03, accuracy: 83.92%\n",
      "Epoch: 789, Loss: 0.03, accuracy: 83.93%\n",
      "Epoch: 790, Loss: 0.03, accuracy: 83.94%\n",
      "Epoch: 791, Loss: 0.03, accuracy: 83.94%\n",
      "Epoch: 792, Loss: 0.03, accuracy: 83.94%\n",
      "Epoch: 793, Loss: 0.03, accuracy: 83.95%\n",
      "Epoch: 794, Loss: 0.03, accuracy: 83.95%\n",
      "Epoch: 795, Loss: 0.03, accuracy: 83.96%\n",
      "Epoch: 796, Loss: 0.03, accuracy: 83.97%\n",
      "Epoch: 797, Loss: 0.03, accuracy: 83.97%\n",
      "Epoch: 798, Loss: 0.03, accuracy: 83.97%\n",
      "Epoch: 799, Loss: 0.03, accuracy: 83.97%\n",
      "Epoch: 800, Loss: 0.03, accuracy: 83.99%\n",
      "Epoch: 801, Loss: 0.03, accuracy: 83.99%\n",
      "Epoch: 802, Loss: 0.03, accuracy: 83.99%\n",
      "Epoch: 803, Loss: 0.03, accuracy: 84.00%\n",
      "Epoch: 804, Loss: 0.03, accuracy: 84.00%\n",
      "Epoch: 805, Loss: 0.03, accuracy: 84.00%\n",
      "Epoch: 806, Loss: 0.03, accuracy: 84.00%\n",
      "Epoch: 807, Loss: 0.03, accuracy: 84.00%\n",
      "Epoch: 808, Loss: 0.03, accuracy: 84.01%\n",
      "Epoch: 809, Loss: 0.03, accuracy: 84.01%\n",
      "Epoch: 810, Loss: 0.03, accuracy: 84.03%\n",
      "Epoch: 811, Loss: 0.03, accuracy: 84.03%\n",
      "Epoch: 812, Loss: 0.03, accuracy: 84.04%\n",
      "Epoch: 813, Loss: 0.03, accuracy: 84.05%\n",
      "Epoch: 814, Loss: 0.03, accuracy: 84.06%\n",
      "Epoch: 815, Loss: 0.03, accuracy: 84.07%\n",
      "Epoch: 816, Loss: 0.03, accuracy: 84.08%\n",
      "Epoch: 817, Loss: 0.03, accuracy: 84.09%\n",
      "Epoch: 818, Loss: 0.03, accuracy: 84.09%\n",
      "Epoch: 819, Loss: 0.03, accuracy: 84.10%\n",
      "Epoch: 820, Loss: 0.03, accuracy: 84.10%\n",
      "Epoch: 821, Loss: 0.03, accuracy: 84.11%\n",
      "Epoch: 822, Loss: 0.03, accuracy: 84.12%\n",
      "Epoch: 823, Loss: 0.03, accuracy: 84.12%\n",
      "Epoch: 824, Loss: 0.03, accuracy: 84.12%\n",
      "Epoch: 825, Loss: 0.03, accuracy: 84.12%\n",
      "Epoch: 826, Loss: 0.03, accuracy: 84.14%\n",
      "Epoch: 827, Loss: 0.03, accuracy: 84.14%\n",
      "Epoch: 828, Loss: 0.03, accuracy: 84.15%\n",
      "Epoch: 829, Loss: 0.03, accuracy: 84.16%\n",
      "Epoch: 830, Loss: 0.03, accuracy: 84.16%\n",
      "Epoch: 831, Loss: 0.03, accuracy: 84.16%\n",
      "Epoch: 832, Loss: 0.03, accuracy: 84.16%\n",
      "Epoch: 833, Loss: 0.03, accuracy: 84.16%\n",
      "Epoch: 834, Loss: 0.03, accuracy: 84.16%\n",
      "Epoch: 835, Loss: 0.03, accuracy: 84.16%\n",
      "Epoch: 836, Loss: 0.03, accuracy: 84.17%\n",
      "Epoch: 837, Loss: 0.03, accuracy: 84.17%\n",
      "Epoch: 838, Loss: 0.03, accuracy: 84.18%\n",
      "Epoch: 839, Loss: 0.03, accuracy: 84.19%\n",
      "Epoch: 840, Loss: 0.03, accuracy: 84.19%\n",
      "Epoch: 841, Loss: 0.03, accuracy: 84.19%\n",
      "Epoch: 842, Loss: 0.03, accuracy: 84.20%\n",
      "Epoch: 843, Loss: 0.03, accuracy: 84.20%\n",
      "Epoch: 844, Loss: 0.03, accuracy: 84.21%\n",
      "Epoch: 845, Loss: 0.03, accuracy: 84.22%\n",
      "Epoch: 846, Loss: 0.03, accuracy: 84.23%\n",
      "Epoch: 847, Loss: 0.03, accuracy: 84.24%\n",
      "Epoch: 848, Loss: 0.03, accuracy: 84.24%\n",
      "Epoch: 849, Loss: 0.03, accuracy: 84.24%\n",
      "Epoch: 850, Loss: 0.03, accuracy: 84.25%\n",
      "Epoch: 851, Loss: 0.03, accuracy: 84.25%\n",
      "Epoch: 852, Loss: 0.03, accuracy: 84.25%\n",
      "Epoch: 853, Loss: 0.03, accuracy: 84.26%\n",
      "Epoch: 854, Loss: 0.03, accuracy: 84.26%\n",
      "Epoch: 855, Loss: 0.03, accuracy: 84.27%\n",
      "Epoch: 856, Loss: 0.03, accuracy: 84.28%\n",
      "Epoch: 857, Loss: 0.03, accuracy: 84.28%\n",
      "Epoch: 858, Loss: 0.03, accuracy: 84.28%\n",
      "Epoch: 859, Loss: 0.03, accuracy: 84.29%\n",
      "Epoch: 860, Loss: 0.03, accuracy: 84.29%\n",
      "Epoch: 861, Loss: 0.03, accuracy: 84.29%\n",
      "Epoch: 862, Loss: 0.03, accuracy: 84.29%\n",
      "Epoch: 863, Loss: 0.03, accuracy: 84.30%\n",
      "Epoch: 864, Loss: 0.03, accuracy: 84.30%\n",
      "Epoch: 865, Loss: 0.03, accuracy: 84.30%\n",
      "Epoch: 866, Loss: 0.03, accuracy: 84.30%\n",
      "Epoch: 867, Loss: 0.03, accuracy: 84.31%\n",
      "Epoch: 868, Loss: 0.03, accuracy: 84.31%\n",
      "Epoch: 869, Loss: 0.03, accuracy: 84.31%\n",
      "Epoch: 870, Loss: 0.03, accuracy: 84.31%\n",
      "Epoch: 871, Loss: 0.03, accuracy: 84.32%\n",
      "Epoch: 872, Loss: 0.03, accuracy: 84.32%\n",
      "Epoch: 873, Loss: 0.03, accuracy: 84.32%\n",
      "Epoch: 874, Loss: 0.03, accuracy: 84.33%\n",
      "Epoch: 875, Loss: 0.03, accuracy: 84.33%\n",
      "Epoch: 876, Loss: 0.03, accuracy: 84.33%\n",
      "Epoch: 877, Loss: 0.03, accuracy: 84.33%\n",
      "Epoch: 878, Loss: 0.03, accuracy: 84.34%\n",
      "Epoch: 879, Loss: 0.03, accuracy: 84.35%\n",
      "Epoch: 880, Loss: 0.03, accuracy: 84.35%\n",
      "Epoch: 881, Loss: 0.03, accuracy: 84.36%\n",
      "Epoch: 882, Loss: 0.03, accuracy: 84.38%\n",
      "Epoch: 883, Loss: 0.03, accuracy: 84.38%\n",
      "Epoch: 884, Loss: 0.03, accuracy: 84.38%\n",
      "Epoch: 885, Loss: 0.03, accuracy: 84.39%\n",
      "Epoch: 886, Loss: 0.03, accuracy: 84.39%\n",
      "Epoch: 887, Loss: 0.03, accuracy: 84.40%\n",
      "Epoch: 888, Loss: 0.03, accuracy: 84.41%\n",
      "Epoch: 889, Loss: 0.03, accuracy: 84.41%\n",
      "Epoch: 890, Loss: 0.03, accuracy: 84.42%\n",
      "Epoch: 891, Loss: 0.03, accuracy: 84.42%\n",
      "Epoch: 892, Loss: 0.03, accuracy: 84.43%\n",
      "Epoch: 893, Loss: 0.03, accuracy: 84.43%\n",
      "Epoch: 894, Loss: 0.03, accuracy: 84.43%\n",
      "Epoch: 895, Loss: 0.03, accuracy: 84.44%\n",
      "Epoch: 896, Loss: 0.03, accuracy: 84.44%\n",
      "Epoch: 897, Loss: 0.03, accuracy: 84.45%\n",
      "Epoch: 898, Loss: 0.03, accuracy: 84.46%\n",
      "Epoch: 899, Loss: 0.03, accuracy: 84.46%\n",
      "Epoch: 900, Loss: 0.03, accuracy: 84.47%\n",
      "Epoch: 901, Loss: 0.03, accuracy: 84.47%\n",
      "Epoch: 902, Loss: 0.03, accuracy: 84.47%\n",
      "Epoch: 903, Loss: 0.03, accuracy: 84.47%\n",
      "Epoch: 904, Loss: 0.03, accuracy: 84.48%\n",
      "Epoch: 905, Loss: 0.03, accuracy: 84.48%\n",
      "Epoch: 906, Loss: 0.03, accuracy: 84.50%\n",
      "Epoch: 907, Loss: 0.03, accuracy: 84.50%\n",
      "Epoch: 908, Loss: 0.03, accuracy: 84.51%\n",
      "Epoch: 909, Loss: 0.03, accuracy: 84.52%\n",
      "Epoch: 910, Loss: 0.03, accuracy: 84.52%\n",
      "Epoch: 911, Loss: 0.03, accuracy: 84.54%\n",
      "Epoch: 912, Loss: 0.03, accuracy: 84.54%\n",
      "Epoch: 913, Loss: 0.03, accuracy: 84.54%\n",
      "Epoch: 914, Loss: 0.03, accuracy: 84.55%\n",
      "Epoch: 915, Loss: 0.03, accuracy: 84.55%\n",
      "Epoch: 916, Loss: 0.03, accuracy: 84.55%\n",
      "Epoch: 917, Loss: 0.03, accuracy: 84.55%\n",
      "Epoch: 918, Loss: 0.03, accuracy: 84.56%\n",
      "Epoch: 919, Loss: 0.03, accuracy: 84.57%\n",
      "Epoch: 920, Loss: 0.03, accuracy: 84.57%\n",
      "Epoch: 921, Loss: 0.03, accuracy: 84.57%\n",
      "Epoch: 922, Loss: 0.03, accuracy: 84.58%\n",
      "Epoch: 923, Loss: 0.03, accuracy: 84.58%\n",
      "Epoch: 924, Loss: 0.03, accuracy: 84.59%\n",
      "Epoch: 925, Loss: 0.03, accuracy: 84.59%\n",
      "Epoch: 926, Loss: 0.03, accuracy: 84.59%\n",
      "Epoch: 927, Loss: 0.03, accuracy: 84.60%\n",
      "Epoch: 928, Loss: 0.03, accuracy: 84.60%\n",
      "Epoch: 929, Loss: 0.03, accuracy: 84.61%\n",
      "Epoch: 930, Loss: 0.03, accuracy: 84.61%\n",
      "Epoch: 931, Loss: 0.03, accuracy: 84.62%\n",
      "Epoch: 932, Loss: 0.03, accuracy: 84.63%\n",
      "Epoch: 933, Loss: 0.03, accuracy: 84.63%\n",
      "Epoch: 934, Loss: 0.03, accuracy: 84.64%\n",
      "Epoch: 935, Loss: 0.03, accuracy: 84.65%\n",
      "Epoch: 936, Loss: 0.03, accuracy: 84.65%\n",
      "Epoch: 937, Loss: 0.03, accuracy: 84.65%\n",
      "Epoch: 938, Loss: 0.03, accuracy: 84.66%\n",
      "Epoch: 939, Loss: 0.03, accuracy: 84.65%\n",
      "Epoch: 940, Loss: 0.03, accuracy: 84.66%\n",
      "Epoch: 941, Loss: 0.03, accuracy: 84.67%\n",
      "Epoch: 942, Loss: 0.03, accuracy: 84.67%\n",
      "Epoch: 943, Loss: 0.03, accuracy: 84.67%\n",
      "Epoch: 944, Loss: 0.03, accuracy: 84.68%\n",
      "Epoch: 945, Loss: 0.03, accuracy: 84.69%\n",
      "Epoch: 946, Loss: 0.03, accuracy: 84.69%\n",
      "Epoch: 947, Loss: 0.03, accuracy: 84.70%\n",
      "Epoch: 948, Loss: 0.03, accuracy: 84.69%\n",
      "Epoch: 949, Loss: 0.03, accuracy: 84.70%\n",
      "Epoch: 950, Loss: 0.03, accuracy: 84.70%\n",
      "Epoch: 951, Loss: 0.03, accuracy: 84.71%\n",
      "Epoch: 952, Loss: 0.03, accuracy: 84.71%\n",
      "Epoch: 953, Loss: 0.03, accuracy: 84.73%\n",
      "Epoch: 954, Loss: 0.03, accuracy: 84.73%\n",
      "Epoch: 955, Loss: 0.03, accuracy: 84.74%\n",
      "Epoch: 956, Loss: 0.03, accuracy: 84.74%\n",
      "Epoch: 957, Loss: 0.03, accuracy: 84.74%\n",
      "Epoch: 958, Loss: 0.03, accuracy: 84.74%\n",
      "Epoch: 959, Loss: 0.03, accuracy: 84.75%\n",
      "Epoch: 960, Loss: 0.03, accuracy: 84.76%\n",
      "Epoch: 961, Loss: 0.03, accuracy: 84.76%\n",
      "Epoch: 962, Loss: 0.03, accuracy: 84.77%\n",
      "Epoch: 963, Loss: 0.03, accuracy: 84.77%\n",
      "Epoch: 964, Loss: 0.03, accuracy: 84.77%\n",
      "Epoch: 965, Loss: 0.03, accuracy: 84.78%\n",
      "Epoch: 966, Loss: 0.03, accuracy: 84.79%\n",
      "Epoch: 967, Loss: 0.03, accuracy: 84.79%\n",
      "Epoch: 968, Loss: 0.03, accuracy: 84.79%\n",
      "Epoch: 969, Loss: 0.03, accuracy: 84.79%\n",
      "Epoch: 970, Loss: 0.03, accuracy: 84.80%\n",
      "Epoch: 971, Loss: 0.03, accuracy: 84.80%\n",
      "Epoch: 972, Loss: 0.03, accuracy: 84.81%\n",
      "Epoch: 973, Loss: 0.03, accuracy: 84.81%\n",
      "Epoch: 974, Loss: 0.03, accuracy: 84.81%\n",
      "Epoch: 975, Loss: 0.03, accuracy: 84.81%\n",
      "Epoch: 976, Loss: 0.03, accuracy: 84.82%\n",
      "Epoch: 977, Loss: 0.03, accuracy: 84.82%\n",
      "Epoch: 978, Loss: 0.03, accuracy: 84.82%\n",
      "Epoch: 979, Loss: 0.03, accuracy: 84.83%\n",
      "Epoch: 980, Loss: 0.03, accuracy: 84.84%\n",
      "Epoch: 981, Loss: 0.03, accuracy: 84.84%\n",
      "Epoch: 982, Loss: 0.03, accuracy: 84.84%\n",
      "Epoch: 983, Loss: 0.03, accuracy: 84.85%\n",
      "Epoch: 984, Loss: 0.03, accuracy: 84.85%\n",
      "Epoch: 985, Loss: 0.03, accuracy: 84.85%\n",
      "Epoch: 986, Loss: 0.03, accuracy: 84.86%\n",
      "Epoch: 987, Loss: 0.03, accuracy: 84.87%\n",
      "Epoch: 988, Loss: 0.03, accuracy: 84.88%\n",
      "Epoch: 989, Loss: 0.03, accuracy: 84.88%\n",
      "Epoch: 990, Loss: 0.03, accuracy: 84.88%\n",
      "Epoch: 991, Loss: 0.03, accuracy: 84.89%\n",
      "Epoch: 992, Loss: 0.03, accuracy: 84.89%\n",
      "Epoch: 993, Loss: 0.03, accuracy: 84.89%\n",
      "Epoch: 994, Loss: 0.03, accuracy: 84.90%\n",
      "Epoch: 995, Loss: 0.03, accuracy: 84.90%\n",
      "Epoch: 996, Loss: 0.03, accuracy: 84.90%\n",
      "Epoch: 997, Loss: 0.03, accuracy: 84.90%\n",
      "Epoch: 998, Loss: 0.03, accuracy: 84.90%\n",
      "Epoch: 999, Loss: 0.03, accuracy: 84.91%\n",
      "Epoch: 1000, Loss: 0.03, accuracy: 84.92%\n",
      "==========================================\n",
      "Test data Accuracy: 83.62%\n"
     ]
    }
   ],
   "source": [
    "X = fashion_mnist[0][0].reshape(60000, 784) / 255\n",
    "y = fashion_mnist[0][1]\n",
    "y_one_hot = np.eye(10)[y]                       # one hot encode the target labels\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "X_train_normalized = X_train_normalized\n",
    "X_test_normalized = X_test_normalized.T\n",
    "y_train = y_train\n",
    "y_test = y_test.T\n",
    "\n",
    "layers_dim = [784,128,10]\n",
    "alpha = 0.2   # Learning rate\n",
    "activation_layer = ReLU\n",
    "loss = MSE\n",
    "enable_bias = True\n",
    "\n",
    "nn = NN(X_train_normalized, y_train, layers_dim, alpha, 1000, activation_layer, loss, enable_bias)\n",
    "nn.train()\n",
    "y_pred = nn.predict(X_test_normalized)\n",
    "\n",
    "accuracy = calculate_accuracy(y_pred, y_test)\n",
    "print(\"==========================================\")\n",
    "print(f\"Test data Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.09, accuracy: 7.47%\n",
      "Epoch: 2, Loss: 0.35, accuracy: 11.92%\n",
      "Epoch: 3, Loss: 0.24, accuracy: 14.90%\n",
      "Epoch: 4, Loss: 0.19, accuracy: 17.29%\n",
      "Epoch: 5, Loss: 0.16, accuracy: 19.21%\n",
      "Epoch: 6, Loss: 0.14, accuracy: 20.69%\n",
      "Epoch: 7, Loss: 0.13, accuracy: 21.83%\n",
      "Epoch: 8, Loss: 0.12, accuracy: 22.80%\n",
      "Epoch: 9, Loss: 0.12, accuracy: 23.77%\n",
      "Epoch: 10, Loss: 0.11, accuracy: 24.66%\n",
      "Epoch: 11, Loss: 0.11, accuracy: 25.49%\n",
      "Epoch: 12, Loss: 0.10, accuracy: 26.27%\n",
      "Epoch: 13, Loss: 0.10, accuracy: 27.11%\n",
      "Epoch: 14, Loss: 0.10, accuracy: 27.90%\n",
      "Epoch: 15, Loss: 0.10, accuracy: 28.70%\n",
      "Epoch: 16, Loss: 0.10, accuracy: 29.48%\n",
      "Epoch: 17, Loss: 0.09, accuracy: 30.28%\n",
      "Epoch: 18, Loss: 0.09, accuracy: 31.01%\n",
      "Epoch: 19, Loss: 0.09, accuracy: 31.79%\n",
      "Epoch: 20, Loss: 0.09, accuracy: 32.50%\n",
      "Epoch: 21, Loss: 0.09, accuracy: 33.27%\n",
      "Epoch: 22, Loss: 0.09, accuracy: 34.03%\n",
      "Epoch: 23, Loss: 0.09, accuracy: 34.81%\n",
      "Epoch: 24, Loss: 0.09, accuracy: 35.59%\n",
      "Epoch: 25, Loss: 0.09, accuracy: 36.39%\n",
      "Epoch: 26, Loss: 0.09, accuracy: 37.13%\n",
      "Epoch: 27, Loss: 0.08, accuracy: 37.89%\n",
      "Epoch: 28, Loss: 0.08, accuracy: 38.64%\n",
      "Epoch: 29, Loss: 0.08, accuracy: 39.43%\n",
      "Epoch: 30, Loss: 0.08, accuracy: 40.22%\n",
      "Epoch: 31, Loss: 0.08, accuracy: 41.04%\n",
      "Epoch: 32, Loss: 0.08, accuracy: 41.81%\n",
      "Epoch: 33, Loss: 0.08, accuracy: 42.55%\n",
      "Epoch: 34, Loss: 0.08, accuracy: 43.26%\n",
      "Epoch: 35, Loss: 0.08, accuracy: 43.94%\n",
      "Epoch: 36, Loss: 0.08, accuracy: 44.63%\n",
      "Epoch: 37, Loss: 0.08, accuracy: 45.31%\n",
      "Epoch: 38, Loss: 0.08, accuracy: 45.93%\n",
      "Epoch: 39, Loss: 0.08, accuracy: 46.59%\n",
      "Epoch: 40, Loss: 0.08, accuracy: 47.25%\n",
      "Epoch: 41, Loss: 0.08, accuracy: 47.84%\n",
      "Epoch: 42, Loss: 0.07, accuracy: 48.40%\n",
      "Epoch: 43, Loss: 0.07, accuracy: 49.01%\n",
      "Epoch: 44, Loss: 0.07, accuracy: 49.60%\n",
      "Epoch: 45, Loss: 0.07, accuracy: 50.22%\n",
      "Epoch: 46, Loss: 0.07, accuracy: 50.81%\n",
      "Epoch: 47, Loss: 0.07, accuracy: 51.33%\n",
      "Epoch: 48, Loss: 0.07, accuracy: 51.88%\n",
      "Epoch: 49, Loss: 0.07, accuracy: 52.47%\n",
      "Epoch: 50, Loss: 0.07, accuracy: 52.91%\n",
      "Epoch: 51, Loss: 0.07, accuracy: 53.42%\n",
      "Epoch: 52, Loss: 0.07, accuracy: 53.96%\n",
      "Epoch: 53, Loss: 0.07, accuracy: 54.55%\n",
      "Epoch: 54, Loss: 0.07, accuracy: 55.09%\n",
      "Epoch: 55, Loss: 0.07, accuracy: 55.68%\n",
      "Epoch: 56, Loss: 0.07, accuracy: 56.23%\n",
      "Epoch: 57, Loss: 0.07, accuracy: 56.73%\n",
      "Epoch: 58, Loss: 0.06, accuracy: 57.23%\n",
      "Epoch: 59, Loss: 0.06, accuracy: 57.65%\n",
      "Epoch: 60, Loss: 0.06, accuracy: 58.07%\n",
      "Epoch: 61, Loss: 0.06, accuracy: 58.55%\n",
      "Epoch: 62, Loss: 0.06, accuracy: 58.99%\n",
      "Epoch: 63, Loss: 0.06, accuracy: 59.46%\n",
      "Epoch: 64, Loss: 0.06, accuracy: 59.91%\n",
      "Epoch: 65, Loss: 0.06, accuracy: 60.36%\n",
      "Epoch: 66, Loss: 0.06, accuracy: 60.75%\n",
      "Epoch: 67, Loss: 0.06, accuracy: 61.11%\n",
      "Epoch: 68, Loss: 0.06, accuracy: 61.46%\n",
      "Epoch: 69, Loss: 0.06, accuracy: 61.76%\n",
      "Epoch: 70, Loss: 0.06, accuracy: 62.06%\n",
      "Epoch: 71, Loss: 0.06, accuracy: 62.39%\n",
      "Epoch: 72, Loss: 0.06, accuracy: 62.67%\n",
      "Epoch: 73, Loss: 0.06, accuracy: 62.97%\n",
      "Epoch: 74, Loss: 0.06, accuracy: 63.24%\n",
      "Epoch: 75, Loss: 0.06, accuracy: 63.49%\n",
      "Epoch: 76, Loss: 0.06, accuracy: 63.78%\n",
      "Epoch: 77, Loss: 0.06, accuracy: 64.03%\n",
      "Epoch: 78, Loss: 0.06, accuracy: 64.29%\n",
      "Epoch: 79, Loss: 0.06, accuracy: 64.57%\n",
      "Epoch: 80, Loss: 0.06, accuracy: 64.83%\n",
      "Epoch: 81, Loss: 0.06, accuracy: 65.05%\n",
      "Epoch: 82, Loss: 0.05, accuracy: 65.27%\n",
      "Epoch: 83, Loss: 0.05, accuracy: 65.51%\n",
      "Epoch: 84, Loss: 0.05, accuracy: 65.76%\n",
      "Epoch: 85, Loss: 0.05, accuracy: 66.01%\n",
      "Epoch: 86, Loss: 0.05, accuracy: 66.22%\n",
      "Epoch: 87, Loss: 0.05, accuracy: 66.43%\n",
      "Epoch: 88, Loss: 0.05, accuracy: 66.61%\n",
      "Epoch: 89, Loss: 0.05, accuracy: 66.80%\n",
      "Epoch: 90, Loss: 0.05, accuracy: 67.03%\n",
      "Epoch: 91, Loss: 0.05, accuracy: 67.25%\n",
      "Epoch: 92, Loss: 0.05, accuracy: 67.43%\n",
      "Epoch: 93, Loss: 0.05, accuracy: 67.58%\n",
      "Epoch: 94, Loss: 0.05, accuracy: 67.76%\n",
      "Epoch: 95, Loss: 0.05, accuracy: 67.96%\n",
      "Epoch: 96, Loss: 0.05, accuracy: 68.11%\n",
      "Epoch: 97, Loss: 0.05, accuracy: 68.26%\n",
      "Epoch: 98, Loss: 0.05, accuracy: 68.45%\n",
      "Epoch: 99, Loss: 0.05, accuracy: 68.58%\n",
      "Epoch: 100, Loss: 0.05, accuracy: 68.73%\n",
      "Epoch: 101, Loss: 0.05, accuracy: 68.89%\n",
      "Epoch: 102, Loss: 0.05, accuracy: 69.03%\n",
      "Epoch: 103, Loss: 0.05, accuracy: 69.18%\n",
      "Epoch: 104, Loss: 0.05, accuracy: 69.34%\n",
      "Epoch: 105, Loss: 0.05, accuracy: 69.50%\n",
      "Epoch: 106, Loss: 0.05, accuracy: 69.64%\n",
      "Epoch: 107, Loss: 0.05, accuracy: 69.77%\n",
      "Epoch: 108, Loss: 0.05, accuracy: 69.94%\n",
      "Epoch: 109, Loss: 0.05, accuracy: 70.07%\n",
      "Epoch: 110, Loss: 0.05, accuracy: 70.20%\n",
      "Epoch: 111, Loss: 0.05, accuracy: 70.33%\n",
      "Epoch: 112, Loss: 0.05, accuracy: 70.44%\n",
      "Epoch: 113, Loss: 0.05, accuracy: 70.60%\n",
      "Epoch: 114, Loss: 0.05, accuracy: 70.73%\n",
      "Epoch: 115, Loss: 0.05, accuracy: 70.88%\n",
      "Epoch: 116, Loss: 0.05, accuracy: 71.01%\n",
      "Epoch: 117, Loss: 0.05, accuracy: 71.13%\n",
      "Epoch: 118, Loss: 0.05, accuracy: 71.27%\n",
      "Epoch: 119, Loss: 0.05, accuracy: 71.40%\n",
      "Epoch: 120, Loss: 0.05, accuracy: 71.56%\n",
      "Epoch: 121, Loss: 0.05, accuracy: 71.68%\n",
      "Epoch: 122, Loss: 0.05, accuracy: 71.80%\n",
      "Epoch: 123, Loss: 0.05, accuracy: 71.90%\n",
      "Epoch: 124, Loss: 0.05, accuracy: 72.03%\n",
      "Epoch: 125, Loss: 0.05, accuracy: 72.15%\n",
      "Epoch: 126, Loss: 0.05, accuracy: 72.25%\n",
      "Epoch: 127, Loss: 0.05, accuracy: 72.37%\n",
      "Epoch: 128, Loss: 0.05, accuracy: 72.49%\n",
      "Epoch: 129, Loss: 0.05, accuracy: 72.61%\n",
      "Epoch: 130, Loss: 0.05, accuracy: 72.75%\n",
      "Epoch: 131, Loss: 0.05, accuracy: 72.84%\n",
      "Epoch: 132, Loss: 0.04, accuracy: 72.98%\n",
      "Epoch: 133, Loss: 0.04, accuracy: 73.08%\n",
      "Epoch: 134, Loss: 0.04, accuracy: 73.22%\n",
      "Epoch: 135, Loss: 0.04, accuracy: 73.32%\n",
      "Epoch: 136, Loss: 0.04, accuracy: 73.43%\n",
      "Epoch: 137, Loss: 0.04, accuracy: 73.54%\n",
      "Epoch: 138, Loss: 0.04, accuracy: 73.62%\n",
      "Epoch: 139, Loss: 0.04, accuracy: 73.69%\n",
      "Epoch: 140, Loss: 0.04, accuracy: 73.79%\n",
      "Epoch: 141, Loss: 0.04, accuracy: 73.91%\n",
      "Epoch: 142, Loss: 0.04, accuracy: 74.02%\n",
      "Epoch: 143, Loss: 0.04, accuracy: 74.14%\n",
      "Epoch: 144, Loss: 0.04, accuracy: 74.23%\n",
      "Epoch: 145, Loss: 0.04, accuracy: 74.30%\n",
      "Epoch: 146, Loss: 0.04, accuracy: 74.41%\n",
      "Epoch: 147, Loss: 0.04, accuracy: 74.50%\n",
      "Epoch: 148, Loss: 0.04, accuracy: 74.59%\n",
      "Epoch: 149, Loss: 0.04, accuracy: 74.68%\n",
      "Epoch: 150, Loss: 0.04, accuracy: 74.77%\n",
      "Epoch: 151, Loss: 0.04, accuracy: 74.85%\n",
      "Epoch: 152, Loss: 0.04, accuracy: 74.93%\n",
      "Epoch: 153, Loss: 0.04, accuracy: 75.01%\n",
      "Epoch: 154, Loss: 0.04, accuracy: 75.09%\n",
      "Epoch: 155, Loss: 0.04, accuracy: 75.17%\n",
      "Epoch: 156, Loss: 0.04, accuracy: 75.28%\n",
      "Epoch: 157, Loss: 0.04, accuracy: 75.34%\n",
      "Epoch: 158, Loss: 0.04, accuracy: 75.42%\n",
      "Epoch: 159, Loss: 0.04, accuracy: 75.51%\n",
      "Epoch: 160, Loss: 0.04, accuracy: 75.58%\n",
      "Epoch: 161, Loss: 0.04, accuracy: 75.66%\n",
      "Epoch: 162, Loss: 0.04, accuracy: 75.74%\n",
      "Epoch: 163, Loss: 0.04, accuracy: 75.83%\n",
      "Epoch: 164, Loss: 0.04, accuracy: 75.89%\n",
      "Epoch: 165, Loss: 0.04, accuracy: 75.96%\n",
      "Epoch: 166, Loss: 0.04, accuracy: 76.06%\n",
      "Epoch: 167, Loss: 0.04, accuracy: 76.14%\n",
      "Epoch: 168, Loss: 0.04, accuracy: 76.21%\n",
      "Epoch: 169, Loss: 0.04, accuracy: 76.31%\n",
      "Epoch: 170, Loss: 0.04, accuracy: 76.40%\n",
      "Epoch: 171, Loss: 0.04, accuracy: 76.48%\n",
      "Epoch: 172, Loss: 0.04, accuracy: 76.56%\n",
      "Epoch: 173, Loss: 0.04, accuracy: 76.65%\n",
      "Epoch: 174, Loss: 0.04, accuracy: 76.72%\n",
      "Epoch: 175, Loss: 0.04, accuracy: 76.79%\n",
      "Epoch: 176, Loss: 0.04, accuracy: 76.85%\n",
      "Epoch: 177, Loss: 0.04, accuracy: 76.92%\n",
      "Epoch: 178, Loss: 0.04, accuracy: 76.99%\n",
      "Epoch: 179, Loss: 0.04, accuracy: 77.04%\n",
      "Epoch: 180, Loss: 0.04, accuracy: 77.10%\n",
      "Epoch: 181, Loss: 0.04, accuracy: 77.17%\n",
      "Epoch: 182, Loss: 0.04, accuracy: 77.25%\n",
      "Epoch: 183, Loss: 0.04, accuracy: 77.35%\n",
      "Epoch: 184, Loss: 0.04, accuracy: 77.40%\n",
      "Epoch: 185, Loss: 0.04, accuracy: 77.47%\n",
      "Epoch: 186, Loss: 0.04, accuracy: 77.53%\n",
      "Epoch: 187, Loss: 0.04, accuracy: 77.61%\n",
      "Epoch: 188, Loss: 0.04, accuracy: 77.67%\n",
      "Epoch: 189, Loss: 0.04, accuracy: 77.73%\n",
      "Epoch: 190, Loss: 0.04, accuracy: 77.79%\n",
      "Epoch: 191, Loss: 0.04, accuracy: 77.85%\n",
      "Epoch: 192, Loss: 0.04, accuracy: 77.91%\n",
      "Epoch: 193, Loss: 0.04, accuracy: 77.99%\n",
      "Epoch: 194, Loss: 0.04, accuracy: 78.05%\n",
      "Epoch: 195, Loss: 0.04, accuracy: 78.11%\n",
      "Epoch: 196, Loss: 0.04, accuracy: 78.16%\n",
      "Epoch: 197, Loss: 0.04, accuracy: 78.20%\n",
      "Epoch: 198, Loss: 0.04, accuracy: 78.26%\n",
      "Epoch: 199, Loss: 0.04, accuracy: 78.31%\n",
      "Epoch: 200, Loss: 0.04, accuracy: 78.35%\n",
      "Epoch: 201, Loss: 0.04, accuracy: 78.41%\n",
      "Epoch: 202, Loss: 0.04, accuracy: 78.45%\n",
      "Epoch: 203, Loss: 0.04, accuracy: 78.51%\n",
      "Epoch: 204, Loss: 0.04, accuracy: 78.56%\n",
      "Epoch: 205, Loss: 0.04, accuracy: 78.62%\n",
      "Epoch: 206, Loss: 0.04, accuracy: 78.69%\n",
      "Epoch: 207, Loss: 0.04, accuracy: 78.74%\n",
      "Epoch: 208, Loss: 0.04, accuracy: 78.78%\n",
      "Epoch: 209, Loss: 0.04, accuracy: 78.82%\n",
      "Epoch: 210, Loss: 0.04, accuracy: 78.87%\n",
      "Epoch: 211, Loss: 0.04, accuracy: 78.91%\n",
      "Epoch: 212, Loss: 0.04, accuracy: 78.97%\n",
      "Epoch: 213, Loss: 0.04, accuracy: 79.02%\n",
      "Epoch: 214, Loss: 0.04, accuracy: 79.08%\n",
      "Epoch: 215, Loss: 0.04, accuracy: 79.13%\n",
      "Epoch: 216, Loss: 0.04, accuracy: 79.17%\n",
      "Epoch: 217, Loss: 0.04, accuracy: 79.20%\n",
      "Epoch: 218, Loss: 0.04, accuracy: 79.25%\n",
      "Epoch: 219, Loss: 0.04, accuracy: 79.30%\n",
      "Epoch: 220, Loss: 0.04, accuracy: 79.34%\n",
      "Epoch: 221, Loss: 0.04, accuracy: 79.38%\n",
      "Epoch: 222, Loss: 0.04, accuracy: 79.41%\n",
      "Epoch: 223, Loss: 0.04, accuracy: 79.48%\n",
      "Epoch: 224, Loss: 0.04, accuracy: 79.52%\n",
      "Epoch: 225, Loss: 0.04, accuracy: 79.58%\n",
      "Epoch: 226, Loss: 0.04, accuracy: 79.64%\n",
      "Epoch: 227, Loss: 0.04, accuracy: 79.70%\n",
      "Epoch: 228, Loss: 0.04, accuracy: 79.75%\n",
      "Epoch: 229, Loss: 0.04, accuracy: 79.79%\n",
      "Epoch: 230, Loss: 0.04, accuracy: 79.85%\n",
      "Epoch: 231, Loss: 0.04, accuracy: 79.90%\n",
      "Epoch: 232, Loss: 0.04, accuracy: 79.96%\n",
      "Epoch: 233, Loss: 0.04, accuracy: 80.02%\n",
      "Epoch: 234, Loss: 0.04, accuracy: 80.06%\n",
      "Epoch: 235, Loss: 0.04, accuracy: 80.10%\n",
      "Epoch: 236, Loss: 0.04, accuracy: 80.13%\n",
      "Epoch: 237, Loss: 0.04, accuracy: 80.18%\n",
      "Epoch: 238, Loss: 0.04, accuracy: 80.22%\n",
      "Epoch: 239, Loss: 0.04, accuracy: 80.28%\n",
      "Epoch: 240, Loss: 0.04, accuracy: 80.34%\n",
      "Epoch: 241, Loss: 0.04, accuracy: 80.38%\n",
      "Epoch: 242, Loss: 0.04, accuracy: 80.42%\n",
      "Epoch: 243, Loss: 0.03, accuracy: 80.47%\n",
      "Epoch: 244, Loss: 0.03, accuracy: 80.53%\n",
      "Epoch: 245, Loss: 0.03, accuracy: 80.56%\n",
      "Epoch: 246, Loss: 0.03, accuracy: 80.59%\n",
      "Epoch: 247, Loss: 0.03, accuracy: 80.64%\n",
      "Epoch: 248, Loss: 0.03, accuracy: 80.68%\n",
      "Epoch: 249, Loss: 0.03, accuracy: 80.72%\n",
      "Epoch: 250, Loss: 0.03, accuracy: 80.76%\n",
      "Epoch: 251, Loss: 0.03, accuracy: 80.80%\n",
      "Epoch: 252, Loss: 0.03, accuracy: 80.84%\n",
      "Epoch: 253, Loss: 0.03, accuracy: 80.90%\n",
      "Epoch: 254, Loss: 0.03, accuracy: 80.94%\n",
      "Epoch: 255, Loss: 0.03, accuracy: 80.96%\n",
      "Epoch: 256, Loss: 0.03, accuracy: 81.02%\n",
      "Epoch: 257, Loss: 0.03, accuracy: 81.08%\n",
      "Epoch: 258, Loss: 0.03, accuracy: 81.12%\n",
      "Epoch: 259, Loss: 0.03, accuracy: 81.15%\n",
      "Epoch: 260, Loss: 0.03, accuracy: 81.19%\n",
      "Epoch: 261, Loss: 0.03, accuracy: 81.23%\n",
      "Epoch: 262, Loss: 0.03, accuracy: 81.27%\n",
      "Epoch: 263, Loss: 0.03, accuracy: 81.30%\n",
      "Epoch: 264, Loss: 0.03, accuracy: 81.33%\n",
      "Epoch: 265, Loss: 0.03, accuracy: 81.38%\n",
      "Epoch: 266, Loss: 0.03, accuracy: 81.43%\n",
      "Epoch: 267, Loss: 0.03, accuracy: 81.47%\n",
      "Epoch: 268, Loss: 0.03, accuracy: 81.51%\n",
      "Epoch: 269, Loss: 0.03, accuracy: 81.55%\n",
      "Epoch: 270, Loss: 0.03, accuracy: 81.58%\n",
      "Epoch: 271, Loss: 0.03, accuracy: 81.61%\n",
      "Epoch: 272, Loss: 0.03, accuracy: 81.66%\n",
      "Epoch: 273, Loss: 0.03, accuracy: 81.70%\n",
      "Epoch: 274, Loss: 0.03, accuracy: 81.72%\n",
      "Epoch: 275, Loss: 0.03, accuracy: 81.75%\n",
      "Epoch: 276, Loss: 0.03, accuracy: 81.77%\n",
      "Epoch: 277, Loss: 0.03, accuracy: 81.80%\n",
      "Epoch: 278, Loss: 0.03, accuracy: 81.84%\n",
      "Epoch: 279, Loss: 0.03, accuracy: 81.87%\n",
      "Epoch: 280, Loss: 0.03, accuracy: 81.90%\n",
      "Epoch: 281, Loss: 0.03, accuracy: 81.94%\n",
      "Epoch: 282, Loss: 0.03, accuracy: 81.96%\n",
      "Epoch: 283, Loss: 0.03, accuracy: 82.00%\n",
      "Epoch: 284, Loss: 0.03, accuracy: 82.02%\n",
      "Epoch: 285, Loss: 0.03, accuracy: 82.07%\n",
      "Epoch: 286, Loss: 0.03, accuracy: 82.12%\n",
      "Epoch: 287, Loss: 0.03, accuracy: 82.16%\n",
      "Epoch: 288, Loss: 0.03, accuracy: 82.19%\n",
      "Epoch: 289, Loss: 0.03, accuracy: 82.23%\n",
      "Epoch: 290, Loss: 0.03, accuracy: 82.26%\n",
      "Epoch: 291, Loss: 0.03, accuracy: 82.29%\n",
      "Epoch: 292, Loss: 0.03, accuracy: 82.31%\n",
      "Epoch: 293, Loss: 0.03, accuracy: 82.33%\n",
      "Epoch: 294, Loss: 0.03, accuracy: 82.36%\n",
      "Epoch: 295, Loss: 0.03, accuracy: 82.39%\n",
      "Epoch: 296, Loss: 0.03, accuracy: 82.41%\n",
      "Epoch: 297, Loss: 0.03, accuracy: 82.45%\n",
      "Epoch: 298, Loss: 0.03, accuracy: 82.49%\n",
      "Epoch: 299, Loss: 0.03, accuracy: 82.52%\n",
      "Epoch: 300, Loss: 0.03, accuracy: 82.54%\n",
      "Epoch: 301, Loss: 0.03, accuracy: 82.58%\n",
      "Epoch: 302, Loss: 0.03, accuracy: 82.61%\n",
      "Epoch: 303, Loss: 0.03, accuracy: 82.64%\n",
      "Epoch: 304, Loss: 0.03, accuracy: 82.66%\n",
      "Epoch: 305, Loss: 0.03, accuracy: 82.69%\n",
      "Epoch: 306, Loss: 0.03, accuracy: 82.71%\n",
      "Epoch: 307, Loss: 0.03, accuracy: 82.75%\n",
      "Epoch: 308, Loss: 0.03, accuracy: 82.78%\n",
      "Epoch: 309, Loss: 0.03, accuracy: 82.81%\n",
      "Epoch: 310, Loss: 0.03, accuracy: 82.84%\n",
      "Epoch: 311, Loss: 0.03, accuracy: 82.88%\n",
      "Epoch: 312, Loss: 0.03, accuracy: 82.90%\n",
      "Epoch: 313, Loss: 0.03, accuracy: 82.92%\n",
      "Epoch: 314, Loss: 0.03, accuracy: 82.95%\n",
      "Epoch: 315, Loss: 0.03, accuracy: 82.99%\n",
      "Epoch: 316, Loss: 0.03, accuracy: 83.01%\n",
      "Epoch: 317, Loss: 0.03, accuracy: 83.05%\n",
      "Epoch: 318, Loss: 0.03, accuracy: 83.08%\n",
      "Epoch: 319, Loss: 0.03, accuracy: 83.12%\n",
      "Epoch: 320, Loss: 0.03, accuracy: 83.15%\n",
      "Epoch: 321, Loss: 0.03, accuracy: 83.19%\n",
      "Epoch: 322, Loss: 0.03, accuracy: 83.23%\n",
      "Epoch: 323, Loss: 0.03, accuracy: 83.26%\n",
      "Epoch: 324, Loss: 0.03, accuracy: 83.30%\n",
      "Epoch: 325, Loss: 0.03, accuracy: 83.33%\n",
      "Epoch: 326, Loss: 0.03, accuracy: 83.34%\n",
      "Epoch: 327, Loss: 0.03, accuracy: 83.37%\n",
      "Epoch: 328, Loss: 0.03, accuracy: 83.41%\n",
      "Epoch: 329, Loss: 0.03, accuracy: 83.44%\n",
      "Epoch: 330, Loss: 0.03, accuracy: 83.46%\n",
      "Epoch: 331, Loss: 0.03, accuracy: 83.49%\n",
      "Epoch: 332, Loss: 0.03, accuracy: 83.51%\n",
      "Epoch: 333, Loss: 0.03, accuracy: 83.55%\n",
      "Epoch: 334, Loss: 0.03, accuracy: 83.56%\n",
      "Epoch: 335, Loss: 0.03, accuracy: 83.58%\n",
      "Epoch: 336, Loss: 0.03, accuracy: 83.59%\n",
      "Epoch: 337, Loss: 0.03, accuracy: 83.62%\n",
      "Epoch: 338, Loss: 0.03, accuracy: 83.64%\n",
      "Epoch: 339, Loss: 0.03, accuracy: 83.66%\n",
      "Epoch: 340, Loss: 0.03, accuracy: 83.70%\n",
      "Epoch: 341, Loss: 0.03, accuracy: 83.73%\n",
      "Epoch: 342, Loss: 0.03, accuracy: 83.75%\n",
      "Epoch: 343, Loss: 0.03, accuracy: 83.77%\n",
      "Epoch: 344, Loss: 0.03, accuracy: 83.79%\n",
      "Epoch: 345, Loss: 0.03, accuracy: 83.82%\n",
      "Epoch: 346, Loss: 0.03, accuracy: 83.84%\n",
      "Epoch: 347, Loss: 0.03, accuracy: 83.87%\n",
      "Epoch: 348, Loss: 0.03, accuracy: 83.90%\n",
      "Epoch: 349, Loss: 0.03, accuracy: 83.92%\n",
      "Epoch: 350, Loss: 0.03, accuracy: 83.94%\n",
      "Epoch: 351, Loss: 0.03, accuracy: 83.96%\n",
      "Epoch: 352, Loss: 0.03, accuracy: 83.99%\n",
      "Epoch: 353, Loss: 0.03, accuracy: 84.01%\n",
      "Epoch: 354, Loss: 0.03, accuracy: 84.03%\n",
      "Epoch: 355, Loss: 0.03, accuracy: 84.04%\n",
      "Epoch: 356, Loss: 0.03, accuracy: 84.07%\n",
      "Epoch: 357, Loss: 0.03, accuracy: 84.09%\n",
      "Epoch: 358, Loss: 0.03, accuracy: 84.12%\n",
      "Epoch: 359, Loss: 0.03, accuracy: 84.16%\n",
      "Epoch: 360, Loss: 0.03, accuracy: 84.18%\n",
      "Epoch: 361, Loss: 0.03, accuracy: 84.20%\n",
      "Epoch: 362, Loss: 0.03, accuracy: 84.23%\n",
      "Epoch: 363, Loss: 0.03, accuracy: 84.26%\n",
      "Epoch: 364, Loss: 0.03, accuracy: 84.28%\n",
      "Epoch: 365, Loss: 0.03, accuracy: 84.31%\n",
      "Epoch: 366, Loss: 0.03, accuracy: 84.33%\n",
      "Epoch: 367, Loss: 0.03, accuracy: 84.34%\n",
      "Epoch: 368, Loss: 0.03, accuracy: 84.36%\n",
      "Epoch: 369, Loss: 0.03, accuracy: 84.38%\n",
      "Epoch: 370, Loss: 0.03, accuracy: 84.41%\n",
      "Epoch: 371, Loss: 0.03, accuracy: 84.42%\n",
      "Epoch: 372, Loss: 0.03, accuracy: 84.44%\n",
      "Epoch: 373, Loss: 0.03, accuracy: 84.46%\n",
      "Epoch: 374, Loss: 0.03, accuracy: 84.48%\n",
      "Epoch: 375, Loss: 0.03, accuracy: 84.50%\n",
      "Epoch: 376, Loss: 0.03, accuracy: 84.52%\n",
      "Epoch: 377, Loss: 0.03, accuracy: 84.54%\n",
      "Epoch: 378, Loss: 0.03, accuracy: 84.56%\n",
      "Epoch: 379, Loss: 0.03, accuracy: 84.57%\n",
      "Epoch: 380, Loss: 0.03, accuracy: 84.59%\n",
      "Epoch: 381, Loss: 0.03, accuracy: 84.62%\n",
      "Epoch: 382, Loss: 0.03, accuracy: 84.63%\n",
      "Epoch: 383, Loss: 0.03, accuracy: 84.65%\n",
      "Epoch: 384, Loss: 0.03, accuracy: 84.68%\n",
      "Epoch: 385, Loss: 0.03, accuracy: 84.70%\n",
      "Epoch: 386, Loss: 0.03, accuracy: 84.73%\n",
      "Epoch: 387, Loss: 0.03, accuracy: 84.75%\n",
      "Epoch: 388, Loss: 0.03, accuracy: 84.77%\n",
      "Epoch: 389, Loss: 0.03, accuracy: 84.79%\n",
      "Epoch: 390, Loss: 0.03, accuracy: 84.81%\n",
      "Epoch: 391, Loss: 0.03, accuracy: 84.82%\n",
      "Epoch: 392, Loss: 0.03, accuracy: 84.86%\n",
      "Epoch: 393, Loss: 0.03, accuracy: 84.88%\n",
      "Epoch: 394, Loss: 0.03, accuracy: 84.91%\n",
      "Epoch: 395, Loss: 0.03, accuracy: 84.91%\n",
      "Epoch: 396, Loss: 0.03, accuracy: 84.94%\n",
      "Epoch: 397, Loss: 0.03, accuracy: 84.96%\n",
      "Epoch: 398, Loss: 0.03, accuracy: 84.99%\n",
      "Epoch: 399, Loss: 0.03, accuracy: 85.02%\n",
      "Epoch: 400, Loss: 0.03, accuracy: 85.05%\n",
      "Epoch: 401, Loss: 0.03, accuracy: 85.06%\n",
      "Epoch: 402, Loss: 0.03, accuracy: 85.08%\n",
      "Epoch: 403, Loss: 0.03, accuracy: 85.10%\n",
      "Epoch: 404, Loss: 0.03, accuracy: 85.12%\n",
      "Epoch: 405, Loss: 0.03, accuracy: 85.15%\n",
      "Epoch: 406, Loss: 0.03, accuracy: 85.17%\n",
      "Epoch: 407, Loss: 0.03, accuracy: 85.19%\n",
      "Epoch: 408, Loss: 0.03, accuracy: 85.21%\n",
      "Epoch: 409, Loss: 0.03, accuracy: 85.22%\n",
      "Epoch: 410, Loss: 0.03, accuracy: 85.24%\n",
      "Epoch: 411, Loss: 0.03, accuracy: 85.27%\n",
      "Epoch: 412, Loss: 0.03, accuracy: 85.28%\n",
      "Epoch: 413, Loss: 0.03, accuracy: 85.30%\n",
      "Epoch: 414, Loss: 0.03, accuracy: 85.31%\n",
      "Epoch: 415, Loss: 0.03, accuracy: 85.34%\n",
      "Epoch: 416, Loss: 0.03, accuracy: 85.35%\n",
      "Epoch: 417, Loss: 0.03, accuracy: 85.38%\n",
      "Epoch: 418, Loss: 0.03, accuracy: 85.39%\n",
      "Epoch: 419, Loss: 0.03, accuracy: 85.40%\n",
      "Epoch: 420, Loss: 0.03, accuracy: 85.42%\n",
      "Epoch: 421, Loss: 0.03, accuracy: 85.44%\n",
      "Epoch: 422, Loss: 0.03, accuracy: 85.46%\n",
      "Epoch: 423, Loss: 0.03, accuracy: 85.47%\n",
      "Epoch: 424, Loss: 0.03, accuracy: 85.48%\n",
      "Epoch: 425, Loss: 0.03, accuracy: 85.51%\n",
      "Epoch: 426, Loss: 0.03, accuracy: 85.52%\n",
      "Epoch: 427, Loss: 0.03, accuracy: 85.54%\n",
      "Epoch: 428, Loss: 0.03, accuracy: 85.57%\n",
      "Epoch: 429, Loss: 0.03, accuracy: 85.59%\n",
      "Epoch: 430, Loss: 0.03, accuracy: 85.60%\n",
      "Epoch: 431, Loss: 0.03, accuracy: 85.60%\n",
      "Epoch: 432, Loss: 0.03, accuracy: 85.63%\n",
      "Epoch: 433, Loss: 0.03, accuracy: 85.64%\n",
      "Epoch: 434, Loss: 0.03, accuracy: 85.65%\n",
      "Epoch: 435, Loss: 0.03, accuracy: 85.67%\n",
      "Epoch: 436, Loss: 0.03, accuracy: 85.69%\n",
      "Epoch: 437, Loss: 0.03, accuracy: 85.69%\n",
      "Epoch: 438, Loss: 0.03, accuracy: 85.70%\n",
      "Epoch: 439, Loss: 0.03, accuracy: 85.71%\n",
      "Epoch: 440, Loss: 0.03, accuracy: 85.74%\n",
      "Epoch: 441, Loss: 0.03, accuracy: 85.76%\n",
      "Epoch: 442, Loss: 0.03, accuracy: 85.78%\n",
      "Epoch: 443, Loss: 0.03, accuracy: 85.78%\n",
      "Epoch: 444, Loss: 0.03, accuracy: 85.79%\n",
      "Epoch: 445, Loss: 0.03, accuracy: 85.81%\n",
      "Epoch: 446, Loss: 0.03, accuracy: 85.82%\n",
      "Epoch: 447, Loss: 0.03, accuracy: 85.85%\n",
      "Epoch: 448, Loss: 0.03, accuracy: 85.86%\n",
      "Epoch: 449, Loss: 0.03, accuracy: 85.89%\n",
      "Epoch: 450, Loss: 0.03, accuracy: 85.90%\n",
      "Epoch: 451, Loss: 0.03, accuracy: 85.92%\n",
      "Epoch: 452, Loss: 0.03, accuracy: 85.93%\n",
      "Epoch: 453, Loss: 0.03, accuracy: 85.95%\n",
      "Epoch: 454, Loss: 0.03, accuracy: 85.99%\n",
      "Epoch: 455, Loss: 0.03, accuracy: 86.00%\n",
      "Epoch: 456, Loss: 0.03, accuracy: 86.02%\n",
      "Epoch: 457, Loss: 0.03, accuracy: 86.02%\n",
      "Epoch: 458, Loss: 0.03, accuracy: 86.04%\n",
      "Epoch: 459, Loss: 0.03, accuracy: 86.06%\n",
      "Epoch: 460, Loss: 0.03, accuracy: 86.09%\n",
      "Epoch: 461, Loss: 0.03, accuracy: 86.11%\n",
      "Epoch: 462, Loss: 0.03, accuracy: 86.12%\n",
      "Epoch: 463, Loss: 0.03, accuracy: 86.15%\n",
      "Epoch: 464, Loss: 0.03, accuracy: 86.17%\n",
      "Epoch: 465, Loss: 0.03, accuracy: 86.18%\n",
      "Epoch: 466, Loss: 0.03, accuracy: 86.19%\n",
      "Epoch: 467, Loss: 0.03, accuracy: 86.21%\n",
      "Epoch: 468, Loss: 0.03, accuracy: 86.23%\n",
      "Epoch: 469, Loss: 0.03, accuracy: 86.25%\n",
      "Epoch: 470, Loss: 0.03, accuracy: 86.25%\n",
      "Epoch: 471, Loss: 0.03, accuracy: 86.27%\n",
      "Epoch: 472, Loss: 0.03, accuracy: 86.30%\n",
      "Epoch: 473, Loss: 0.03, accuracy: 86.33%\n",
      "Epoch: 474, Loss: 0.03, accuracy: 86.34%\n",
      "Epoch: 475, Loss: 0.03, accuracy: 86.35%\n",
      "Epoch: 476, Loss: 0.03, accuracy: 86.37%\n",
      "Epoch: 477, Loss: 0.03, accuracy: 86.38%\n",
      "Epoch: 478, Loss: 0.03, accuracy: 86.38%\n",
      "Epoch: 479, Loss: 0.03, accuracy: 86.41%\n",
      "Epoch: 480, Loss: 0.03, accuracy: 86.42%\n",
      "Epoch: 481, Loss: 0.03, accuracy: 86.43%\n",
      "Epoch: 482, Loss: 0.03, accuracy: 86.45%\n",
      "Epoch: 483, Loss: 0.03, accuracy: 86.45%\n",
      "Epoch: 484, Loss: 0.03, accuracy: 86.47%\n",
      "Epoch: 485, Loss: 0.03, accuracy: 86.48%\n",
      "Epoch: 486, Loss: 0.03, accuracy: 86.50%\n",
      "Epoch: 487, Loss: 0.03, accuracy: 86.51%\n",
      "Epoch: 488, Loss: 0.03, accuracy: 86.51%\n",
      "Epoch: 489, Loss: 0.03, accuracy: 86.52%\n",
      "Epoch: 490, Loss: 0.03, accuracy: 86.55%\n",
      "Epoch: 491, Loss: 0.03, accuracy: 86.56%\n",
      "Epoch: 492, Loss: 0.03, accuracy: 86.57%\n",
      "Epoch: 493, Loss: 0.03, accuracy: 86.58%\n",
      "Epoch: 494, Loss: 0.03, accuracy: 86.59%\n",
      "Epoch: 495, Loss: 0.03, accuracy: 86.60%\n",
      "Epoch: 496, Loss: 0.03, accuracy: 86.61%\n",
      "Epoch: 497, Loss: 0.03, accuracy: 86.63%\n",
      "Epoch: 498, Loss: 0.03, accuracy: 86.64%\n",
      "Epoch: 499, Loss: 0.03, accuracy: 86.66%\n",
      "Epoch: 500, Loss: 0.03, accuracy: 86.67%\n",
      "Epoch: 501, Loss: 0.03, accuracy: 86.68%\n",
      "Epoch: 502, Loss: 0.03, accuracy: 86.69%\n",
      "Epoch: 503, Loss: 0.03, accuracy: 86.71%\n",
      "Epoch: 504, Loss: 0.03, accuracy: 86.72%\n",
      "Epoch: 505, Loss: 0.03, accuracy: 86.75%\n",
      "Epoch: 506, Loss: 0.03, accuracy: 86.76%\n",
      "Epoch: 507, Loss: 0.03, accuracy: 86.78%\n",
      "Epoch: 508, Loss: 0.03, accuracy: 86.79%\n",
      "Epoch: 509, Loss: 0.03, accuracy: 86.79%\n",
      "Epoch: 510, Loss: 0.03, accuracy: 86.80%\n",
      "Epoch: 511, Loss: 0.03, accuracy: 86.81%\n",
      "Epoch: 512, Loss: 0.03, accuracy: 86.83%\n",
      "Epoch: 513, Loss: 0.03, accuracy: 86.84%\n",
      "Epoch: 514, Loss: 0.03, accuracy: 86.85%\n",
      "Epoch: 515, Loss: 0.03, accuracy: 86.87%\n",
      "Epoch: 516, Loss: 0.03, accuracy: 86.88%\n",
      "Epoch: 517, Loss: 0.03, accuracy: 86.89%\n",
      "Epoch: 518, Loss: 0.03, accuracy: 86.90%\n",
      "Epoch: 519, Loss: 0.03, accuracy: 86.91%\n",
      "Epoch: 520, Loss: 0.03, accuracy: 86.92%\n",
      "Epoch: 521, Loss: 0.03, accuracy: 86.93%\n",
      "Epoch: 522, Loss: 0.03, accuracy: 86.94%\n",
      "Epoch: 523, Loss: 0.03, accuracy: 86.95%\n",
      "Epoch: 524, Loss: 0.03, accuracy: 86.97%\n",
      "Epoch: 525, Loss: 0.03, accuracy: 86.98%\n",
      "Epoch: 526, Loss: 0.03, accuracy: 87.00%\n",
      "Epoch: 527, Loss: 0.03, accuracy: 87.01%\n",
      "Epoch: 528, Loss: 0.03, accuracy: 87.03%\n",
      "Epoch: 529, Loss: 0.03, accuracy: 87.05%\n",
      "Epoch: 530, Loss: 0.03, accuracy: 87.07%\n",
      "Epoch: 531, Loss: 0.02, accuracy: 87.09%\n",
      "Epoch: 532, Loss: 0.02, accuracy: 87.09%\n",
      "Epoch: 533, Loss: 0.02, accuracy: 87.11%\n",
      "Epoch: 534, Loss: 0.02, accuracy: 87.11%\n",
      "Epoch: 535, Loss: 0.02, accuracy: 87.13%\n",
      "Epoch: 536, Loss: 0.02, accuracy: 87.13%\n",
      "Epoch: 537, Loss: 0.02, accuracy: 87.14%\n",
      "Epoch: 538, Loss: 0.02, accuracy: 87.15%\n",
      "Epoch: 539, Loss: 0.02, accuracy: 87.17%\n",
      "Epoch: 540, Loss: 0.02, accuracy: 87.18%\n",
      "Epoch: 541, Loss: 0.02, accuracy: 87.19%\n",
      "Epoch: 542, Loss: 0.02, accuracy: 87.19%\n",
      "Epoch: 543, Loss: 0.02, accuracy: 87.21%\n",
      "Epoch: 544, Loss: 0.02, accuracy: 87.22%\n",
      "Epoch: 545, Loss: 0.02, accuracy: 87.23%\n",
      "Epoch: 546, Loss: 0.02, accuracy: 87.24%\n",
      "Epoch: 547, Loss: 0.02, accuracy: 87.25%\n",
      "Epoch: 548, Loss: 0.02, accuracy: 87.26%\n",
      "Epoch: 549, Loss: 0.02, accuracy: 87.28%\n",
      "Epoch: 550, Loss: 0.02, accuracy: 87.29%\n",
      "Epoch: 551, Loss: 0.02, accuracy: 87.30%\n",
      "Epoch: 552, Loss: 0.02, accuracy: 87.32%\n",
      "Epoch: 553, Loss: 0.02, accuracy: 87.34%\n",
      "Epoch: 554, Loss: 0.02, accuracy: 87.35%\n",
      "Epoch: 555, Loss: 0.02, accuracy: 87.37%\n",
      "Epoch: 556, Loss: 0.02, accuracy: 87.38%\n",
      "Epoch: 557, Loss: 0.02, accuracy: 87.39%\n",
      "Epoch: 558, Loss: 0.02, accuracy: 87.40%\n",
      "Epoch: 559, Loss: 0.02, accuracy: 87.42%\n",
      "Epoch: 560, Loss: 0.02, accuracy: 87.43%\n",
      "Epoch: 561, Loss: 0.02, accuracy: 87.44%\n",
      "Epoch: 562, Loss: 0.02, accuracy: 87.44%\n",
      "Epoch: 563, Loss: 0.02, accuracy: 87.45%\n",
      "Epoch: 564, Loss: 0.02, accuracy: 87.46%\n",
      "Epoch: 565, Loss: 0.02, accuracy: 87.47%\n",
      "Epoch: 566, Loss: 0.02, accuracy: 87.48%\n",
      "Epoch: 567, Loss: 0.02, accuracy: 87.49%\n",
      "Epoch: 568, Loss: 0.02, accuracy: 87.50%\n",
      "Epoch: 569, Loss: 0.02, accuracy: 87.51%\n",
      "Epoch: 570, Loss: 0.02, accuracy: 87.52%\n",
      "Epoch: 571, Loss: 0.02, accuracy: 87.52%\n",
      "Epoch: 572, Loss: 0.02, accuracy: 87.53%\n",
      "Epoch: 573, Loss: 0.02, accuracy: 87.54%\n",
      "Epoch: 574, Loss: 0.02, accuracy: 87.55%\n",
      "Epoch: 575, Loss: 0.02, accuracy: 87.56%\n",
      "Epoch: 576, Loss: 0.02, accuracy: 87.57%\n",
      "Epoch: 577, Loss: 0.02, accuracy: 87.58%\n",
      "Epoch: 578, Loss: 0.02, accuracy: 87.59%\n",
      "Epoch: 579, Loss: 0.02, accuracy: 87.60%\n",
      "Epoch: 580, Loss: 0.02, accuracy: 87.61%\n",
      "Epoch: 581, Loss: 0.02, accuracy: 87.62%\n",
      "Epoch: 582, Loss: 0.02, accuracy: 87.63%\n",
      "Epoch: 583, Loss: 0.02, accuracy: 87.64%\n",
      "Epoch: 584, Loss: 0.02, accuracy: 87.65%\n",
      "Epoch: 585, Loss: 0.02, accuracy: 87.66%\n",
      "Epoch: 586, Loss: 0.02, accuracy: 87.67%\n",
      "Epoch: 587, Loss: 0.02, accuracy: 87.68%\n",
      "Epoch: 588, Loss: 0.02, accuracy: 87.69%\n",
      "Epoch: 589, Loss: 0.02, accuracy: 87.71%\n",
      "Epoch: 590, Loss: 0.02, accuracy: 87.71%\n",
      "Epoch: 591, Loss: 0.02, accuracy: 87.73%\n",
      "Epoch: 592, Loss: 0.02, accuracy: 87.74%\n",
      "Epoch: 593, Loss: 0.02, accuracy: 87.74%\n",
      "Epoch: 594, Loss: 0.02, accuracy: 87.76%\n",
      "Epoch: 595, Loss: 0.02, accuracy: 87.78%\n",
      "Epoch: 596, Loss: 0.02, accuracy: 87.79%\n",
      "Epoch: 597, Loss: 0.02, accuracy: 87.81%\n",
      "Epoch: 598, Loss: 0.02, accuracy: 87.82%\n",
      "Epoch: 599, Loss: 0.02, accuracy: 87.83%\n",
      "Epoch: 600, Loss: 0.02, accuracy: 87.84%\n",
      "Epoch: 601, Loss: 0.02, accuracy: 87.85%\n",
      "Epoch: 602, Loss: 0.02, accuracy: 87.85%\n",
      "Epoch: 603, Loss: 0.02, accuracy: 87.86%\n",
      "Epoch: 604, Loss: 0.02, accuracy: 87.87%\n",
      "Epoch: 605, Loss: 0.02, accuracy: 87.88%\n",
      "Epoch: 606, Loss: 0.02, accuracy: 87.88%\n",
      "Epoch: 607, Loss: 0.02, accuracy: 87.89%\n",
      "Epoch: 608, Loss: 0.02, accuracy: 87.90%\n",
      "Epoch: 609, Loss: 0.02, accuracy: 87.91%\n",
      "Epoch: 610, Loss: 0.02, accuracy: 87.92%\n",
      "Epoch: 611, Loss: 0.02, accuracy: 87.93%\n",
      "Epoch: 612, Loss: 0.02, accuracy: 87.93%\n",
      "Epoch: 613, Loss: 0.02, accuracy: 87.94%\n",
      "Epoch: 614, Loss: 0.02, accuracy: 87.95%\n",
      "Epoch: 615, Loss: 0.02, accuracy: 87.95%\n",
      "Epoch: 616, Loss: 0.02, accuracy: 87.96%\n",
      "Epoch: 617, Loss: 0.02, accuracy: 87.98%\n",
      "Epoch: 618, Loss: 0.02, accuracy: 87.98%\n",
      "Epoch: 619, Loss: 0.02, accuracy: 87.99%\n",
      "Epoch: 620, Loss: 0.02, accuracy: 88.00%\n",
      "Epoch: 621, Loss: 0.02, accuracy: 88.01%\n",
      "Epoch: 622, Loss: 0.02, accuracy: 88.02%\n",
      "Epoch: 623, Loss: 0.02, accuracy: 88.04%\n",
      "Epoch: 624, Loss: 0.02, accuracy: 88.04%\n",
      "Epoch: 625, Loss: 0.02, accuracy: 88.04%\n",
      "Epoch: 626, Loss: 0.02, accuracy: 88.06%\n",
      "Epoch: 627, Loss: 0.02, accuracy: 88.07%\n",
      "Epoch: 628, Loss: 0.02, accuracy: 88.09%\n",
      "Epoch: 629, Loss: 0.02, accuracy: 88.09%\n",
      "Epoch: 630, Loss: 0.02, accuracy: 88.10%\n",
      "Epoch: 631, Loss: 0.02, accuracy: 88.11%\n",
      "Epoch: 632, Loss: 0.02, accuracy: 88.12%\n",
      "Epoch: 633, Loss: 0.02, accuracy: 88.13%\n",
      "Epoch: 634, Loss: 0.02, accuracy: 88.13%\n",
      "Epoch: 635, Loss: 0.02, accuracy: 88.13%\n",
      "Epoch: 636, Loss: 0.02, accuracy: 88.15%\n",
      "Epoch: 637, Loss: 0.02, accuracy: 88.16%\n",
      "Epoch: 638, Loss: 0.02, accuracy: 88.17%\n",
      "Epoch: 639, Loss: 0.02, accuracy: 88.17%\n",
      "Epoch: 640, Loss: 0.02, accuracy: 88.19%\n",
      "Epoch: 641, Loss: 0.02, accuracy: 88.20%\n",
      "Epoch: 642, Loss: 0.02, accuracy: 88.20%\n",
      "Epoch: 643, Loss: 0.02, accuracy: 88.21%\n",
      "Epoch: 644, Loss: 0.02, accuracy: 88.21%\n",
      "Epoch: 645, Loss: 0.02, accuracy: 88.22%\n",
      "Epoch: 646, Loss: 0.02, accuracy: 88.23%\n",
      "Epoch: 647, Loss: 0.02, accuracy: 88.24%\n",
      "Epoch: 648, Loss: 0.02, accuracy: 88.25%\n",
      "Epoch: 649, Loss: 0.02, accuracy: 88.26%\n",
      "Epoch: 650, Loss: 0.02, accuracy: 88.27%\n",
      "Epoch: 651, Loss: 0.02, accuracy: 88.28%\n",
      "Epoch: 652, Loss: 0.02, accuracy: 88.29%\n",
      "Epoch: 653, Loss: 0.02, accuracy: 88.31%\n",
      "Epoch: 654, Loss: 0.02, accuracy: 88.31%\n",
      "Epoch: 655, Loss: 0.02, accuracy: 88.32%\n",
      "Epoch: 656, Loss: 0.02, accuracy: 88.33%\n",
      "Epoch: 657, Loss: 0.02, accuracy: 88.34%\n",
      "Epoch: 658, Loss: 0.02, accuracy: 88.35%\n",
      "Epoch: 659, Loss: 0.02, accuracy: 88.36%\n",
      "Epoch: 660, Loss: 0.02, accuracy: 88.36%\n",
      "Epoch: 661, Loss: 0.02, accuracy: 88.37%\n",
      "Epoch: 662, Loss: 0.02, accuracy: 88.38%\n",
      "Epoch: 663, Loss: 0.02, accuracy: 88.38%\n",
      "Epoch: 664, Loss: 0.02, accuracy: 88.39%\n",
      "Epoch: 665, Loss: 0.02, accuracy: 88.39%\n",
      "Epoch: 666, Loss: 0.02, accuracy: 88.40%\n",
      "Epoch: 667, Loss: 0.02, accuracy: 88.41%\n",
      "Epoch: 668, Loss: 0.02, accuracy: 88.42%\n",
      "Epoch: 669, Loss: 0.02, accuracy: 88.42%\n",
      "Epoch: 670, Loss: 0.02, accuracy: 88.44%\n",
      "Epoch: 671, Loss: 0.02, accuracy: 88.45%\n",
      "Epoch: 672, Loss: 0.02, accuracy: 88.45%\n",
      "Epoch: 673, Loss: 0.02, accuracy: 88.47%\n",
      "Epoch: 674, Loss: 0.02, accuracy: 88.48%\n",
      "Epoch: 675, Loss: 0.02, accuracy: 88.49%\n",
      "Epoch: 676, Loss: 0.02, accuracy: 88.49%\n",
      "Epoch: 677, Loss: 0.02, accuracy: 88.49%\n",
      "Epoch: 678, Loss: 0.02, accuracy: 88.50%\n",
      "Epoch: 679, Loss: 0.02, accuracy: 88.50%\n",
      "Epoch: 680, Loss: 0.02, accuracy: 88.51%\n",
      "Epoch: 681, Loss: 0.02, accuracy: 88.52%\n",
      "Epoch: 682, Loss: 0.02, accuracy: 88.53%\n",
      "Epoch: 683, Loss: 0.02, accuracy: 88.54%\n",
      "Epoch: 684, Loss: 0.02, accuracy: 88.54%\n",
      "Epoch: 685, Loss: 0.02, accuracy: 88.55%\n",
      "Epoch: 686, Loss: 0.02, accuracy: 88.55%\n",
      "Epoch: 687, Loss: 0.02, accuracy: 88.56%\n",
      "Epoch: 688, Loss: 0.02, accuracy: 88.56%\n",
      "Epoch: 689, Loss: 0.02, accuracy: 88.56%\n",
      "Epoch: 690, Loss: 0.02, accuracy: 88.58%\n",
      "Epoch: 691, Loss: 0.02, accuracy: 88.59%\n",
      "Epoch: 692, Loss: 0.02, accuracy: 88.60%\n",
      "Epoch: 693, Loss: 0.02, accuracy: 88.60%\n",
      "Epoch: 694, Loss: 0.02, accuracy: 88.61%\n",
      "Epoch: 695, Loss: 0.02, accuracy: 88.62%\n",
      "Epoch: 696, Loss: 0.02, accuracy: 88.62%\n",
      "Epoch: 697, Loss: 0.02, accuracy: 88.65%\n",
      "Epoch: 698, Loss: 0.02, accuracy: 88.66%\n",
      "Epoch: 699, Loss: 0.02, accuracy: 88.67%\n",
      "Epoch: 700, Loss: 0.02, accuracy: 88.67%\n",
      "Epoch: 701, Loss: 0.02, accuracy: 88.68%\n",
      "Epoch: 702, Loss: 0.02, accuracy: 88.69%\n",
      "Epoch: 703, Loss: 0.02, accuracy: 88.69%\n",
      "Epoch: 704, Loss: 0.02, accuracy: 88.70%\n",
      "Epoch: 705, Loss: 0.02, accuracy: 88.70%\n",
      "Epoch: 706, Loss: 0.02, accuracy: 88.70%\n",
      "Epoch: 707, Loss: 0.02, accuracy: 88.70%\n",
      "Epoch: 708, Loss: 0.02, accuracy: 88.72%\n",
      "Epoch: 709, Loss: 0.02, accuracy: 88.73%\n",
      "Epoch: 710, Loss: 0.02, accuracy: 88.73%\n",
      "Epoch: 711, Loss: 0.02, accuracy: 88.74%\n",
      "Epoch: 712, Loss: 0.02, accuracy: 88.74%\n",
      "Epoch: 713, Loss: 0.02, accuracy: 88.75%\n",
      "Epoch: 714, Loss: 0.02, accuracy: 88.75%\n",
      "Epoch: 715, Loss: 0.02, accuracy: 88.75%\n",
      "Epoch: 716, Loss: 0.02, accuracy: 88.76%\n",
      "Epoch: 717, Loss: 0.02, accuracy: 88.77%\n",
      "Epoch: 718, Loss: 0.02, accuracy: 88.77%\n",
      "Epoch: 719, Loss: 0.02, accuracy: 88.78%\n",
      "Epoch: 720, Loss: 0.02, accuracy: 88.79%\n",
      "Epoch: 721, Loss: 0.02, accuracy: 88.80%\n",
      "Epoch: 722, Loss: 0.02, accuracy: 88.80%\n",
      "Epoch: 723, Loss: 0.02, accuracy: 88.81%\n",
      "Epoch: 724, Loss: 0.02, accuracy: 88.81%\n",
      "Epoch: 725, Loss: 0.02, accuracy: 88.81%\n",
      "Epoch: 726, Loss: 0.02, accuracy: 88.82%\n",
      "Epoch: 727, Loss: 0.02, accuracy: 88.83%\n",
      "Epoch: 728, Loss: 0.02, accuracy: 88.84%\n",
      "Epoch: 729, Loss: 0.02, accuracy: 88.84%\n",
      "Epoch: 730, Loss: 0.02, accuracy: 88.84%\n",
      "Epoch: 731, Loss: 0.02, accuracy: 88.85%\n",
      "Epoch: 732, Loss: 0.02, accuracy: 88.85%\n",
      "Epoch: 733, Loss: 0.02, accuracy: 88.85%\n",
      "Epoch: 734, Loss: 0.02, accuracy: 88.86%\n",
      "Epoch: 735, Loss: 0.02, accuracy: 88.88%\n",
      "Epoch: 736, Loss: 0.02, accuracy: 88.89%\n",
      "Epoch: 737, Loss: 0.02, accuracy: 88.90%\n",
      "Epoch: 738, Loss: 0.02, accuracy: 88.90%\n",
      "Epoch: 739, Loss: 0.02, accuracy: 88.90%\n",
      "Epoch: 740, Loss: 0.02, accuracy: 88.90%\n",
      "Epoch: 741, Loss: 0.02, accuracy: 88.91%\n",
      "Epoch: 742, Loss: 0.02, accuracy: 88.91%\n",
      "Epoch: 743, Loss: 0.02, accuracy: 88.92%\n",
      "Epoch: 744, Loss: 0.02, accuracy: 88.92%\n",
      "Epoch: 745, Loss: 0.02, accuracy: 88.93%\n",
      "Epoch: 746, Loss: 0.02, accuracy: 88.94%\n",
      "Epoch: 747, Loss: 0.02, accuracy: 88.95%\n",
      "Epoch: 748, Loss: 0.02, accuracy: 88.96%\n",
      "Epoch: 749, Loss: 0.02, accuracy: 88.96%\n",
      "Epoch: 750, Loss: 0.02, accuracy: 88.97%\n",
      "Epoch: 751, Loss: 0.02, accuracy: 88.98%\n",
      "Epoch: 752, Loss: 0.02, accuracy: 88.99%\n",
      "Epoch: 753, Loss: 0.02, accuracy: 88.99%\n",
      "Epoch: 754, Loss: 0.02, accuracy: 89.00%\n",
      "Epoch: 755, Loss: 0.02, accuracy: 89.01%\n",
      "Epoch: 756, Loss: 0.02, accuracy: 89.03%\n",
      "Epoch: 757, Loss: 0.02, accuracy: 89.04%\n",
      "Epoch: 758, Loss: 0.02, accuracy: 89.05%\n",
      "Epoch: 759, Loss: 0.02, accuracy: 89.06%\n",
      "Epoch: 760, Loss: 0.02, accuracy: 89.07%\n",
      "Epoch: 761, Loss: 0.02, accuracy: 89.08%\n",
      "Epoch: 762, Loss: 0.02, accuracy: 89.08%\n",
      "Epoch: 763, Loss: 0.02, accuracy: 89.08%\n",
      "Epoch: 764, Loss: 0.02, accuracy: 89.09%\n",
      "Epoch: 765, Loss: 0.02, accuracy: 89.09%\n",
      "Epoch: 766, Loss: 0.02, accuracy: 89.09%\n",
      "Epoch: 767, Loss: 0.02, accuracy: 89.10%\n",
      "Epoch: 768, Loss: 0.02, accuracy: 89.11%\n",
      "Epoch: 769, Loss: 0.02, accuracy: 89.11%\n",
      "Epoch: 770, Loss: 0.02, accuracy: 89.12%\n",
      "Epoch: 771, Loss: 0.02, accuracy: 89.13%\n",
      "Epoch: 772, Loss: 0.02, accuracy: 89.14%\n",
      "Epoch: 773, Loss: 0.02, accuracy: 89.14%\n",
      "Epoch: 774, Loss: 0.02, accuracy: 89.15%\n",
      "Epoch: 775, Loss: 0.02, accuracy: 89.16%\n",
      "Epoch: 776, Loss: 0.02, accuracy: 89.16%\n",
      "Epoch: 777, Loss: 0.02, accuracy: 89.16%\n",
      "Epoch: 778, Loss: 0.02, accuracy: 89.17%\n",
      "Epoch: 779, Loss: 0.02, accuracy: 89.18%\n",
      "Epoch: 780, Loss: 0.02, accuracy: 89.18%\n",
      "Epoch: 781, Loss: 0.02, accuracy: 89.19%\n",
      "Epoch: 782, Loss: 0.02, accuracy: 89.19%\n",
      "Epoch: 783, Loss: 0.02, accuracy: 89.20%\n",
      "Epoch: 784, Loss: 0.02, accuracy: 89.20%\n",
      "Epoch: 785, Loss: 0.02, accuracy: 89.21%\n",
      "Epoch: 786, Loss: 0.02, accuracy: 89.21%\n",
      "Epoch: 787, Loss: 0.02, accuracy: 89.22%\n",
      "Epoch: 788, Loss: 0.02, accuracy: 89.23%\n",
      "Epoch: 789, Loss: 0.02, accuracy: 89.23%\n",
      "Epoch: 790, Loss: 0.02, accuracy: 89.24%\n",
      "Epoch: 791, Loss: 0.02, accuracy: 89.25%\n",
      "Epoch: 792, Loss: 0.02, accuracy: 89.25%\n",
      "Epoch: 793, Loss: 0.02, accuracy: 89.26%\n",
      "Epoch: 794, Loss: 0.02, accuracy: 89.27%\n",
      "Epoch: 795, Loss: 0.02, accuracy: 89.27%\n",
      "Epoch: 796, Loss: 0.02, accuracy: 89.28%\n",
      "Epoch: 797, Loss: 0.02, accuracy: 89.28%\n",
      "Epoch: 798, Loss: 0.02, accuracy: 89.29%\n",
      "Epoch: 799, Loss: 0.02, accuracy: 89.29%\n",
      "Epoch: 800, Loss: 0.02, accuracy: 89.30%\n",
      "Epoch: 801, Loss: 0.02, accuracy: 89.30%\n",
      "Epoch: 802, Loss: 0.02, accuracy: 89.30%\n",
      "Epoch: 803, Loss: 0.02, accuracy: 89.31%\n",
      "Epoch: 804, Loss: 0.02, accuracy: 89.33%\n",
      "Epoch: 805, Loss: 0.02, accuracy: 89.33%\n",
      "Epoch: 806, Loss: 0.02, accuracy: 89.34%\n",
      "Epoch: 807, Loss: 0.02, accuracy: 89.34%\n",
      "Epoch: 808, Loss: 0.02, accuracy: 89.35%\n",
      "Epoch: 809, Loss: 0.02, accuracy: 89.35%\n",
      "Epoch: 810, Loss: 0.02, accuracy: 89.36%\n",
      "Epoch: 811, Loss: 0.02, accuracy: 89.36%\n",
      "Epoch: 812, Loss: 0.02, accuracy: 89.37%\n",
      "Epoch: 813, Loss: 0.02, accuracy: 89.38%\n",
      "Epoch: 814, Loss: 0.02, accuracy: 89.39%\n",
      "Epoch: 815, Loss: 0.02, accuracy: 89.40%\n",
      "Epoch: 816, Loss: 0.02, accuracy: 89.41%\n",
      "Epoch: 817, Loss: 0.02, accuracy: 89.41%\n",
      "Epoch: 818, Loss: 0.02, accuracy: 89.41%\n",
      "Epoch: 819, Loss: 0.02, accuracy: 89.42%\n",
      "Epoch: 820, Loss: 0.02, accuracy: 89.42%\n",
      "Epoch: 821, Loss: 0.02, accuracy: 89.42%\n",
      "Epoch: 822, Loss: 0.02, accuracy: 89.42%\n",
      "Epoch: 823, Loss: 0.02, accuracy: 89.43%\n",
      "Epoch: 824, Loss: 0.02, accuracy: 89.44%\n",
      "Epoch: 825, Loss: 0.02, accuracy: 89.45%\n",
      "Epoch: 826, Loss: 0.02, accuracy: 89.46%\n",
      "Epoch: 827, Loss: 0.02, accuracy: 89.46%\n",
      "Epoch: 828, Loss: 0.02, accuracy: 89.47%\n",
      "Epoch: 829, Loss: 0.02, accuracy: 89.48%\n",
      "Epoch: 830, Loss: 0.02, accuracy: 89.49%\n",
      "Epoch: 831, Loss: 0.02, accuracy: 89.50%\n",
      "Epoch: 832, Loss: 0.02, accuracy: 89.50%\n",
      "Epoch: 833, Loss: 0.02, accuracy: 89.51%\n",
      "Epoch: 834, Loss: 0.02, accuracy: 89.51%\n",
      "Epoch: 835, Loss: 0.02, accuracy: 89.52%\n",
      "Epoch: 836, Loss: 0.02, accuracy: 89.53%\n",
      "Epoch: 837, Loss: 0.02, accuracy: 89.53%\n",
      "Epoch: 838, Loss: 0.02, accuracy: 89.53%\n",
      "Epoch: 839, Loss: 0.02, accuracy: 89.54%\n",
      "Epoch: 840, Loss: 0.02, accuracy: 89.54%\n",
      "Epoch: 841, Loss: 0.02, accuracy: 89.55%\n",
      "Epoch: 842, Loss: 0.02, accuracy: 89.55%\n",
      "Epoch: 843, Loss: 0.02, accuracy: 89.56%\n",
      "Epoch: 844, Loss: 0.02, accuracy: 89.56%\n",
      "Epoch: 845, Loss: 0.02, accuracy: 89.57%\n",
      "Epoch: 846, Loss: 0.02, accuracy: 89.57%\n",
      "Epoch: 847, Loss: 0.02, accuracy: 89.57%\n",
      "Epoch: 848, Loss: 0.02, accuracy: 89.58%\n",
      "Epoch: 849, Loss: 0.02, accuracy: 89.58%\n",
      "Epoch: 850, Loss: 0.02, accuracy: 89.59%\n",
      "Epoch: 851, Loss: 0.02, accuracy: 89.59%\n",
      "Epoch: 852, Loss: 0.02, accuracy: 89.60%\n",
      "Epoch: 853, Loss: 0.02, accuracy: 89.60%\n",
      "Epoch: 854, Loss: 0.02, accuracy: 89.60%\n",
      "Epoch: 855, Loss: 0.02, accuracy: 89.61%\n",
      "Epoch: 856, Loss: 0.02, accuracy: 89.61%\n",
      "Epoch: 857, Loss: 0.02, accuracy: 89.62%\n",
      "Epoch: 858, Loss: 0.02, accuracy: 89.62%\n",
      "Epoch: 859, Loss: 0.02, accuracy: 89.63%\n",
      "Epoch: 860, Loss: 0.02, accuracy: 89.64%\n",
      "Epoch: 861, Loss: 0.02, accuracy: 89.65%\n",
      "Epoch: 862, Loss: 0.02, accuracy: 89.65%\n",
      "Epoch: 863, Loss: 0.02, accuracy: 89.65%\n",
      "Epoch: 864, Loss: 0.02, accuracy: 89.66%\n",
      "Epoch: 865, Loss: 0.02, accuracy: 89.66%\n",
      "Epoch: 866, Loss: 0.02, accuracy: 89.66%\n",
      "Epoch: 867, Loss: 0.02, accuracy: 89.67%\n",
      "Epoch: 868, Loss: 0.02, accuracy: 89.67%\n",
      "Epoch: 869, Loss: 0.02, accuracy: 89.68%\n",
      "Epoch: 870, Loss: 0.02, accuracy: 89.68%\n",
      "Epoch: 871, Loss: 0.02, accuracy: 89.69%\n",
      "Epoch: 872, Loss: 0.02, accuracy: 89.70%\n",
      "Epoch: 873, Loss: 0.02, accuracy: 89.70%\n",
      "Epoch: 874, Loss: 0.02, accuracy: 89.71%\n",
      "Epoch: 875, Loss: 0.02, accuracy: 89.71%\n",
      "Epoch: 876, Loss: 0.02, accuracy: 89.72%\n",
      "Epoch: 877, Loss: 0.02, accuracy: 89.73%\n",
      "Epoch: 878, Loss: 0.02, accuracy: 89.74%\n",
      "Epoch: 879, Loss: 0.02, accuracy: 89.75%\n",
      "Epoch: 880, Loss: 0.02, accuracy: 89.76%\n",
      "Epoch: 881, Loss: 0.02, accuracy: 89.76%\n",
      "Epoch: 882, Loss: 0.02, accuracy: 89.77%\n",
      "Epoch: 883, Loss: 0.02, accuracy: 89.78%\n",
      "Epoch: 884, Loss: 0.02, accuracy: 89.78%\n",
      "Epoch: 885, Loss: 0.02, accuracy: 89.78%\n",
      "Epoch: 886, Loss: 0.02, accuracy: 89.78%\n",
      "Epoch: 887, Loss: 0.02, accuracy: 89.79%\n",
      "Epoch: 888, Loss: 0.02, accuracy: 89.80%\n",
      "Epoch: 889, Loss: 0.02, accuracy: 89.80%\n",
      "Epoch: 890, Loss: 0.02, accuracy: 89.81%\n",
      "Epoch: 891, Loss: 0.02, accuracy: 89.81%\n",
      "Epoch: 892, Loss: 0.02, accuracy: 89.82%\n",
      "Epoch: 893, Loss: 0.02, accuracy: 89.83%\n",
      "Epoch: 894, Loss: 0.02, accuracy: 89.84%\n",
      "Epoch: 895, Loss: 0.02, accuracy: 89.84%\n",
      "Epoch: 896, Loss: 0.02, accuracy: 89.85%\n",
      "Epoch: 897, Loss: 0.02, accuracy: 89.86%\n",
      "Epoch: 898, Loss: 0.02, accuracy: 89.86%\n",
      "Epoch: 899, Loss: 0.02, accuracy: 89.86%\n",
      "Epoch: 900, Loss: 0.02, accuracy: 89.87%\n",
      "Epoch: 901, Loss: 0.02, accuracy: 89.88%\n",
      "Epoch: 902, Loss: 0.02, accuracy: 89.88%\n",
      "Epoch: 903, Loss: 0.02, accuracy: 89.88%\n",
      "Epoch: 904, Loss: 0.02, accuracy: 89.88%\n",
      "Epoch: 905, Loss: 0.02, accuracy: 89.89%\n",
      "Epoch: 906, Loss: 0.02, accuracy: 89.89%\n",
      "Epoch: 907, Loss: 0.02, accuracy: 89.89%\n",
      "Epoch: 908, Loss: 0.02, accuracy: 89.89%\n",
      "Epoch: 909, Loss: 0.02, accuracy: 89.89%\n",
      "Epoch: 910, Loss: 0.02, accuracy: 89.90%\n",
      "Epoch: 911, Loss: 0.02, accuracy: 89.91%\n",
      "Epoch: 912, Loss: 0.02, accuracy: 89.91%\n",
      "Epoch: 913, Loss: 0.02, accuracy: 89.92%\n",
      "Epoch: 914, Loss: 0.02, accuracy: 89.92%\n",
      "Epoch: 915, Loss: 0.02, accuracy: 89.93%\n",
      "Epoch: 916, Loss: 0.02, accuracy: 89.93%\n",
      "Epoch: 917, Loss: 0.02, accuracy: 89.94%\n",
      "Epoch: 918, Loss: 0.02, accuracy: 89.94%\n",
      "Epoch: 919, Loss: 0.02, accuracy: 89.94%\n",
      "Epoch: 920, Loss: 0.02, accuracy: 89.94%\n",
      "Epoch: 921, Loss: 0.02, accuracy: 89.95%\n",
      "Epoch: 922, Loss: 0.02, accuracy: 89.95%\n",
      "Epoch: 923, Loss: 0.02, accuracy: 89.96%\n",
      "Epoch: 924, Loss: 0.02, accuracy: 89.96%\n",
      "Epoch: 925, Loss: 0.02, accuracy: 89.97%\n",
      "Epoch: 926, Loss: 0.02, accuracy: 89.98%\n",
      "Epoch: 927, Loss: 0.02, accuracy: 89.99%\n",
      "Epoch: 928, Loss: 0.02, accuracy: 90.00%\n",
      "Epoch: 929, Loss: 0.02, accuracy: 90.00%\n",
      "Epoch: 930, Loss: 0.02, accuracy: 90.01%\n",
      "Epoch: 931, Loss: 0.02, accuracy: 90.01%\n",
      "Epoch: 932, Loss: 0.02, accuracy: 90.01%\n",
      "Epoch: 933, Loss: 0.02, accuracy: 90.01%\n",
      "Epoch: 934, Loss: 0.02, accuracy: 90.03%\n",
      "Epoch: 935, Loss: 0.02, accuracy: 90.03%\n",
      "Epoch: 936, Loss: 0.02, accuracy: 90.03%\n",
      "Epoch: 937, Loss: 0.02, accuracy: 90.04%\n",
      "Epoch: 938, Loss: 0.02, accuracy: 90.05%\n",
      "Epoch: 939, Loss: 0.02, accuracy: 90.05%\n",
      "Epoch: 940, Loss: 0.02, accuracy: 90.05%\n",
      "Epoch: 941, Loss: 0.02, accuracy: 90.06%\n",
      "Epoch: 942, Loss: 0.02, accuracy: 90.06%\n",
      "Epoch: 943, Loss: 0.02, accuracy: 90.07%\n",
      "Epoch: 944, Loss: 0.02, accuracy: 90.08%\n",
      "Epoch: 945, Loss: 0.02, accuracy: 90.08%\n",
      "Epoch: 946, Loss: 0.02, accuracy: 90.08%\n",
      "Epoch: 947, Loss: 0.02, accuracy: 90.08%\n",
      "Epoch: 948, Loss: 0.02, accuracy: 90.09%\n",
      "Epoch: 949, Loss: 0.02, accuracy: 90.09%\n",
      "Epoch: 950, Loss: 0.02, accuracy: 90.09%\n",
      "Epoch: 951, Loss: 0.02, accuracy: 90.09%\n",
      "Epoch: 952, Loss: 0.02, accuracy: 90.10%\n",
      "Epoch: 953, Loss: 0.02, accuracy: 90.10%\n",
      "Epoch: 954, Loss: 0.02, accuracy: 90.10%\n",
      "Epoch: 955, Loss: 0.02, accuracy: 90.11%\n",
      "Epoch: 956, Loss: 0.02, accuracy: 90.12%\n",
      "Epoch: 957, Loss: 0.02, accuracy: 90.12%\n",
      "Epoch: 958, Loss: 0.02, accuracy: 90.12%\n",
      "Epoch: 959, Loss: 0.02, accuracy: 90.12%\n",
      "Epoch: 960, Loss: 0.02, accuracy: 90.12%\n",
      "Epoch: 961, Loss: 0.02, accuracy: 90.13%\n",
      "Epoch: 962, Loss: 0.02, accuracy: 90.14%\n",
      "Epoch: 963, Loss: 0.02, accuracy: 90.14%\n",
      "Epoch: 964, Loss: 0.02, accuracy: 90.15%\n",
      "Epoch: 965, Loss: 0.02, accuracy: 90.15%\n",
      "Epoch: 966, Loss: 0.02, accuracy: 90.15%\n",
      "Epoch: 967, Loss: 0.02, accuracy: 90.16%\n",
      "Epoch: 968, Loss: 0.02, accuracy: 90.16%\n",
      "Epoch: 969, Loss: 0.02, accuracy: 90.16%\n",
      "Epoch: 970, Loss: 0.02, accuracy: 90.17%\n",
      "Epoch: 971, Loss: 0.02, accuracy: 90.18%\n",
      "Epoch: 972, Loss: 0.02, accuracy: 90.19%\n",
      "Epoch: 973, Loss: 0.02, accuracy: 90.20%\n",
      "Epoch: 974, Loss: 0.02, accuracy: 90.20%\n",
      "Epoch: 975, Loss: 0.02, accuracy: 90.21%\n",
      "Epoch: 976, Loss: 0.02, accuracy: 90.21%\n",
      "Epoch: 977, Loss: 0.02, accuracy: 90.22%\n",
      "Epoch: 978, Loss: 0.02, accuracy: 90.22%\n",
      "Epoch: 979, Loss: 0.02, accuracy: 90.22%\n",
      "Epoch: 980, Loss: 0.02, accuracy: 90.22%\n",
      "Epoch: 981, Loss: 0.02, accuracy: 90.23%\n",
      "Epoch: 982, Loss: 0.02, accuracy: 90.24%\n",
      "Epoch: 983, Loss: 0.02, accuracy: 90.25%\n",
      "Epoch: 984, Loss: 0.02, accuracy: 90.25%\n",
      "Epoch: 985, Loss: 0.02, accuracy: 90.26%\n",
      "Epoch: 986, Loss: 0.02, accuracy: 90.25%\n",
      "Epoch: 987, Loss: 0.02, accuracy: 90.26%\n",
      "Epoch: 988, Loss: 0.02, accuracy: 90.26%\n",
      "Epoch: 989, Loss: 0.02, accuracy: 90.27%\n",
      "Epoch: 990, Loss: 0.02, accuracy: 90.27%\n",
      "Epoch: 991, Loss: 0.02, accuracy: 90.27%\n",
      "Epoch: 992, Loss: 0.02, accuracy: 90.28%\n",
      "Epoch: 993, Loss: 0.02, accuracy: 90.29%\n",
      "Epoch: 994, Loss: 0.02, accuracy: 90.29%\n",
      "Epoch: 995, Loss: 0.02, accuracy: 90.29%\n",
      "Epoch: 996, Loss: 0.02, accuracy: 90.30%\n",
      "Epoch: 997, Loss: 0.02, accuracy: 90.30%\n",
      "Epoch: 998, Loss: 0.02, accuracy: 90.30%\n",
      "Epoch: 999, Loss: 0.02, accuracy: 90.31%\n",
      "Epoch: 1000, Loss: 0.02, accuracy: 90.31%\n",
      "==========================================\n",
      "Test data Accuracy: 89.53%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "mnist_dataset = mnist.load_data()\n",
    "\n",
    "X = mnist_dataset[0][0].reshape(60000, 784) / 255\n",
    "y = mnist_dataset[0][1]\n",
    "y_one_hot = np.eye(10)[y]                       # one hot encode the target labels\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "X_train_normalized = X_train_normalized\n",
    "X_test_normalized = X_test_normalized.T\n",
    "y_train = y_train\n",
    "y_test = y_test.T\n",
    "\n",
    "layers_dim = [784,64,10]\n",
    "lr = 0.2   # Learning rate\n",
    "activation_layer = ReLU\n",
    "loss = MSE\n",
    "enable_bias = True\n",
    "\n",
    "nn = NN(X_train_normalized, y_train, layers_dim, lr, 1000, activation_layer, loss, enable_bias)\n",
    "nn.train()\n",
    "y_pred = nn.predict(X_test_normalized)\n",
    "\n",
    "# Calculate the accuracy of the model with the test data\n",
    "accuracy = calculate_accuracy(y_pred, y_test)\n",
    "print(\"==========================================\")\n",
    "print(f\"Test data Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
