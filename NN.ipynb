{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self,x):\n",
    "        raise NotImplementedError()\n",
    "    def backward(self, out_grad, learning_rate):\n",
    "        raise NotImplementedError()\n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, in_size, out_size, enable_bias=True) -> None:\n",
    "        '''\n",
    "        in_size: the number of datapoints in the input, i.e. the input size\n",
    "        out_size: the number of neurons in the layer, i.e. the output size\n",
    "        enable_bias: whether to include bias in the layer\n",
    "        '''\n",
    "        # intialize weights and bias\n",
    "        self.initialize_parameters(in_size, out_size, enable_bias)\n",
    "\n",
    "    def initialize_parameters(self, in_size, out_size, enable_bias):\n",
    "        '''\n",
    "        reinitialize the weights and bias\n",
    "        '''\n",
    "        #self.weights = kaiming_uniform(out_size, in_size)\n",
    "        #self.bias = kaiming_uniform(out_size, 1) if enable_bias is not None else None\n",
    "        self.weights = np.random.randn(out_size, in_size) * np.sqrt(2/in_size)\n",
    "        self.bias = np.random.randn(out_size, 1) * np.sqrt(2/in_size) if enable_bias is not None else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        input: input to the layer\n",
    "        return: linear transformation of the input, WX + B\n",
    "        '''\n",
    "        self.input = input\n",
    "        return  (self.weights @ input) + (self.bias if self.bias is not None else 0)\n",
    "    \n",
    "    def backward(self, out_grad, learning_rate):\n",
    "        '''\n",
    "        out_grad: gradient of the loss w.r.t the output of the layer\n",
    "        learning_rate: learning rate for updating the weights\n",
    "        return: gradient of the loss w.r.t the input of the layer so that it can be used as input to the previous layer to ues in updating its parameters \n",
    "        '''\n",
    "        new_outgrad = self.weights.T @ out_grad     # gradient of the loss w.r.t the input of the layer\n",
    "        \n",
    "        w_grad = out_grad @ self.input.T            # gradient of the loss w.r.t the weights\n",
    "        self.weights -= learning_rate * w_grad      # update the weights\n",
    "\n",
    "        if (self.bias is not None):\n",
    "            b_grad = np.mean(out_grad, axis=1).reshape(-1,1)    # gradient of the loss w.r.t the bias\n",
    "                                                                # mean to equalize the broadcasting\n",
    "                                                                # reshape from (n,) to (n,1) to make it compatible with the bias shape\n",
    "            self.bias -= learning_rate * b_grad              # update the bias        \n",
    "\n",
    "        return new_outgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        None\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: the input of the RelU layer\n",
    "        return: RelU(x)\n",
    "        '''\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, 0)\n",
    "    \n",
    "    def backward(self, out_grad, learning_rate):\n",
    "        '''\n",
    "        out_grad: the output gradient of the previous layer\n",
    "        return: RelU'(x) âŠ™ outgrad\n",
    "        '''\n",
    "        Relu_prime = np.where(self.x > 0, 1, 0)     # derivative of the Relu function\n",
    "        return np.multiply(Relu_prime, out_grad)    # element-wise multiplication of the derivative of the Relu function and the output gradient of the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    def __init__(self) -> None:\n",
    "        None\n",
    "    def loss(self, y_predict, y):\n",
    "        '''\n",
    "        y_predict: the predicted value\n",
    "        y: the actual value (target): must be one hot encoded\n",
    "        return: the mean squared error loss\n",
    "        '''\n",
    "        return np.mean((y_predict - y)**2)\n",
    "    \n",
    "    def loss_gradient(self, y_predict, y):\n",
    "        '''\n",
    "        y_predict: the predicted value\n",
    "        y: the actual value (target)\n",
    "        return: the gradient of the loss w.r.t the predicted value\n",
    "        '''\n",
    "        no_of_classes = y.shape[0]          # y is transposed\n",
    "        no_of_datapoints = y.shape[1]\n",
    "        return (2/ (no_of_classes * no_of_datapoints)) * (y_predict - y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, x, y, layers_dim, learning_rate, max_iterations, activation_layer, loss, enable_bias=True) -> None:\n",
    "        '''\n",
    "        x: the input data\n",
    "        y: the target data\n",
    "        layers_dim: the dimensions of the layers of the neural network\n",
    "        learning_rate: the learning rate of the neural network\n",
    "        max_iterations: the maximum number of iterations to train the neural network\n",
    "        activation_function: the activation function of the neural network\n",
    "        error: the error function of the neural network\n",
    "        '''\n",
    "        self.layers_dim = layers_dim\n",
    "        self.x = x.T\n",
    "        self.y = y.T\n",
    "        self.activation_layer = activation_layer\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.network = []\n",
    "        self.enable_bias = enable_bias\n",
    "        self.layers = self.initialize_network()\n",
    "    \n",
    "    def initialize_network(self) -> None:\n",
    "        '''\n",
    "        initialize the layers of the neural network\n",
    "        '''\n",
    "        # check for the dimensions of the input and output layers\n",
    "        assert self.layers_dim[0] == self.x.shape[0], f\"The input layer dimension does not match the input data, it should be {self.x.shape[0]}\"\n",
    "        assert self.layers_dim[-1] == self.y.shape[0], f\"The output layer dimension does not match the target data, it should be {self.y.shape[0]}\"\n",
    "        \n",
    "        for i in range(len(self.layers_dim) - 1):\n",
    "            self.network.append(Linear(self.layers_dim[i], self.layers_dim[i+1]))\n",
    "            self.network.append(self.activation_layer())\n",
    "\n",
    "    def forward(self):\n",
    "        output = self.x   # input to the first layer\n",
    "        for layer in self.network:\n",
    "            output = layer(output)      # the output of the previous layer is the input to the next layer\n",
    "            \n",
    "        return output   # the output of the last layer\n",
    "    \n",
    "    def backward(self, y_predict):\n",
    "        out_grad = self.loss.loss_gradient(self, y_predict, self.y)    # gradient of the loss w.r.t the output of the last layer\n",
    "        for layer in reversed(self.network):\n",
    "            out_grad = layer.backward(out_grad, self.learning_rate)    # gradient of the loss w.r.t the input of the layer\n",
    "        return out_grad\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.max_iterations):\n",
    "            y_predict = self.forward()    # the output of the last layer\n",
    "            loss = self.loss.loss(self, y_predict, self.y)\n",
    "            accuracy = calculate_accuracy(y_predict, self.y)\n",
    "            self.backward(y_predict)\n",
    "            print(f\"Epoch: {epoch + 1}, Loss: {loss:.2f}, accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        output = x\n",
    "        for layer in self.network:\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, true_labels):\n",
    "    '''\n",
    "    predictions: the predicted labels\n",
    "    true_labels: the actual labels\n",
    "    return: the accuracy of the model\n",
    "    '''\n",
    "    pred_labels = np.argmax(predictions, axis=0)\n",
    "    true_labels = np.argmax(true_labels, axis=0)\n",
    "    accuracy = np.mean(pred_labels == true_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_uniform(fan_in, fan_out, **kwargs):\n",
    "    bound = math.sqrt(2) * math.sqrt(3/fan_in)\n",
    "    u = random.uniform(-bound, bound,fan_in*fan_out) \n",
    "    return np.reshape(u,(fan_in,fan_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = fashion_mnist.load_data()       # load the fashion mnist dataset\n",
    "\n",
    "X = fashion_mnist[0][0].reshape(60000, 784) / 255\n",
    "y = fashion_mnist[0][1]\n",
    "y_one_hot = np.eye(10)[y]                       # one hot encode the target labels\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "X_train_normalized = X_train_normalized\n",
    "X_test_normalized = X_test_normalized.T\n",
    "y_train = y_train\n",
    "y_test = y_test.T\n",
    "\n",
    "layers_dim = [784,128,64,10]\n",
    "alpha = 0.1   # Learning rate\n",
    "\n",
    "nn = NN(X_train_normalized, y_train, layers_dim, alpha, 1000, ReLU, MSE)\n",
    "nn.train()\n",
    "y_pred = nn.predict(X_test_normalized)\n",
    "\n",
    "accuracy = calculate_accuracy(y_pred, y_test)\n",
    "print(\"==========================================\")\n",
    "print(f\"Test data Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
