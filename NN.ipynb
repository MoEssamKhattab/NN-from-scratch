{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError()\n",
    "    def backward(self, out_grad, lr):\n",
    "        raise NotImplementedError()\n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer\n",
    "### Forward\n",
    "\n",
    "$$Y = WX + B$$\n",
    "\n",
    "### Backward\n",
    "\n",
    "- The new out_grad:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial X}= W^T \\frac{\\partial E}{\\partial Y}$$\n",
    "\n",
    "- The new weight_grad:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W} = \\frac{\\partial E}{\\partial Y}X^T\n",
    "$$\n",
    "- The new bias_grad:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W} = mean\\Bigg(\\frac{\\partial E}{\\partial Y}, axis=1\\Bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_size, out_size) -> None:\n",
    "        # initializes the trainable parameters of this layer\n",
    "        self.weights = random.randn(out_size, in_size) * np.sqrt(1/in_size)\n",
    "        self.bias = random.randn(out_size, 1) * np.sqrt(1/in_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x      # to be used in BKW propagation\n",
    "        return self.weights @ x + self.bias\n",
    "    \n",
    "    def backward(self, out_grad, lr):\n",
    "        new_out_grad = self.weights.T @ out_grad\n",
    "        grad_w = out_grad @ self.x.T\n",
    "        grad_b = np.mean(out_grad, axis=1).reshape(-1,1)\n",
    "\n",
    "        self.weights -= lr * grad_w\n",
    "        self.bias -= lr * grad_b\n",
    "\n",
    "        return new_out_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Layer\n",
    "### Forward\n",
    "$$Y = f(X)$$\n",
    "\n",
    "#### RelU:\n",
    "<img src=\"relu.png\" width=\"300\">\n",
    "\n",
    "$$\\operatorname{RelU}(x)= \\begin{cases}0, & x \\leq 0 \\\\ x, & x>0\\end{cases}$$\n",
    "\n",
    "\n",
    "### Backward\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial X} = \\frac{\\partial E}{\\partial Y} \\odot f'(X)\n",
    "$$\n",
    "\n",
    "$$\\operatorname{RelU'}(x)= \\begin{cases}0, & x < 0 \\\\ 1, & x>0\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelU(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(x,0)\n",
    "\n",
    "    def backward(self, out_grad, lr):\n",
    "        Relu_prime = (self.x > 0).astype(int)\n",
    "        #Relu_prime = np.where(self.x > 0, 1, 0)\n",
    "\n",
    "        return np.multiply(Relu_prime, out_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "### MSE\n",
    "\n",
    "$$E = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "### MSE Gradient\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial Y} = \\frac{2}{nm}(\\hat{Y} - Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    def __init__(self) -> None:\n",
    "        None\n",
    "    def loss(self, y, y_true):\n",
    "        return np.mean((y-y_true)**2)\n",
    "    \n",
    "    def loss_grad(self, y, y_true):\n",
    "        no_classes = y.shape[0]\n",
    "        no_datapoints = y.shape[1]\n",
    "        return (2/(no_classes*no_datapoints)) * (y-y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, x, y, layers_dim, lr, max_iterations, activation=RelU, loss=MSE):\n",
    "        self.x = x.T\n",
    "        self.y = y.T\n",
    "        self.loss = loss()\n",
    "        self.lr = lr\n",
    "        self.max_iterations = max_iterations\n",
    "        self.network = []\n",
    "        self.initialize_network(layers_dim, activation)\n",
    "    \n",
    "    def initialize_network(self, layers_dim, activation) -> None:\n",
    "        assert layers_dim[0] == self.x.shape[0], f\"The input layer must be of size {self.x.shape[0]}\"\n",
    "        assert layers_dim[-1] == self.y.shape[0], f\"The output layer must be of size {self.y.shape[0]}\"\n",
    "\n",
    "        for i in range(len(layers_dim) - 1):\n",
    "            self.network.extend(\n",
    "                [ Linear(layers_dim[i], layers_dim[i+1]), activation() ]\n",
    "                 )\n",
    "            \n",
    "    def forward(self):\n",
    "        output = self.x\n",
    "        for layer in self.network:\n",
    "            output = layer(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backward(self, y_pred):\n",
    "        out_grad = self.loss.loss_grad(y_pred, self.y)\n",
    "        for layer in reversed(self.network):\n",
    "            out_grad = layer.backward(out_grad, self.lr)\n",
    "        \n",
    "        return out_grad\n",
    "    \n",
    "    def train(self) -> None:\n",
    "        for epoch in range(self.max_iterations):\n",
    "            y_pred = self.forward()\n",
    "            loss = self.loss.loss(y_pred, self.y)\n",
    "            acc = calculate_accuracy(y_pred, self.y)\n",
    "            self.backward(y_pred)\n",
    "            print(f\"Epoch {epoch}/{self.max_iterations}: Loss = {loss}, acc = {acc*100:.2f}%\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        output = x\n",
    "        for layer in self.network:\n",
    "            output = layer(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y_true):\n",
    "    pred_labels = np.argmax(y_pred, axis=0)\n",
    "    true_labels = np.argmax(y_true, axis=0)\n",
    "    return np.mean(pred_labels == true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "mnist_dataset = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10 random examples of mnist dataset\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 10, figsize=(20, 20))\n",
    "for i in range(10):\n",
    "    ax[i].imshow(mnist_dataset[0][0][i], cmap='gray')\n",
    "    ax[i].set_title(mnist_dataset[0][1][i])\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/400: Loss = 0.21761785418608975, acc = 11.52%\n",
      "Epoch 1/400: Loss = 0.10787422422200563, acc = 25.32%\n",
      "Epoch 2/400: Loss = 0.09418589621602023, acc = 32.01%\n",
      "Epoch 3/400: Loss = 0.08614324619592326, acc = 37.48%\n",
      "Epoch 4/400: Loss = 0.0800130721025274, acc = 42.26%\n",
      "Epoch 5/400: Loss = 0.07453980804413766, acc = 46.84%\n",
      "Epoch 6/400: Loss = 0.06994561745921554, acc = 50.70%\n",
      "Epoch 7/400: Loss = 0.06639029327963292, acc = 53.74%\n",
      "Epoch 8/400: Loss = 0.0635332662208792, acc = 56.21%\n",
      "Epoch 9/400: Loss = 0.06107367939781848, acc = 58.30%\n",
      "Epoch 10/400: Loss = 0.058900742458238904, acc = 60.08%\n",
      "Epoch 11/400: Loss = 0.05697097324988249, acc = 61.73%\n",
      "Epoch 12/400: Loss = 0.05523408869596329, acc = 63.16%\n",
      "Epoch 13/400: Loss = 0.05366410594161629, acc = 64.46%\n",
      "Epoch 14/400: Loss = 0.05221381783328939, acc = 65.58%\n",
      "Epoch 15/400: Loss = 0.050851263919924145, acc = 66.71%\n",
      "Epoch 16/400: Loss = 0.04956817874128961, acc = 67.89%\n",
      "Epoch 17/400: Loss = 0.04836708965846783, acc = 69.03%\n",
      "Epoch 18/400: Loss = 0.04722554150760823, acc = 70.20%\n",
      "Epoch 19/400: Loss = 0.04614083135232495, acc = 71.17%\n",
      "Epoch 20/400: Loss = 0.04511586343832181, acc = 72.07%\n",
      "Epoch 21/400: Loss = 0.044158163904060514, acc = 72.98%\n",
      "Epoch 22/400: Loss = 0.04325486536516691, acc = 73.77%\n",
      "Epoch 23/400: Loss = 0.042395215036318164, acc = 74.50%\n",
      "Epoch 24/400: Loss = 0.04158201357650814, acc = 75.15%\n",
      "Epoch 25/400: Loss = 0.0408080599772305, acc = 75.81%\n",
      "Epoch 26/400: Loss = 0.04007207621608325, acc = 76.37%\n",
      "Epoch 27/400: Loss = 0.03936986605493106, acc = 76.99%\n",
      "Epoch 28/400: Loss = 0.03869526019029892, acc = 77.54%\n",
      "Epoch 29/400: Loss = 0.03805238307823127, acc = 78.04%\n",
      "Epoch 30/400: Loss = 0.03744033978224333, acc = 78.47%\n",
      "Epoch 31/400: Loss = 0.036857250817592856, acc = 78.95%\n",
      "Epoch 32/400: Loss = 0.03629681781497323, acc = 79.33%\n",
      "Epoch 33/400: Loss = 0.03576345570272835, acc = 79.71%\n",
      "Epoch 34/400: Loss = 0.035252256377638576, acc = 80.11%\n",
      "Epoch 35/400: Loss = 0.034762196191077864, acc = 80.49%\n",
      "Epoch 36/400: Loss = 0.034296314336315574, acc = 80.85%\n",
      "Epoch 37/400: Loss = 0.033850838970044206, acc = 81.16%\n",
      "Epoch 38/400: Loss = 0.03342237093634242, acc = 81.50%\n",
      "Epoch 39/400: Loss = 0.033008485260654426, acc = 81.74%\n",
      "Epoch 40/400: Loss = 0.032617718075482874, acc = 82.01%\n",
      "Epoch 41/400: Loss = 0.03224648689930075, acc = 82.25%\n",
      "Epoch 42/400: Loss = 0.031892594323829446, acc = 82.51%\n",
      "Epoch 43/400: Loss = 0.031551601495216366, acc = 82.75%\n",
      "Epoch 44/400: Loss = 0.031224482278682172, acc = 82.91%\n",
      "Epoch 45/400: Loss = 0.030911359227475035, acc = 83.12%\n",
      "Epoch 46/400: Loss = 0.0306120555764851, acc = 83.35%\n",
      "Epoch 47/400: Loss = 0.030324486302765425, acc = 83.59%\n",
      "Epoch 48/400: Loss = 0.030046778653511748, acc = 83.76%\n",
      "Epoch 49/400: Loss = 0.0297793803411435, acc = 83.95%\n",
      "Epoch 50/400: Loss = 0.029523462975021297, acc = 84.17%\n",
      "Epoch 51/400: Loss = 0.029277576767942484, acc = 84.30%\n",
      "Epoch 52/400: Loss = 0.02904066537916653, acc = 84.49%\n",
      "Epoch 53/400: Loss = 0.028812152153015726, acc = 84.64%\n",
      "Epoch 54/400: Loss = 0.028591361513435755, acc = 84.80%\n",
      "Epoch 55/400: Loss = 0.028377918145731186, acc = 84.97%\n",
      "Epoch 56/400: Loss = 0.028170561699058943, acc = 85.11%\n",
      "Epoch 57/400: Loss = 0.027968932026805622, acc = 85.26%\n",
      "Epoch 58/400: Loss = 0.02777279518547929, acc = 85.38%\n",
      "Epoch 59/400: Loss = 0.027582695572727748, acc = 85.49%\n",
      "Epoch 60/400: Loss = 0.02739849223260209, acc = 85.60%\n",
      "Epoch 61/400: Loss = 0.02721886252130628, acc = 85.71%\n",
      "Epoch 62/400: Loss = 0.0270435113800733, acc = 85.81%\n",
      "Epoch 63/400: Loss = 0.026872790682903915, acc = 85.92%\n",
      "Epoch 64/400: Loss = 0.026706144269372904, acc = 86.02%\n",
      "Epoch 65/400: Loss = 0.026544486772962023, acc = 86.13%\n",
      "Epoch 66/400: Loss = 0.02638644303007047, acc = 86.23%\n",
      "Epoch 67/400: Loss = 0.02623211717729799, acc = 86.35%\n",
      "Epoch 68/400: Loss = 0.026081179276772348, acc = 86.43%\n",
      "Epoch 69/400: Loss = 0.025933741566927293, acc = 86.49%\n",
      "Epoch 70/400: Loss = 0.025789540752595815, acc = 86.59%\n",
      "Epoch 71/400: Loss = 0.025648810974597053, acc = 86.69%\n",
      "Epoch 72/400: Loss = 0.025511029479346203, acc = 86.77%\n",
      "Epoch 73/400: Loss = 0.02537612765337228, acc = 86.86%\n",
      "Epoch 74/400: Loss = 0.02524332449069437, acc = 86.94%\n",
      "Epoch 75/400: Loss = 0.025112342668606836, acc = 87.02%\n",
      "Epoch 76/400: Loss = 0.024983483951189867, acc = 87.12%\n",
      "Epoch 77/400: Loss = 0.024857710976163953, acc = 87.21%\n",
      "Epoch 78/400: Loss = 0.024734500529820125, acc = 87.29%\n",
      "Epoch 79/400: Loss = 0.024612870100830075, acc = 87.38%\n",
      "Epoch 80/400: Loss = 0.024493616517031736, acc = 87.44%\n",
      "Epoch 81/400: Loss = 0.024376493052403653, acc = 87.50%\n",
      "Epoch 82/400: Loss = 0.02426112674159727, acc = 87.61%\n",
      "Epoch 83/400: Loss = 0.02414753193991366, acc = 87.67%\n",
      "Epoch 84/400: Loss = 0.024035687865382287, acc = 87.74%\n",
      "Epoch 85/400: Loss = 0.023926407177468804, acc = 87.81%\n",
      "Epoch 86/400: Loss = 0.023819114062695976, acc = 87.87%\n",
      "Epoch 87/400: Loss = 0.02371382826930072, acc = 87.97%\n",
      "Epoch 88/400: Loss = 0.023609891217597972, acc = 88.06%\n",
      "Epoch 89/400: Loss = 0.023507942934597167, acc = 88.12%\n",
      "Epoch 90/400: Loss = 0.023407911124115593, acc = 88.18%\n",
      "Epoch 91/400: Loss = 0.023309307957084222, acc = 88.24%\n",
      "Epoch 92/400: Loss = 0.02321254660462222, acc = 88.31%\n",
      "Epoch 93/400: Loss = 0.02311640978517837, acc = 88.38%\n",
      "Epoch 94/400: Loss = 0.023021562595309613, acc = 88.43%\n",
      "Epoch 95/400: Loss = 0.02292867523536179, acc = 88.49%\n",
      "Epoch 96/400: Loss = 0.022837471329820613, acc = 88.53%\n",
      "Epoch 97/400: Loss = 0.022747436448771984, acc = 88.57%\n",
      "Epoch 98/400: Loss = 0.022659064471647956, acc = 88.64%\n",
      "Epoch 99/400: Loss = 0.022571318918086877, acc = 88.70%\n",
      "Epoch 100/400: Loss = 0.022485513222456673, acc = 88.74%\n",
      "Epoch 101/400: Loss = 0.02240139050971158, acc = 88.78%\n",
      "Epoch 102/400: Loss = 0.022318584813740643, acc = 88.84%\n",
      "Epoch 103/400: Loss = 0.0222365426543968, acc = 88.90%\n",
      "Epoch 104/400: Loss = 0.02215615330310369, acc = 88.96%\n",
      "Epoch 105/400: Loss = 0.022077251135406876, acc = 89.02%\n",
      "Epoch 106/400: Loss = 0.02199925899778707, acc = 89.08%\n",
      "Epoch 107/400: Loss = 0.021922265385529396, acc = 89.12%\n",
      "Epoch 108/400: Loss = 0.021846643215916613, acc = 89.17%\n",
      "Epoch 109/400: Loss = 0.02177200524396171, acc = 89.22%\n",
      "Epoch 110/400: Loss = 0.021697838373348464, acc = 89.27%\n",
      "Epoch 111/400: Loss = 0.021624804875783044, acc = 89.31%\n",
      "Epoch 112/400: Loss = 0.021552755027324993, acc = 89.35%\n",
      "Epoch 113/400: Loss = 0.02148132909547608, acc = 89.40%\n",
      "Epoch 114/400: Loss = 0.021410827087177533, acc = 89.43%\n",
      "Epoch 115/400: Loss = 0.02134122151267558, acc = 89.47%\n",
      "Epoch 116/400: Loss = 0.021272557590163194, acc = 89.50%\n",
      "Epoch 117/400: Loss = 0.0212049721963623, acc = 89.54%\n",
      "Epoch 118/400: Loss = 0.021138435376390867, acc = 89.59%\n",
      "Epoch 119/400: Loss = 0.0210727641077749, acc = 89.63%\n",
      "Epoch 120/400: Loss = 0.021007923402367054, acc = 89.67%\n",
      "Epoch 121/400: Loss = 0.02094371465883687, acc = 89.70%\n",
      "Epoch 122/400: Loss = 0.020880402210387738, acc = 89.74%\n",
      "Epoch 123/400: Loss = 0.020817876003714885, acc = 89.79%\n",
      "Epoch 124/400: Loss = 0.02075582679675955, acc = 89.83%\n",
      "Epoch 125/400: Loss = 0.020694386280012557, acc = 89.86%\n",
      "Epoch 126/400: Loss = 0.020633722399281424, acc = 89.90%\n",
      "Epoch 127/400: Loss = 0.020573683628947268, acc = 89.93%\n",
      "Epoch 128/400: Loss = 0.02051433293368046, acc = 89.96%\n",
      "Epoch 129/400: Loss = 0.02045547538966768, acc = 89.99%\n",
      "Epoch 130/400: Loss = 0.02039742485938656, acc = 90.02%\n",
      "Epoch 131/400: Loss = 0.020340146927744548, acc = 90.05%\n",
      "Epoch 132/400: Loss = 0.020283484013694356, acc = 90.08%\n",
      "Epoch 133/400: Loss = 0.02022739142819885, acc = 90.12%\n",
      "Epoch 134/400: Loss = 0.02017187656593237, acc = 90.14%\n",
      "Epoch 135/400: Loss = 0.020117037122155693, acc = 90.18%\n",
      "Epoch 136/400: Loss = 0.020062755391362402, acc = 90.21%\n",
      "Epoch 137/400: Loss = 0.020009045764486148, acc = 90.24%\n",
      "Epoch 138/400: Loss = 0.01995588647295085, acc = 90.27%\n",
      "Epoch 139/400: Loss = 0.0199034246136508, acc = 90.29%\n",
      "Epoch 140/400: Loss = 0.019851534750612986, acc = 90.32%\n",
      "Epoch 141/400: Loss = 0.019800151858038094, acc = 90.34%\n",
      "Epoch 142/400: Loss = 0.01974936910443882, acc = 90.36%\n",
      "Epoch 143/400: Loss = 0.019698905735971217, acc = 90.39%\n",
      "Epoch 144/400: Loss = 0.019648870439602232, acc = 90.42%\n",
      "Epoch 145/400: Loss = 0.01959925759652789, acc = 90.46%\n",
      "Epoch 146/400: Loss = 0.01955005232124756, acc = 90.50%\n",
      "Epoch 147/400: Loss = 0.01950119933025066, acc = 90.53%\n",
      "Epoch 148/400: Loss = 0.019452606875534916, acc = 90.56%\n",
      "Epoch 149/400: Loss = 0.019404456773992117, acc = 90.58%\n",
      "Epoch 150/400: Loss = 0.01935669405972023, acc = 90.61%\n",
      "Epoch 151/400: Loss = 0.019309163505052598, acc = 90.64%\n",
      "Epoch 152/400: Loss = 0.019262124327649646, acc = 90.66%\n",
      "Epoch 153/400: Loss = 0.019215480129328395, acc = 90.69%\n",
      "Epoch 154/400: Loss = 0.01916924511159302, acc = 90.72%\n",
      "Epoch 155/400: Loss = 0.019123484492873814, acc = 90.74%\n",
      "Epoch 156/400: Loss = 0.01907814301915093, acc = 90.77%\n",
      "Epoch 157/400: Loss = 0.01903297315359875, acc = 90.80%\n",
      "Epoch 158/400: Loss = 0.018988397447243913, acc = 90.83%\n",
      "Epoch 159/400: Loss = 0.01894425670932739, acc = 90.85%\n",
      "Epoch 160/400: Loss = 0.01890058892298121, acc = 90.88%\n",
      "Epoch 161/400: Loss = 0.01885728578031266, acc = 90.90%\n",
      "Epoch 162/400: Loss = 0.018814321436853836, acc = 90.93%\n",
      "Epoch 163/400: Loss = 0.01877165022951235, acc = 90.96%\n",
      "Epoch 164/400: Loss = 0.018729411579050625, acc = 90.98%\n",
      "Epoch 165/400: Loss = 0.01868744356081369, acc = 90.99%\n",
      "Epoch 166/400: Loss = 0.018645877001297346, acc = 91.00%\n",
      "Epoch 167/400: Loss = 0.01860475611485492, acc = 91.02%\n",
      "Epoch 168/400: Loss = 0.01856383821620979, acc = 91.04%\n",
      "Epoch 169/400: Loss = 0.018523336644603577, acc = 91.05%\n",
      "Epoch 170/400: Loss = 0.018483216678061392, acc = 91.07%\n",
      "Epoch 171/400: Loss = 0.018443523488294627, acc = 91.11%\n",
      "Epoch 172/400: Loss = 0.018404152897039422, acc = 91.14%\n",
      "Epoch 173/400: Loss = 0.01836508004296402, acc = 91.16%\n",
      "Epoch 174/400: Loss = 0.018326336449814115, acc = 91.19%\n",
      "Epoch 175/400: Loss = 0.018287901548010292, acc = 91.21%\n",
      "Epoch 176/400: Loss = 0.01824979455273219, acc = 91.24%\n",
      "Epoch 177/400: Loss = 0.018211966616266992, acc = 91.26%\n",
      "Epoch 178/400: Loss = 0.018174507774411916, acc = 91.28%\n",
      "Epoch 179/400: Loss = 0.018137340267993397, acc = 91.30%\n",
      "Epoch 180/400: Loss = 0.01810049689930236, acc = 91.32%\n",
      "Epoch 181/400: Loss = 0.01806390549619664, acc = 91.36%\n",
      "Epoch 182/400: Loss = 0.018027499205584465, acc = 91.38%\n",
      "Epoch 183/400: Loss = 0.017991246788500722, acc = 91.39%\n",
      "Epoch 184/400: Loss = 0.01795529023228073, acc = 91.43%\n",
      "Epoch 185/400: Loss = 0.017919726998744702, acc = 91.45%\n",
      "Epoch 186/400: Loss = 0.017884322437690364, acc = 91.47%\n",
      "Epoch 187/400: Loss = 0.017849158866819224, acc = 91.49%\n",
      "Epoch 188/400: Loss = 0.017814190773549936, acc = 91.52%\n",
      "Epoch 189/400: Loss = 0.01777946123672444, acc = 91.54%\n",
      "Epoch 190/400: Loss = 0.017745080776532897, acc = 91.56%\n",
      "Epoch 191/400: Loss = 0.01771105738092839, acc = 91.57%\n",
      "Epoch 192/400: Loss = 0.017677271309577133, acc = 91.60%\n",
      "Epoch 193/400: Loss = 0.017643664389876596, acc = 91.61%\n",
      "Epoch 194/400: Loss = 0.01761018444816936, acc = 91.63%\n",
      "Epoch 195/400: Loss = 0.01757685984563575, acc = 91.65%\n",
      "Epoch 196/400: Loss = 0.01754384081371337, acc = 91.66%\n",
      "Epoch 197/400: Loss = 0.017511123491488055, acc = 91.68%\n",
      "Epoch 198/400: Loss = 0.01747859866810289, acc = 91.70%\n",
      "Epoch 199/400: Loss = 0.01744630776147402, acc = 91.72%\n",
      "Epoch 200/400: Loss = 0.017414274921676985, acc = 91.74%\n",
      "Epoch 201/400: Loss = 0.01738245533822727, acc = 91.76%\n",
      "Epoch 202/400: Loss = 0.017350765226011457, acc = 91.79%\n",
      "Epoch 203/400: Loss = 0.017319155282257817, acc = 91.81%\n",
      "Epoch 204/400: Loss = 0.017287782121760516, acc = 91.83%\n",
      "Epoch 205/400: Loss = 0.017256681105722396, acc = 91.85%\n",
      "Epoch 206/400: Loss = 0.01722579938761336, acc = 91.87%\n",
      "Epoch 207/400: Loss = 0.01719517878245736, acc = 91.90%\n",
      "Epoch 208/400: Loss = 0.017164877212944598, acc = 91.90%\n",
      "Epoch 209/400: Loss = 0.017134794423741398, acc = 91.92%\n",
      "Epoch 210/400: Loss = 0.0171048806341952, acc = 91.94%\n",
      "Epoch 211/400: Loss = 0.017075188035909163, acc = 91.96%\n",
      "Epoch 212/400: Loss = 0.01704553686196664, acc = 91.98%\n",
      "Epoch 213/400: Loss = 0.01701604289415702, acc = 92.00%\n",
      "Epoch 214/400: Loss = 0.016986643629314745, acc = 92.02%\n",
      "Epoch 215/400: Loss = 0.016957362175518548, acc = 92.04%\n",
      "Epoch 216/400: Loss = 0.016928220268117643, acc = 92.07%\n",
      "Epoch 217/400: Loss = 0.01689938818272444, acc = 92.09%\n",
      "Epoch 218/400: Loss = 0.016870858787093736, acc = 92.10%\n",
      "Epoch 219/400: Loss = 0.016842550563968533, acc = 92.11%\n",
      "Epoch 220/400: Loss = 0.01681453377747999, acc = 92.12%\n",
      "Epoch 221/400: Loss = 0.016786707415562055, acc = 92.14%\n",
      "Epoch 222/400: Loss = 0.01675897582426861, acc = 92.16%\n",
      "Epoch 223/400: Loss = 0.01673144232386349, acc = 92.17%\n",
      "Epoch 224/400: Loss = 0.016704055525123124, acc = 92.18%\n",
      "Epoch 225/400: Loss = 0.016676879215069534, acc = 92.19%\n",
      "Epoch 226/400: Loss = 0.016649867960459912, acc = 92.20%\n",
      "Epoch 227/400: Loss = 0.016623070290603694, acc = 92.21%\n",
      "Epoch 228/400: Loss = 0.016596339525929035, acc = 92.22%\n",
      "Epoch 229/400: Loss = 0.01656976438130453, acc = 92.24%\n",
      "Epoch 230/400: Loss = 0.01654339705731541, acc = 92.25%\n",
      "Epoch 231/400: Loss = 0.016517295316408852, acc = 92.27%\n",
      "Epoch 232/400: Loss = 0.016491403287944604, acc = 92.28%\n",
      "Epoch 233/400: Loss = 0.0164656999447886, acc = 92.30%\n",
      "Epoch 234/400: Loss = 0.016440211703103554, acc = 92.31%\n",
      "Epoch 235/400: Loss = 0.01641476381864402, acc = 92.33%\n",
      "Epoch 236/400: Loss = 0.01638946117732484, acc = 92.34%\n",
      "Epoch 237/400: Loss = 0.016364297963192678, acc = 92.35%\n",
      "Epoch 238/400: Loss = 0.01633925951095415, acc = 92.36%\n",
      "Epoch 239/400: Loss = 0.016314384356660463, acc = 92.36%\n",
      "Epoch 240/400: Loss = 0.016289697663754606, acc = 92.37%\n",
      "Epoch 241/400: Loss = 0.01626514240072476, acc = 92.39%\n",
      "Epoch 242/400: Loss = 0.016240814025021862, acc = 92.40%\n",
      "Epoch 243/400: Loss = 0.016216636551865556, acc = 92.41%\n",
      "Epoch 244/400: Loss = 0.01619261455137957, acc = 92.43%\n",
      "Epoch 245/400: Loss = 0.016168683483858165, acc = 92.44%\n",
      "Epoch 246/400: Loss = 0.01614496046668184, acc = 92.46%\n",
      "Epoch 247/400: Loss = 0.016121432717775292, acc = 92.48%\n",
      "Epoch 248/400: Loss = 0.016098091585413372, acc = 92.50%\n",
      "Epoch 249/400: Loss = 0.01607496602671892, acc = 92.50%\n",
      "Epoch 250/400: Loss = 0.016051979978615932, acc = 92.51%\n",
      "Epoch 251/400: Loss = 0.016029060201347586, acc = 92.53%\n",
      "Epoch 252/400: Loss = 0.016006294017022055, acc = 92.55%\n",
      "Epoch 253/400: Loss = 0.015983669278411366, acc = 92.56%\n",
      "Epoch 254/400: Loss = 0.015961170247223697, acc = 92.57%\n",
      "Epoch 255/400: Loss = 0.01593873120165076, acc = 92.59%\n",
      "Epoch 256/400: Loss = 0.015916429955498468, acc = 92.60%\n",
      "Epoch 257/400: Loss = 0.015894227111565417, acc = 92.61%\n",
      "Epoch 258/400: Loss = 0.015872166050548597, acc = 92.62%\n",
      "Epoch 259/400: Loss = 0.015850279632129285, acc = 92.64%\n",
      "Epoch 260/400: Loss = 0.015828529869239976, acc = 92.65%\n",
      "Epoch 261/400: Loss = 0.015806873714922554, acc = 92.66%\n",
      "Epoch 262/400: Loss = 0.01578529642888115, acc = 92.68%\n",
      "Epoch 263/400: Loss = 0.015763835380898247, acc = 92.69%\n",
      "Epoch 264/400: Loss = 0.015742460169681453, acc = 92.70%\n",
      "Epoch 265/400: Loss = 0.015721208168124726, acc = 92.70%\n",
      "Epoch 266/400: Loss = 0.015700079228204974, acc = 92.70%\n",
      "Epoch 267/400: Loss = 0.015679049416009675, acc = 92.72%\n",
      "Epoch 268/400: Loss = 0.01565813745370287, acc = 92.73%\n",
      "Epoch 269/400: Loss = 0.01563736596985093, acc = 92.74%\n",
      "Epoch 270/400: Loss = 0.015616675891712634, acc = 92.75%\n",
      "Epoch 271/400: Loss = 0.015596076955070965, acc = 92.76%\n",
      "Epoch 272/400: Loss = 0.015575605667526901, acc = 92.76%\n",
      "Epoch 273/400: Loss = 0.015555205654037955, acc = 92.78%\n",
      "Epoch 274/400: Loss = 0.01553491783022777, acc = 92.78%\n",
      "Epoch 275/400: Loss = 0.015514743898758912, acc = 92.79%\n",
      "Epoch 276/400: Loss = 0.015494704561450029, acc = 92.80%\n",
      "Epoch 277/400: Loss = 0.015474789941316666, acc = 92.81%\n",
      "Epoch 278/400: Loss = 0.015455006976750008, acc = 92.83%\n",
      "Epoch 279/400: Loss = 0.015435332487606302, acc = 92.83%\n",
      "Epoch 280/400: Loss = 0.015415779367919394, acc = 92.84%\n",
      "Epoch 281/400: Loss = 0.015396332858910585, acc = 92.86%\n",
      "Epoch 282/400: Loss = 0.015377008661042059, acc = 92.87%\n",
      "Epoch 283/400: Loss = 0.015357811832512997, acc = 92.88%\n",
      "Epoch 284/400: Loss = 0.0153386556401374, acc = 92.88%\n",
      "Epoch 285/400: Loss = 0.015319601197513884, acc = 92.89%\n",
      "Epoch 286/400: Loss = 0.015300651241070792, acc = 92.91%\n",
      "Epoch 287/400: Loss = 0.01528174079999073, acc = 92.93%\n",
      "Epoch 288/400: Loss = 0.015262975329956336, acc = 92.93%\n",
      "Epoch 289/400: Loss = 0.015244340010649665, acc = 92.95%\n",
      "Epoch 290/400: Loss = 0.015225772141610196, acc = 92.95%\n",
      "Epoch 291/400: Loss = 0.01520727721305698, acc = 92.96%\n",
      "Epoch 292/400: Loss = 0.015188883360304679, acc = 92.97%\n",
      "Epoch 293/400: Loss = 0.015170607239271286, acc = 92.98%\n",
      "Epoch 294/400: Loss = 0.015152412398830183, acc = 92.99%\n",
      "Epoch 295/400: Loss = 0.015134275644213685, acc = 93.00%\n",
      "Epoch 296/400: Loss = 0.015116267077995677, acc = 93.01%\n",
      "Epoch 297/400: Loss = 0.015098337723066058, acc = 93.01%\n",
      "Epoch 298/400: Loss = 0.015080485531047337, acc = 93.03%\n",
      "Epoch 299/400: Loss = 0.015062713287795189, acc = 93.03%\n",
      "Epoch 300/400: Loss = 0.015045056495003704, acc = 93.04%\n",
      "Epoch 301/400: Loss = 0.01502749964683686, acc = 93.04%\n",
      "Epoch 302/400: Loss = 0.015010011034250662, acc = 93.06%\n",
      "Epoch 303/400: Loss = 0.014992559577080037, acc = 93.06%\n",
      "Epoch 304/400: Loss = 0.014975161719814693, acc = 93.08%\n",
      "Epoch 305/400: Loss = 0.01495786356287398, acc = 93.08%\n",
      "Epoch 306/400: Loss = 0.01494068177948224, acc = 93.09%\n",
      "Epoch 307/400: Loss = 0.014923604249428736, acc = 93.10%\n",
      "Epoch 308/400: Loss = 0.014906619639913311, acc = 93.10%\n",
      "Epoch 309/400: Loss = 0.014889747349290884, acc = 93.11%\n",
      "Epoch 310/400: Loss = 0.014872963081830086, acc = 93.12%\n",
      "Epoch 311/400: Loss = 0.014856219561838576, acc = 93.13%\n",
      "Epoch 312/400: Loss = 0.014839521309398368, acc = 93.13%\n",
      "Epoch 313/400: Loss = 0.014822852312089503, acc = 93.14%\n",
      "Epoch 314/400: Loss = 0.014806275890006512, acc = 93.15%\n",
      "Epoch 315/400: Loss = 0.014789789048746555, acc = 93.15%\n",
      "Epoch 316/400: Loss = 0.014773385400018685, acc = 93.16%\n",
      "Epoch 317/400: Loss = 0.014756991658813648, acc = 93.16%\n",
      "Epoch 318/400: Loss = 0.014740681336484185, acc = 93.17%\n",
      "Epoch 319/400: Loss = 0.014724466719363116, acc = 93.18%\n",
      "Epoch 320/400: Loss = 0.014708345244240539, acc = 93.20%\n",
      "Epoch 321/400: Loss = 0.01469229467180618, acc = 93.22%\n",
      "Epoch 322/400: Loss = 0.01467630828283519, acc = 93.23%\n",
      "Epoch 323/400: Loss = 0.01466033682635266, acc = 93.23%\n",
      "Epoch 324/400: Loss = 0.014644431657185463, acc = 93.24%\n",
      "Epoch 325/400: Loss = 0.014628611690578235, acc = 93.25%\n",
      "Epoch 326/400: Loss = 0.014612864503916459, acc = 93.27%\n",
      "Epoch 327/400: Loss = 0.014597175466993415, acc = 93.29%\n",
      "Epoch 328/400: Loss = 0.014581547468296017, acc = 93.30%\n",
      "Epoch 329/400: Loss = 0.01456600895840121, acc = 93.30%\n",
      "Epoch 330/400: Loss = 0.014550544272433838, acc = 93.32%\n",
      "Epoch 331/400: Loss = 0.014535086663385106, acc = 93.33%\n",
      "Epoch 332/400: Loss = 0.014519207476714159, acc = 93.34%\n",
      "Epoch 333/400: Loss = 0.014503551614057432, acc = 93.35%\n",
      "Epoch 334/400: Loss = 0.014488057964993382, acc = 93.36%\n",
      "Epoch 335/400: Loss = 0.014472711990150065, acc = 93.36%\n",
      "Epoch 336/400: Loss = 0.014457501676780726, acc = 93.37%\n",
      "Epoch 337/400: Loss = 0.014442364634399308, acc = 93.37%\n",
      "Epoch 338/400: Loss = 0.014427286028162324, acc = 93.38%\n",
      "Epoch 339/400: Loss = 0.01441230515656082, acc = 93.39%\n",
      "Epoch 340/400: Loss = 0.014397419599781544, acc = 93.40%\n",
      "Epoch 341/400: Loss = 0.014382623418173457, acc = 93.40%\n",
      "Epoch 342/400: Loss = 0.014367899716943933, acc = 93.40%\n",
      "Epoch 343/400: Loss = 0.01435325532708185, acc = 93.41%\n",
      "Epoch 344/400: Loss = 0.014338689216815893, acc = 93.42%\n",
      "Epoch 345/400: Loss = 0.01432418107322698, acc = 93.42%\n",
      "Epoch 346/400: Loss = 0.01430971920603602, acc = 93.43%\n",
      "Epoch 347/400: Loss = 0.014295328572982179, acc = 93.43%\n",
      "Epoch 348/400: Loss = 0.014280986970446843, acc = 93.45%\n",
      "Epoch 349/400: Loss = 0.014266704170592656, acc = 93.45%\n",
      "Epoch 350/400: Loss = 0.014252482734420124, acc = 93.46%\n",
      "Epoch 351/400: Loss = 0.014238316331910129, acc = 93.47%\n",
      "Epoch 352/400: Loss = 0.014224218696773263, acc = 93.47%\n",
      "Epoch 353/400: Loss = 0.014210077553191815, acc = 93.47%\n",
      "Epoch 354/400: Loss = 0.014195961189932342, acc = 93.47%\n",
      "Epoch 355/400: Loss = 0.014181864734922766, acc = 93.49%\n",
      "Epoch 356/400: Loss = 0.01416784498359667, acc = 93.49%\n",
      "Epoch 357/400: Loss = 0.014153910748746366, acc = 93.50%\n",
      "Epoch 358/400: Loss = 0.014140039111618007, acc = 93.50%\n",
      "Epoch 359/400: Loss = 0.01412623412860819, acc = 93.51%\n",
      "Epoch 360/400: Loss = 0.01411249103373093, acc = 93.51%\n",
      "Epoch 361/400: Loss = 0.014098810306077332, acc = 93.53%\n",
      "Epoch 362/400: Loss = 0.014085178229840851, acc = 93.54%\n",
      "Epoch 363/400: Loss = 0.014071562342632855, acc = 93.54%\n",
      "Epoch 364/400: Loss = 0.014057935414866956, acc = 93.54%\n",
      "Epoch 365/400: Loss = 0.014044376744847127, acc = 93.55%\n",
      "Epoch 366/400: Loss = 0.014030892353040518, acc = 93.56%\n",
      "Epoch 367/400: Loss = 0.014017441260050603, acc = 93.58%\n",
      "Epoch 368/400: Loss = 0.01400395829728552, acc = 93.59%\n",
      "Epoch 369/400: Loss = 0.013990504065061502, acc = 93.59%\n",
      "Epoch 370/400: Loss = 0.013977133477524568, acc = 93.60%\n",
      "Epoch 371/400: Loss = 0.01396383558353766, acc = 93.61%\n",
      "Epoch 372/400: Loss = 0.013950581321890624, acc = 93.62%\n",
      "Epoch 373/400: Loss = 0.013937399071693357, acc = 93.61%\n",
      "Epoch 374/400: Loss = 0.01392427475374489, acc = 93.62%\n",
      "Epoch 375/400: Loss = 0.013911194184621807, acc = 93.63%\n",
      "Epoch 376/400: Loss = 0.013898176081783821, acc = 93.63%\n",
      "Epoch 377/400: Loss = 0.01388520259009831, acc = 93.64%\n",
      "Epoch 378/400: Loss = 0.013872314860216492, acc = 93.65%\n",
      "Epoch 379/400: Loss = 0.013859495116319719, acc = 93.65%\n",
      "Epoch 380/400: Loss = 0.013846723219701654, acc = 93.66%\n",
      "Epoch 381/400: Loss = 0.013833957215688085, acc = 93.68%\n",
      "Epoch 382/400: Loss = 0.013821257186328194, acc = 93.69%\n",
      "Epoch 383/400: Loss = 0.013808594433313377, acc = 93.69%\n",
      "Epoch 384/400: Loss = 0.013795971675409975, acc = 93.70%\n",
      "Epoch 385/400: Loss = 0.013783369045172966, acc = 93.70%\n",
      "Epoch 386/400: Loss = 0.013770808946541457, acc = 93.71%\n",
      "Epoch 387/400: Loss = 0.013758296977036918, acc = 93.71%\n",
      "Epoch 388/400: Loss = 0.013745831022894716, acc = 93.73%\n",
      "Epoch 389/400: Loss = 0.013733388690136333, acc = 93.75%\n",
      "Epoch 390/400: Loss = 0.013720991384712954, acc = 93.75%\n",
      "Epoch 391/400: Loss = 0.013708655388785388, acc = 93.76%\n",
      "Epoch 392/400: Loss = 0.01369637486395845, acc = 93.76%\n",
      "Epoch 393/400: Loss = 0.013684125340615924, acc = 93.77%\n",
      "Epoch 394/400: Loss = 0.01367190750675902, acc = 93.78%\n",
      "Epoch 395/400: Loss = 0.013659718229255375, acc = 93.79%\n",
      "Epoch 396/400: Loss = 0.013647558111495312, acc = 93.80%\n",
      "Epoch 397/400: Loss = 0.013635428187929292, acc = 93.81%\n",
      "Epoch 398/400: Loss = 0.013623361723498102, acc = 93.81%\n",
      "Epoch 399/400: Loss = 0.013611343778965872, acc = 93.81%\n",
      "==========================================\n",
      "Test data Accuracy: 93.01%\n"
     ]
    }
   ],
   "source": [
    "X = mnist_dataset[0][0].reshape(60000, 784) / 255       # flatten, normalize\n",
    "y = mnist_dataset[0][1]\n",
    "y_one_hot = np.eye(10)[y]                       # one hot encode the target labels\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "X_train_normalized = X_train_normalized\n",
    "X_test_normalized = X_test_normalized.T\n",
    "y_train = y_train\n",
    "y_test = y_test.T\n",
    "\n",
    "layers_dim = [784,64,10]\n",
    "lr = 0.6   # Learning rate\n",
    "activation_layer = RelU\n",
    "loss = MSE\n",
    "enable_bias = True\n",
    "\n",
    "nn = NN(X_train_normalized, y_train, layers_dim, lr, 400, activation_layer, loss)\n",
    "nn.train()\n",
    "y_pred = nn.predict(X_test_normalized)\n",
    "\n",
    "# Calculate the accuracy of the model with the test data\n",
    "accuracy = calculate_accuracy(y_pred, y_test)\n",
    "print(\"==========================================\")\n",
    "print(f\"Test data Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
