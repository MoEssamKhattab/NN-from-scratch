{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError()\n",
    "    def backward(self, out_grad, lr):\n",
    "        raise NotImplementedError()\n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer\n",
    "### Forward\n",
    "\n",
    "$$Y = WX + B$$\n",
    "\n",
    "### Backward\n",
    "\n",
    "- The new out_grad:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial X}= W^T \\frac{\\partial E}{\\partial Y}$$\n",
    "\n",
    "- The new weight_grad:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W} = \\frac{\\partial E}{\\partial Y}X^T\n",
    "$$\n",
    "- The new bias_grad:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W} = mean\\Bigg(\\frac{\\partial E}{\\partial Y}, axis=1\\Bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_size, out_size) -> None:\n",
    "        # initializes the trainable parameters of this layer\n",
    "        self.weights = random.randn(out_size, in_size) * np.sqrt(1/in_size)\n",
    "        self.bias = random.randn(out_size, 1) * np.sqrt(1/in_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x      # to be used in BKW propagation\n",
    "        return self.weights @ x + self.bias\n",
    "    \n",
    "    def backward(self, out_grad, lr):\n",
    "        new_out_grad = self.weights.T @ out_grad\n",
    "        grad_w = out_grad @ self.x.T\n",
    "        grad_b = np.mean(out_grad, axis=1).reshape(-1,1)\n",
    "\n",
    "        self.weights -= lr * grad_w\n",
    "        self.bias -= lr * grad_b\n",
    "\n",
    "        return new_out_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Layer\n",
    "### Forward\n",
    "$$Y = f(X)$$\n",
    "\n",
    "#### RelU:\n",
    "<img src=\"relu.png\" width=\"300\">\n",
    "\n",
    "$$\\operatorname{RelU}(x)= \\begin{cases}0, & x \\leq 0 \\\\ x, & x>0\\end{cases}$$\n",
    "\n",
    "\n",
    "### Backward\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial X} = \\frac{\\partial E}{\\partial Y} \\odot f'(X)\n",
    "$$\n",
    "\n",
    "$$\\operatorname{RelU'}(x)= \\begin{cases}0, & x < 0 \\\\ 1, & x>0\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelU(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(x,0)\n",
    "\n",
    "    def backward(self, out_grad, lr):\n",
    "        Relu_prime = (self.x > 0).astype(int)\n",
    "        #Relu_prime = np.where(self.x > 0, 1, 0)\n",
    "\n",
    "        return np.multiply(Relu_prime, out_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "### MSE\n",
    "\n",
    "$$E = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "### MSE Gradient\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial Y} = \\frac{2}{nm}(\\hat{Y} - Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    def loss(self, y, y_true):\n",
    "        return np.mean((y-y_true)**2)\n",
    "    \n",
    "    def loss_grad(self, y, y_true):\n",
    "        no_classes = y.shape[0]\n",
    "        no_datapoints = y.shape[1]\n",
    "        return (2/(no_classes*no_datapoints)) * (y-y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, x, y, layers_dim, lr, max_iterations, activation=RelU, loss=MSE):\n",
    "        self.x = x.T\n",
    "        self.y = y.T\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "        self.max_iterations = max_iterations\n",
    "        self.network = []\n",
    "        self.initialize_network(layers_dim, activation)\n",
    "    \n",
    "    def initialize_network(self, layers_dim, activation) -> None:\n",
    "        assert layers_dim[0] == self.x.shape[0], f\"The input layer must be of size {self.x.shape[0]}\"\n",
    "        assert layers_dim[-1] == self.y.shape[0], f\"The output layer must be of size {self.y.shape[0]}\"\n",
    "\n",
    "        for i in range(len(layers_dim) - 1):\n",
    "            self.network.extend(\n",
    "                [ Linear(layers_dim[i], layers_dim[i+1]), activation() ]\n",
    "                 )\n",
    "            \n",
    "    def forward(self):\n",
    "        output = self.x\n",
    "        for layer in self.network:\n",
    "            output = layer(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backward(self, y_pred):\n",
    "        out_grad = self.loss.loss_grad(self, y_pred, self.y)\n",
    "        for layer in reversed(self.network):\n",
    "            out_grad = layer.backward(out_grad, self.lr)\n",
    "        \n",
    "        return out_grad\n",
    "    \n",
    "    def train(self) -> None:\n",
    "        for epoch in range(self.max_iterations):\n",
    "            y_pred = self.forward()\n",
    "            loss = self.loss.loss(self, y_pred, self.y)\n",
    "            acc = calculate_accuracy(y_pred, self.y)\n",
    "            self.backward(y_pred)\n",
    "            print(f\"Epoch {epoch}/{self.max_iterations}: Loss = {loss}, acc = {acc*100:.2f}%\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        output = x\n",
    "        for layer in self.network:\n",
    "            output = layer(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y_true):\n",
    "    pred_labels = np.argmax(y_pred, axis=0)\n",
    "    true_labels = np.argmax(y_true, axis=0)\n",
    "    return np.mean(pred_labels == true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "mnist_dataset = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the 10 classes of the mnist dataset\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 10, figsize=(20, 20))\n",
    "for i in range(10):\n",
    "    ax[i].imshow(mnist_dataset[0][0][i], cmap='gray')\n",
    "    ax[i].set_title(mnist_dataset[0][1][i])\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/400: Loss = 0.2846639373637039, acc = 11.15%\n",
      "Epoch 1/400: Loss = 0.11809815605841975, acc = 19.70%\n",
      "Epoch 2/400: Loss = 0.10142902910964072, acc = 26.11%\n",
      "Epoch 3/400: Loss = 0.0927505710212492, acc = 31.45%\n",
      "Epoch 4/400: Loss = 0.08610758778771775, acc = 37.25%\n",
      "Epoch 5/400: Loss = 0.07971651765457125, acc = 42.70%\n",
      "Epoch 6/400: Loss = 0.07431001070081748, acc = 46.57%\n",
      "Epoch 7/400: Loss = 0.07026317774556601, acc = 49.58%\n",
      "Epoch 8/400: Loss = 0.06698661412097306, acc = 52.29%\n",
      "Epoch 9/400: Loss = 0.06410933240115749, acc = 54.81%\n",
      "Epoch 10/400: Loss = 0.06139162470516078, acc = 57.31%\n",
      "Epoch 11/400: Loss = 0.05875152872500929, acc = 59.81%\n",
      "Epoch 12/400: Loss = 0.05621395068075168, acc = 62.16%\n",
      "Epoch 13/400: Loss = 0.053976719535976055, acc = 64.23%\n",
      "Epoch 14/400: Loss = 0.05213136346548588, acc = 65.76%\n",
      "Epoch 15/400: Loss = 0.05061901214115518, acc = 67.06%\n",
      "Epoch 16/400: Loss = 0.049341894449383986, acc = 68.07%\n",
      "Epoch 17/400: Loss = 0.04822864069860058, acc = 68.96%\n",
      "Epoch 18/400: Loss = 0.04723241438028797, acc = 69.73%\n",
      "Epoch 19/400: Loss = 0.046320449046131355, acc = 70.48%\n",
      "Epoch 20/400: Loss = 0.04548149056379473, acc = 71.14%\n",
      "Epoch 21/400: Loss = 0.04470164249633775, acc = 71.67%\n",
      "Epoch 22/400: Loss = 0.04397156413309239, acc = 72.17%\n",
      "Epoch 23/400: Loss = 0.04328358044870908, acc = 72.69%\n",
      "Epoch 24/400: Loss = 0.04262878885466911, acc = 73.19%\n",
      "Epoch 25/400: Loss = 0.0419987017402442, acc = 73.66%\n",
      "Epoch 26/400: Loss = 0.04139130338117538, acc = 74.17%\n",
      "Epoch 27/400: Loss = 0.040792791711773344, acc = 74.67%\n",
      "Epoch 28/400: Loss = 0.040193931410532964, acc = 75.15%\n",
      "Epoch 29/400: Loss = 0.03957974009635468, acc = 75.69%\n",
      "Epoch 30/400: Loss = 0.03895093447299636, acc = 76.21%\n",
      "Epoch 31/400: Loss = 0.03832154940551668, acc = 76.78%\n",
      "Epoch 32/400: Loss = 0.03770314119291333, acc = 77.34%\n",
      "Epoch 33/400: Loss = 0.03710579942771756, acc = 77.87%\n",
      "Epoch 34/400: Loss = 0.03653603691085304, acc = 78.38%\n",
      "Epoch 35/400: Loss = 0.03599496760218313, acc = 78.87%\n",
      "Epoch 36/400: Loss = 0.0354866987219568, acc = 79.31%\n",
      "Epoch 37/400: Loss = 0.03500266425820071, acc = 79.74%\n",
      "Epoch 38/400: Loss = 0.03454414327370037, acc = 80.16%\n",
      "Epoch 39/400: Loss = 0.034110451745469776, acc = 80.54%\n",
      "Epoch 40/400: Loss = 0.03369669983221238, acc = 80.83%\n",
      "Epoch 41/400: Loss = 0.03330204756598813, acc = 81.16%\n",
      "Epoch 42/400: Loss = 0.032926656763161595, acc = 81.50%\n",
      "Epoch 43/400: Loss = 0.032563552210501716, acc = 81.79%\n",
      "Epoch 44/400: Loss = 0.032212605645007095, acc = 82.03%\n",
      "Epoch 45/400: Loss = 0.031874990892482885, acc = 82.30%\n",
      "Epoch 46/400: Loss = 0.031548684773109015, acc = 82.55%\n",
      "Epoch 47/400: Loss = 0.031234629925707733, acc = 82.80%\n",
      "Epoch 48/400: Loss = 0.03093235398382104, acc = 83.04%\n",
      "Epoch 49/400: Loss = 0.03063928147974809, acc = 83.23%\n",
      "Epoch 50/400: Loss = 0.03035451469927054, acc = 83.45%\n",
      "Epoch 51/400: Loss = 0.030077944040048276, acc = 83.66%\n",
      "Epoch 52/400: Loss = 0.029810314639317853, acc = 83.84%\n",
      "Epoch 53/400: Loss = 0.02955036433398723, acc = 84.03%\n",
      "Epoch 54/400: Loss = 0.02929916505791625, acc = 84.24%\n",
      "Epoch 55/400: Loss = 0.029056349539264077, acc = 84.40%\n",
      "Epoch 56/400: Loss = 0.028822304576964938, acc = 84.55%\n",
      "Epoch 57/400: Loss = 0.02859394590522125, acc = 84.74%\n",
      "Epoch 58/400: Loss = 0.028372450315432, acc = 84.90%\n",
      "Epoch 59/400: Loss = 0.02815674730793112, acc = 85.04%\n",
      "Epoch 60/400: Loss = 0.027947782600397302, acc = 85.18%\n",
      "Epoch 61/400: Loss = 0.02774541424596309, acc = 85.33%\n",
      "Epoch 62/400: Loss = 0.027547619885765793, acc = 85.47%\n",
      "Epoch 63/400: Loss = 0.02735564351233406, acc = 85.64%\n",
      "Epoch 64/400: Loss = 0.027168597002727016, acc = 85.74%\n",
      "Epoch 65/400: Loss = 0.02698674801216449, acc = 85.88%\n",
      "Epoch 66/400: Loss = 0.026809420464185114, acc = 85.99%\n",
      "Epoch 67/400: Loss = 0.02663701983962632, acc = 86.14%\n",
      "Epoch 68/400: Loss = 0.026469037450085508, acc = 86.26%\n",
      "Epoch 69/400: Loss = 0.026305007350794277, acc = 86.36%\n",
      "Epoch 70/400: Loss = 0.026144970650020246, acc = 86.45%\n",
      "Epoch 71/400: Loss = 0.025989030782041126, acc = 86.55%\n",
      "Epoch 72/400: Loss = 0.02583641553770955, acc = 86.65%\n",
      "Epoch 73/400: Loss = 0.025687328725961082, acc = 86.75%\n",
      "Epoch 74/400: Loss = 0.025541809590910017, acc = 86.85%\n",
      "Epoch 75/400: Loss = 0.02540018068172237, acc = 86.94%\n",
      "Epoch 76/400: Loss = 0.025261781107627033, acc = 87.02%\n",
      "Epoch 77/400: Loss = 0.025126304082494364, acc = 87.11%\n",
      "Epoch 78/400: Loss = 0.024993850750485467, acc = 87.20%\n",
      "Epoch 79/400: Loss = 0.024863913155802695, acc = 87.28%\n",
      "Epoch 80/400: Loss = 0.02473655017070749, acc = 87.38%\n",
      "Epoch 81/400: Loss = 0.024611824083042712, acc = 87.47%\n",
      "Epoch 82/400: Loss = 0.02448887045069765, acc = 87.54%\n",
      "Epoch 83/400: Loss = 0.024368281939563297, acc = 87.61%\n",
      "Epoch 84/400: Loss = 0.02424989210468325, acc = 87.69%\n",
      "Epoch 85/400: Loss = 0.02413392875011915, acc = 87.74%\n",
      "Epoch 86/400: Loss = 0.02402058024862087, acc = 87.81%\n",
      "Epoch 87/400: Loss = 0.023909500168550247, acc = 87.89%\n",
      "Epoch 88/400: Loss = 0.02380047125661057, acc = 87.96%\n",
      "Epoch 89/400: Loss = 0.023693589520588802, acc = 88.02%\n",
      "Epoch 90/400: Loss = 0.023588680551341435, acc = 88.08%\n",
      "Epoch 91/400: Loss = 0.02348579691170131, acc = 88.13%\n",
      "Epoch 92/400: Loss = 0.02338478814714112, acc = 88.20%\n",
      "Epoch 93/400: Loss = 0.023285300854878864, acc = 88.26%\n",
      "Epoch 94/400: Loss = 0.02318788912275041, acc = 88.31%\n",
      "Epoch 95/400: Loss = 0.02309222488822496, acc = 88.39%\n",
      "Epoch 96/400: Loss = 0.022998073268159336, acc = 88.47%\n",
      "Epoch 97/400: Loss = 0.02290508480811843, acc = 88.54%\n",
      "Epoch 98/400: Loss = 0.02281367805413611, acc = 88.59%\n",
      "Epoch 99/400: Loss = 0.02272393981043278, acc = 88.64%\n",
      "Epoch 100/400: Loss = 0.02263587269068229, acc = 88.70%\n",
      "Epoch 101/400: Loss = 0.022549143872411807, acc = 88.73%\n",
      "Epoch 102/400: Loss = 0.02246365447235956, acc = 88.76%\n",
      "Epoch 103/400: Loss = 0.02237939579296646, acc = 88.81%\n",
      "Epoch 104/400: Loss = 0.022296371953062655, acc = 88.85%\n",
      "Epoch 105/400: Loss = 0.022214749275154332, acc = 88.90%\n",
      "Epoch 106/400: Loss = 0.022134189017771023, acc = 88.94%\n",
      "Epoch 107/400: Loss = 0.02205472924382246, acc = 88.99%\n",
      "Epoch 108/400: Loss = 0.021976497834975604, acc = 89.03%\n",
      "Epoch 109/400: Loss = 0.021899306560927707, acc = 89.06%\n",
      "Epoch 110/400: Loss = 0.021823187322022333, acc = 89.11%\n",
      "Epoch 111/400: Loss = 0.021747918721928487, acc = 89.17%\n",
      "Epoch 112/400: Loss = 0.02167368656742226, acc = 89.21%\n",
      "Epoch 113/400: Loss = 0.021600286730298353, acc = 89.24%\n",
      "Epoch 114/400: Loss = 0.021527985785215954, acc = 89.29%\n",
      "Epoch 115/400: Loss = 0.02145662289814767, acc = 89.33%\n",
      "Epoch 116/400: Loss = 0.021386118124531124, acc = 89.38%\n",
      "Epoch 117/400: Loss = 0.021316352073680996, acc = 89.43%\n",
      "Epoch 118/400: Loss = 0.021247392042402374, acc = 89.46%\n",
      "Epoch 119/400: Loss = 0.021179480419258667, acc = 89.50%\n",
      "Epoch 120/400: Loss = 0.02111249946922638, acc = 89.55%\n",
      "Epoch 121/400: Loss = 0.02104649595633228, acc = 89.58%\n",
      "Epoch 122/400: Loss = 0.02098125366430457, acc = 89.61%\n",
      "Epoch 123/400: Loss = 0.020916732456608172, acc = 89.67%\n",
      "Epoch 124/400: Loss = 0.020852897713270476, acc = 89.71%\n",
      "Epoch 125/400: Loss = 0.020789923058884056, acc = 89.72%\n",
      "Epoch 126/400: Loss = 0.020727751130738013, acc = 89.76%\n",
      "Epoch 127/400: Loss = 0.020666246370437548, acc = 89.80%\n",
      "Epoch 128/400: Loss = 0.020605473735057556, acc = 89.83%\n",
      "Epoch 129/400: Loss = 0.020545412887935318, acc = 89.87%\n",
      "Epoch 130/400: Loss = 0.020485918659217662, acc = 89.91%\n",
      "Epoch 131/400: Loss = 0.020426998393087738, acc = 89.94%\n",
      "Epoch 132/400: Loss = 0.02036877047122209, acc = 89.98%\n",
      "Epoch 133/400: Loss = 0.020311244121847576, acc = 90.01%\n",
      "Epoch 134/400: Loss = 0.02025409507633727, acc = 90.04%\n",
      "Epoch 135/400: Loss = 0.020197441658673876, acc = 90.09%\n",
      "Epoch 136/400: Loss = 0.02014146330943038, acc = 90.11%\n",
      "Epoch 137/400: Loss = 0.02008624590678604, acc = 90.15%\n",
      "Epoch 138/400: Loss = 0.020031682305502237, acc = 90.18%\n",
      "Epoch 139/400: Loss = 0.019977603034182095, acc = 90.21%\n",
      "Epoch 140/400: Loss = 0.0199242019119679, acc = 90.25%\n",
      "Epoch 141/400: Loss = 0.01987142042939691, acc = 90.27%\n",
      "Epoch 142/400: Loss = 0.019819191918438426, acc = 90.30%\n",
      "Epoch 143/400: Loss = 0.01976750566944339, acc = 90.31%\n",
      "Epoch 144/400: Loss = 0.019716345648072217, acc = 90.34%\n",
      "Epoch 145/400: Loss = 0.019665762126831864, acc = 90.38%\n",
      "Epoch 146/400: Loss = 0.019615674893894652, acc = 90.41%\n",
      "Epoch 147/400: Loss = 0.019565822538336955, acc = 90.44%\n",
      "Epoch 148/400: Loss = 0.019516481732943427, acc = 90.48%\n",
      "Epoch 149/400: Loss = 0.019467479374775568, acc = 90.50%\n",
      "Epoch 150/400: Loss = 0.019418977516105922, acc = 90.53%\n",
      "Epoch 151/400: Loss = 0.019370604637190657, acc = 90.55%\n",
      "Epoch 152/400: Loss = 0.019322356866461374, acc = 90.59%\n",
      "Epoch 153/400: Loss = 0.019274475258666497, acc = 90.61%\n",
      "Epoch 154/400: Loss = 0.01922712073871796, acc = 90.65%\n",
      "Epoch 155/400: Loss = 0.019180134010758982, acc = 90.69%\n",
      "Epoch 156/400: Loss = 0.019133233467729573, acc = 90.72%\n",
      "Epoch 157/400: Loss = 0.019086652504727582, acc = 90.75%\n",
      "Epoch 158/400: Loss = 0.01904051729280826, acc = 90.78%\n",
      "Epoch 159/400: Loss = 0.018994656930016545, acc = 90.82%\n",
      "Epoch 160/400: Loss = 0.018949292506581856, acc = 90.84%\n",
      "Epoch 161/400: Loss = 0.018904517025235355, acc = 90.87%\n",
      "Epoch 162/400: Loss = 0.01886038026016409, acc = 90.89%\n",
      "Epoch 163/400: Loss = 0.018816652197686946, acc = 90.92%\n",
      "Epoch 164/400: Loss = 0.01877322752345481, acc = 90.94%\n",
      "Epoch 165/400: Loss = 0.018730100737722927, acc = 90.96%\n",
      "Epoch 166/400: Loss = 0.018687515363385063, acc = 90.98%\n",
      "Epoch 167/400: Loss = 0.01864541856350325, acc = 91.01%\n",
      "Epoch 168/400: Loss = 0.018603742129518687, acc = 91.03%\n",
      "Epoch 169/400: Loss = 0.01856235724712845, acc = 91.06%\n",
      "Epoch 170/400: Loss = 0.018521148846905307, acc = 91.07%\n",
      "Epoch 171/400: Loss = 0.018480423177208466, acc = 91.10%\n",
      "Epoch 172/400: Loss = 0.01844015186147346, acc = 91.12%\n",
      "Epoch 173/400: Loss = 0.018400293428920224, acc = 91.14%\n",
      "Epoch 174/400: Loss = 0.018360843399414278, acc = 91.16%\n",
      "Epoch 175/400: Loss = 0.01832172405844878, acc = 91.19%\n",
      "Epoch 176/400: Loss = 0.018282949999254554, acc = 91.20%\n",
      "Epoch 177/400: Loss = 0.01824458548910247, acc = 91.22%\n",
      "Epoch 178/400: Loss = 0.018206395135765368, acc = 91.23%\n",
      "Epoch 179/400: Loss = 0.01816848493847737, acc = 91.27%\n",
      "Epoch 180/400: Loss = 0.018130773475966946, acc = 91.29%\n",
      "Epoch 181/400: Loss = 0.01809335604038952, acc = 91.31%\n",
      "Epoch 182/400: Loss = 0.018056054682245942, acc = 91.33%\n",
      "Epoch 183/400: Loss = 0.018019211676720347, acc = 91.37%\n",
      "Epoch 184/400: Loss = 0.01798270437279714, acc = 91.40%\n",
      "Epoch 185/400: Loss = 0.017946392572177627, acc = 91.42%\n",
      "Epoch 186/400: Loss = 0.017910423605166127, acc = 91.45%\n",
      "Epoch 187/400: Loss = 0.01787463663426141, acc = 91.46%\n",
      "Epoch 188/400: Loss = 0.01783909167117133, acc = 91.48%\n",
      "Epoch 189/400: Loss = 0.01780386738064933, acc = 91.50%\n",
      "Epoch 190/400: Loss = 0.017768921387789467, acc = 91.53%\n",
      "Epoch 191/400: Loss = 0.017734270675013878, acc = 91.56%\n",
      "Epoch 192/400: Loss = 0.017699826671200923, acc = 91.58%\n",
      "Epoch 193/400: Loss = 0.017665683985475654, acc = 91.60%\n",
      "Epoch 194/400: Loss = 0.017631860896316288, acc = 91.61%\n",
      "Epoch 195/400: Loss = 0.017598353169135585, acc = 91.63%\n",
      "Epoch 196/400: Loss = 0.017565118710503234, acc = 91.66%\n",
      "Epoch 197/400: Loss = 0.017532030712159526, acc = 91.67%\n",
      "Epoch 198/400: Loss = 0.017499268901187203, acc = 91.69%\n",
      "Epoch 199/400: Loss = 0.01746685308466891, acc = 91.71%\n",
      "Epoch 200/400: Loss = 0.01743467894286282, acc = 91.72%\n",
      "Epoch 201/400: Loss = 0.017402723046104407, acc = 91.74%\n",
      "Epoch 202/400: Loss = 0.01737102564317682, acc = 91.76%\n",
      "Epoch 203/400: Loss = 0.017339478079519653, acc = 91.78%\n",
      "Epoch 204/400: Loss = 0.01730817125673924, acc = 91.80%\n",
      "Epoch 205/400: Loss = 0.017276979621101137, acc = 91.81%\n",
      "Epoch 206/400: Loss = 0.017245794120594727, acc = 91.83%\n",
      "Epoch 207/400: Loss = 0.01721471736096561, acc = 91.86%\n",
      "Epoch 208/400: Loss = 0.017183903053007505, acc = 91.88%\n",
      "Epoch 209/400: Loss = 0.01715314161803622, acc = 91.90%\n",
      "Epoch 210/400: Loss = 0.017122608923567564, acc = 91.92%\n",
      "Epoch 211/400: Loss = 0.017092285946117766, acc = 91.94%\n",
      "Epoch 212/400: Loss = 0.017062149027003932, acc = 91.96%\n",
      "Epoch 213/400: Loss = 0.017032114060219736, acc = 91.99%\n",
      "Epoch 214/400: Loss = 0.0170019997779081, acc = 92.02%\n",
      "Epoch 215/400: Loss = 0.016972044362766756, acc = 92.04%\n",
      "Epoch 216/400: Loss = 0.016942278837909223, acc = 92.07%\n",
      "Epoch 217/400: Loss = 0.016912585848769607, acc = 92.08%\n",
      "Epoch 218/400: Loss = 0.016883187115249238, acc = 92.10%\n",
      "Epoch 219/400: Loss = 0.01685410679019179, acc = 92.11%\n",
      "Epoch 220/400: Loss = 0.016825294906191484, acc = 92.13%\n",
      "Epoch 221/400: Loss = 0.01679665940367583, acc = 92.14%\n",
      "Epoch 222/400: Loss = 0.016768220328849705, acc = 92.16%\n",
      "Epoch 223/400: Loss = 0.01673996732464508, acc = 92.18%\n",
      "Epoch 224/400: Loss = 0.016711925270846553, acc = 92.20%\n",
      "Epoch 225/400: Loss = 0.016684081163931788, acc = 92.22%\n",
      "Epoch 226/400: Loss = 0.016656361429815907, acc = 92.23%\n",
      "Epoch 227/400: Loss = 0.016628792629654353, acc = 92.25%\n",
      "Epoch 228/400: Loss = 0.01660145972426775, acc = 92.27%\n",
      "Epoch 229/400: Loss = 0.016574257571053592, acc = 92.28%\n",
      "Epoch 230/400: Loss = 0.01654708039241916, acc = 92.29%\n",
      "Epoch 231/400: Loss = 0.016519927233869743, acc = 92.30%\n",
      "Epoch 232/400: Loss = 0.01649305534404098, acc = 92.32%\n",
      "Epoch 233/400: Loss = 0.016466455892488394, acc = 92.33%\n",
      "Epoch 234/400: Loss = 0.016440116437820743, acc = 92.35%\n",
      "Epoch 235/400: Loss = 0.01641389870084966, acc = 92.36%\n",
      "Epoch 236/400: Loss = 0.016387856816033577, acc = 92.37%\n",
      "Epoch 237/400: Loss = 0.016361962397507465, acc = 92.40%\n",
      "Epoch 238/400: Loss = 0.01633611594486409, acc = 92.40%\n",
      "Epoch 239/400: Loss = 0.016310425014958295, acc = 92.41%\n",
      "Epoch 240/400: Loss = 0.016284795254033035, acc = 92.42%\n",
      "Epoch 241/400: Loss = 0.016259231378796635, acc = 92.43%\n",
      "Epoch 242/400: Loss = 0.016233727281210468, acc = 92.44%\n",
      "Epoch 243/400: Loss = 0.0162084251517647, acc = 92.45%\n",
      "Epoch 244/400: Loss = 0.016183401795634295, acc = 92.46%\n",
      "Epoch 245/400: Loss = 0.016158585095206134, acc = 92.47%\n",
      "Epoch 246/400: Loss = 0.016133947274249126, acc = 92.48%\n",
      "Epoch 247/400: Loss = 0.016109459222584645, acc = 92.48%\n",
      "Epoch 248/400: Loss = 0.016085161181750674, acc = 92.49%\n",
      "Epoch 249/400: Loss = 0.016061002503989584, acc = 92.50%\n",
      "Epoch 250/400: Loss = 0.0160368052496844, acc = 92.52%\n",
      "Epoch 251/400: Loss = 0.016012845384006127, acc = 92.53%\n",
      "Epoch 252/400: Loss = 0.015989045574639846, acc = 92.54%\n",
      "Epoch 253/400: Loss = 0.01596536524990839, acc = 92.55%\n",
      "Epoch 254/400: Loss = 0.015941852042150536, acc = 92.57%\n",
      "Epoch 255/400: Loss = 0.015918323258796992, acc = 92.57%\n",
      "Epoch 256/400: Loss = 0.01589482313974757, acc = 92.60%\n",
      "Epoch 257/400: Loss = 0.015871477736794395, acc = 92.62%\n",
      "Epoch 258/400: Loss = 0.01584832826955225, acc = 92.62%\n",
      "Epoch 259/400: Loss = 0.015825336206928126, acc = 92.64%\n",
      "Epoch 260/400: Loss = 0.015802524796164296, acc = 92.65%\n",
      "Epoch 261/400: Loss = 0.015779915993958966, acc = 92.66%\n",
      "Epoch 262/400: Loss = 0.015757508460200458, acc = 92.66%\n",
      "Epoch 263/400: Loss = 0.015735276734683556, acc = 92.68%\n",
      "Epoch 264/400: Loss = 0.015713159680015222, acc = 92.69%\n",
      "Epoch 265/400: Loss = 0.01569119836115762, acc = 92.70%\n",
      "Epoch 266/400: Loss = 0.015669355011807943, acc = 92.72%\n",
      "Epoch 267/400: Loss = 0.015647393279358116, acc = 92.74%\n",
      "Epoch 268/400: Loss = 0.015625408811046303, acc = 92.74%\n",
      "Epoch 269/400: Loss = 0.015603633707815326, acc = 92.75%\n",
      "Epoch 270/400: Loss = 0.015582017748121417, acc = 92.76%\n",
      "Epoch 271/400: Loss = 0.01556054697067896, acc = 92.76%\n",
      "Epoch 272/400: Loss = 0.015539213694636499, acc = 92.78%\n",
      "Epoch 273/400: Loss = 0.01551785843361486, acc = 92.79%\n",
      "Epoch 274/400: Loss = 0.015496466222665527, acc = 92.81%\n",
      "Epoch 275/400: Loss = 0.015475254048535333, acc = 92.82%\n",
      "Epoch 276/400: Loss = 0.015454151399937135, acc = 92.84%\n",
      "Epoch 277/400: Loss = 0.01543322167248207, acc = 92.85%\n",
      "Epoch 278/400: Loss = 0.01541244654420218, acc = 92.86%\n",
      "Epoch 279/400: Loss = 0.015391822105906165, acc = 92.87%\n",
      "Epoch 280/400: Loss = 0.01537133151098047, acc = 92.88%\n",
      "Epoch 281/400: Loss = 0.015350962030789356, acc = 92.90%\n",
      "Epoch 282/400: Loss = 0.015330687355113682, acc = 92.90%\n",
      "Epoch 283/400: Loss = 0.015310505111057547, acc = 92.92%\n",
      "Epoch 284/400: Loss = 0.015290285687250634, acc = 92.92%\n",
      "Epoch 285/400: Loss = 0.015270164874731955, acc = 92.92%\n",
      "Epoch 286/400: Loss = 0.015250171275321994, acc = 92.93%\n",
      "Epoch 287/400: Loss = 0.015230318918278994, acc = 92.94%\n",
      "Epoch 288/400: Loss = 0.015210547939594947, acc = 92.95%\n",
      "Epoch 289/400: Loss = 0.015190921975356486, acc = 92.97%\n",
      "Epoch 290/400: Loss = 0.015171418769746281, acc = 92.98%\n",
      "Epoch 291/400: Loss = 0.015152035210963116, acc = 92.99%\n",
      "Epoch 292/400: Loss = 0.01513275679895061, acc = 93.00%\n",
      "Epoch 293/400: Loss = 0.015113606823107459, acc = 93.01%\n",
      "Epoch 294/400: Loss = 0.015094348486389976, acc = 93.01%\n",
      "Epoch 295/400: Loss = 0.01507520885624761, acc = 93.02%\n",
      "Epoch 296/400: Loss = 0.015056148259471385, acc = 93.03%\n",
      "Epoch 297/400: Loss = 0.015037156347534742, acc = 93.05%\n",
      "Epoch 298/400: Loss = 0.015018209425239603, acc = 93.06%\n",
      "Epoch 299/400: Loss = 0.014999398937589952, acc = 93.06%\n",
      "Epoch 300/400: Loss = 0.014980705058932401, acc = 93.07%\n",
      "Epoch 301/400: Loss = 0.014962137666543902, acc = 93.08%\n",
      "Epoch 302/400: Loss = 0.014943667150657607, acc = 93.08%\n",
      "Epoch 303/400: Loss = 0.014925304893495497, acc = 93.09%\n",
      "Epoch 304/400: Loss = 0.014907059047283462, acc = 93.11%\n",
      "Epoch 305/400: Loss = 0.014888893686196928, acc = 93.12%\n",
      "Epoch 306/400: Loss = 0.014870819647484478, acc = 93.12%\n",
      "Epoch 307/400: Loss = 0.014852874570340997, acc = 93.14%\n",
      "Epoch 308/400: Loss = 0.014835044326692835, acc = 93.14%\n",
      "Epoch 309/400: Loss = 0.014817315584228429, acc = 93.15%\n",
      "Epoch 310/400: Loss = 0.014799610524360324, acc = 93.16%\n",
      "Epoch 311/400: Loss = 0.014782019372197678, acc = 93.17%\n",
      "Epoch 312/400: Loss = 0.014764540378799929, acc = 93.18%\n",
      "Epoch 313/400: Loss = 0.014747166743949124, acc = 93.19%\n",
      "Epoch 314/400: Loss = 0.014729875394227382, acc = 93.21%\n",
      "Epoch 315/400: Loss = 0.014712693680831012, acc = 93.22%\n",
      "Epoch 316/400: Loss = 0.01469561212038481, acc = 93.23%\n",
      "Epoch 317/400: Loss = 0.01467860550976927, acc = 93.24%\n",
      "Epoch 318/400: Loss = 0.014661679941782043, acc = 93.25%\n",
      "Epoch 319/400: Loss = 0.014644869953240217, acc = 93.26%\n",
      "Epoch 320/400: Loss = 0.01462817109769606, acc = 93.27%\n",
      "Epoch 321/400: Loss = 0.014611568649042646, acc = 93.28%\n",
      "Epoch 322/400: Loss = 0.01459505783449763, acc = 93.29%\n",
      "Epoch 323/400: Loss = 0.014578625900094832, acc = 93.30%\n",
      "Epoch 324/400: Loss = 0.014562278503495039, acc = 93.30%\n",
      "Epoch 325/400: Loss = 0.014545965093334575, acc = 93.31%\n",
      "Epoch 326/400: Loss = 0.01452972840014958, acc = 93.31%\n",
      "Epoch 327/400: Loss = 0.014513519153794617, acc = 93.33%\n",
      "Epoch 328/400: Loss = 0.014497391649437431, acc = 93.33%\n",
      "Epoch 329/400: Loss = 0.01448132455325207, acc = 93.34%\n",
      "Epoch 330/400: Loss = 0.014465338386543347, acc = 93.34%\n",
      "Epoch 331/400: Loss = 0.01444936983108313, acc = 93.34%\n",
      "Epoch 332/400: Loss = 0.014433489333268452, acc = 93.35%\n",
      "Epoch 333/400: Loss = 0.014417708991275003, acc = 93.35%\n",
      "Epoch 334/400: Loss = 0.014402022866232414, acc = 93.36%\n",
      "Epoch 335/400: Loss = 0.01438639254455458, acc = 93.37%\n",
      "Epoch 336/400: Loss = 0.014370818974951884, acc = 93.38%\n",
      "Epoch 337/400: Loss = 0.014355236344436587, acc = 93.39%\n",
      "Epoch 338/400: Loss = 0.014339702955024302, acc = 93.40%\n",
      "Epoch 339/400: Loss = 0.014324252417276796, acc = 93.42%\n",
      "Epoch 340/400: Loss = 0.014308827057395305, acc = 93.42%\n",
      "Epoch 341/400: Loss = 0.014293465550600884, acc = 93.44%\n",
      "Epoch 342/400: Loss = 0.014278150134494846, acc = 93.45%\n",
      "Epoch 343/400: Loss = 0.014262924482467165, acc = 93.45%\n",
      "Epoch 344/400: Loss = 0.01424775960585024, acc = 93.46%\n",
      "Epoch 345/400: Loss = 0.014232662921624683, acc = 93.46%\n",
      "Epoch 346/400: Loss = 0.014217663110313281, acc = 93.46%\n",
      "Epoch 347/400: Loss = 0.014202762789674491, acc = 93.47%\n",
      "Epoch 348/400: Loss = 0.014187942386930136, acc = 93.47%\n",
      "Epoch 349/400: Loss = 0.014173161975570434, acc = 93.47%\n",
      "Epoch 350/400: Loss = 0.014158449130104651, acc = 93.47%\n",
      "Epoch 351/400: Loss = 0.014143807113379689, acc = 93.48%\n",
      "Epoch 352/400: Loss = 0.014129245329892738, acc = 93.48%\n",
      "Epoch 353/400: Loss = 0.014114758460410697, acc = 93.50%\n",
      "Epoch 354/400: Loss = 0.01410029275387998, acc = 93.50%\n",
      "Epoch 355/400: Loss = 0.014085894812487173, acc = 93.52%\n",
      "Epoch 356/400: Loss = 0.014071545198287698, acc = 93.53%\n",
      "Epoch 357/400: Loss = 0.014057259619759413, acc = 93.53%\n",
      "Epoch 358/400: Loss = 0.014043037155464946, acc = 93.54%\n",
      "Epoch 359/400: Loss = 0.01402888748159398, acc = 93.55%\n",
      "Epoch 360/400: Loss = 0.014014830708279412, acc = 93.55%\n",
      "Epoch 361/400: Loss = 0.014000854801621608, acc = 93.55%\n",
      "Epoch 362/400: Loss = 0.013986951379370581, acc = 93.56%\n",
      "Epoch 363/400: Loss = 0.013973102449582182, acc = 93.56%\n",
      "Epoch 364/400: Loss = 0.013959293662255609, acc = 93.57%\n",
      "Epoch 365/400: Loss = 0.013945538562029961, acc = 93.58%\n",
      "Epoch 366/400: Loss = 0.013931839355877643, acc = 93.59%\n",
      "Epoch 367/400: Loss = 0.013918216119360945, acc = 93.59%\n",
      "Epoch 368/400: Loss = 0.013904666903082136, acc = 93.59%\n",
      "Epoch 369/400: Loss = 0.013891184442328676, acc = 93.59%\n",
      "Epoch 370/400: Loss = 0.013877759482870254, acc = 93.60%\n",
      "Epoch 371/400: Loss = 0.013864356650225567, acc = 93.60%\n",
      "Epoch 372/400: Loss = 0.013851002026615216, acc = 93.61%\n",
      "Epoch 373/400: Loss = 0.013837670936815658, acc = 93.62%\n",
      "Epoch 374/400: Loss = 0.013824372465321451, acc = 93.62%\n",
      "Epoch 375/400: Loss = 0.013811122252203949, acc = 93.62%\n",
      "Epoch 376/400: Loss = 0.013797929917851186, acc = 93.63%\n",
      "Epoch 377/400: Loss = 0.013784807417461997, acc = 93.65%\n",
      "Epoch 378/400: Loss = 0.013771732860199247, acc = 93.65%\n",
      "Epoch 379/400: Loss = 0.01375868561174708, acc = 93.65%\n",
      "Epoch 380/400: Loss = 0.013745700327587358, acc = 93.66%\n",
      "Epoch 381/400: Loss = 0.013732778375020053, acc = 93.67%\n",
      "Epoch 382/400: Loss = 0.013719902103688552, acc = 93.67%\n",
      "Epoch 383/400: Loss = 0.013707091143834931, acc = 93.69%\n",
      "Epoch 384/400: Loss = 0.013694345801617802, acc = 93.69%\n",
      "Epoch 385/400: Loss = 0.013681635844228893, acc = 93.69%\n",
      "Epoch 386/400: Loss = 0.013668941542063207, acc = 93.70%\n",
      "Epoch 387/400: Loss = 0.013656286380926513, acc = 93.70%\n",
      "Epoch 388/400: Loss = 0.013643671349967672, acc = 93.70%\n",
      "Epoch 389/400: Loss = 0.013631071067774812, acc = 93.71%\n",
      "Epoch 390/400: Loss = 0.013618505696954313, acc = 93.73%\n",
      "Epoch 391/400: Loss = 0.013605986130903584, acc = 93.74%\n",
      "Epoch 392/400: Loss = 0.01359349305187162, acc = 93.75%\n",
      "Epoch 393/400: Loss = 0.013581059894565716, acc = 93.76%\n",
      "Epoch 394/400: Loss = 0.013568604700426864, acc = 93.76%\n",
      "Epoch 395/400: Loss = 0.013556166605043527, acc = 93.77%\n",
      "Epoch 396/400: Loss = 0.01354380242019383, acc = 93.77%\n",
      "Epoch 397/400: Loss = 0.013531505032926772, acc = 93.78%\n",
      "Epoch 398/400: Loss = 0.013519259611632157, acc = 93.78%\n",
      "Epoch 399/400: Loss = 0.013507005568688045, acc = 93.78%\n",
      "==========================================\n",
      "Test data Accuracy: 92.92%\n"
     ]
    }
   ],
   "source": [
    "X = mnist_dataset[0][0].reshape(60000, 784) / 255       # flatten, normalize\n",
    "y = mnist_dataset[0][1]\n",
    "y_one_hot = np.eye(10)[y]                       # one hot encode the target labels\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "X_train_normalized = X_train_normalized\n",
    "X_test_normalized = X_test_normalized.T\n",
    "y_train = y_train\n",
    "y_test = y_test.T\n",
    "\n",
    "layers_dim = [784,64,10]\n",
    "lr = 0.6   # Learning rate\n",
    "activation_layer = RelU\n",
    "loss = MSE\n",
    "enable_bias = True\n",
    "\n",
    "nn = NN(X_train_normalized, y_train, layers_dim, lr, 400, activation_layer, loss)\n",
    "nn.train()\n",
    "y_pred = nn.predict(X_test_normalized)\n",
    "\n",
    "# Calculate the accuracy of the model with the test data\n",
    "accuracy = calculate_accuracy(y_pred, y_test)\n",
    "print(\"==========================================\")\n",
    "print(f\"Test data Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
